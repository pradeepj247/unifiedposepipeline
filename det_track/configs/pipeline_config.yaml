# Detection & Tracking Pipeline Configuration
# Multi-stage pipeline for person detection, tracking, and identity resolution

# ============================================================================
# Global Path Variables (Single Source of Truth)
# ============================================================================
global:
  # Repository root - change this for different environments
  repo_root: /content/unifiedposepipeline  # Colab
  # repo_root: D:/trials/unifiedpipeline/newrepo  # Windows (uncomment for local)
  
  # Derived paths (use ${variable} syntax)
  models_dir: ${repo_root}/models
  demo_data_dir: ${repo_root}/demo_data
  outputs_dir: ${demo_data_dir}/outputs
  
  # Video input settings - change video_file to process different videos
  video_dir: ${demo_data_dir}/videos/
  video_file: kohli_nets.mp4
  
  # Logging and output control
  verbose: false                             # Set to true for detailed debug output across all stages

# ============================================================================
# Pipeline Stage Control - SINGLE SOURCE OF TRUTH
# ============================================================================
# NOTE: These settings ALONE control which stages execute.
# Use ONLY the settings below to control pipeline execution.
# ============================================================================
pipeline:
  # Stage execution control - Simplified 5-stage pipeline (Phase 3)
  # 0. Video Normalization
  # 1. Detection
  # 2. Tracking
  # 3. Analysis & Ranking (3a → 3b → 3c)
  # 4. HTML Generation (on-demand extraction + WebP + viewer)
  stages:
    stage0: true                           # Stage 0: Video normalization & validation
    stage1: true                           # Stage 1: YOLO detection
    stage2: true                           # Stage 2: ByteTrack tracking
    stage3a: true                          # Stage 3a: Tracklet analysis
    stage3b: true                          # Stage 3b: Canonical grouping
    stage3c: true                          # Stage 3c: Filter persons (top 8) & crop extraction
    stage3d: true                          # Stage 3d: OSNet-based visual refinement (ReID merging)
    stage4: true                           # Stage 4: Generate HTML viewer
  
  # Global settings inherited by all stages (can be overridden per-stage if needed)
  advanced:
    verbose: false                         # Debug output for all stages (set true to see detailed logs)


# ============================================================================
# Stage 0: Video Normalization & Validation
# ============================================================================
# This is the FIRST stage - runs BEFORE YOLO detection
# Ensures all videos are in a consistent, optimal format
stage0_normalize:
  enabled: true  # Set false to skip normalization (use original video)
  
  # Video validation limits
  limits:
    max_duration_seconds: 120        # Max video length (2 minutes)
    max_filesize_mb: 200             # Max file size (200 MB)
    max_resolution: [1920, 1080]     # Max width and height
  
  # Normalization settings
  normalization:
    target_fps: 25                   # Constant FPS (matches current videos)
    force_constant_fps: true         # Convert VFR to CFR
  
  # Encoding settings (canonical format)
  encoding:
    codec: libx264                   # H.264 codec
    preset: veryfast                 # Encoding speed (ultrafast/veryfast/fast/medium)
    profile: main                    # H.264 profile (baseline/main/high)
    pix_fmt: yuv420p                 # Pixel format (universal compatibility)
    keyframe_interval: 30            # GOP size (keyframe every 30 frames = 1.2s at 25fps)
  
  # Input/Output
  input:
    video_file: ${video_dir}${video_file}
  
  output:
    canonical_video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    timing_file: ${outputs_dir}/${current_video}/stage0_timing.json
    symlink_if_canonical: true       # If already canonical, symlink instead of re-encoding

# ============================================================================
# Stage 1: YOLO Detection
# ============================================================================
stage1_detect:
  detector:
    model_path: ${models_dir}/yolo/yolov8s.pt  # Use .engine for TensorRT (requires tensorrt package)
    confidence: 0.3
    device: cuda
    detect_only_humans: true
  
  detection_limit:
    method: hybrid  # Options: top_n, confidence, hybrid
    max_count: 15   # Maximum detections per frame
    min_confidence: 0.3  # Minimum confidence threshold
  
  input:
    # Use canonical video from Stage 0 (falls back to original if Stage 0 disabled)
    video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    max_frames: 0  # 0 = all frames
  
  output:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
    # crops_cache_file REMOVED in Phase 3 (on-demand extraction replaces this)

# ============================================================================
# Stage 2: ByteTrack Offline Tracking
# ============================================================================
# NOTE: Only ByteTrack is currently supported (hardcoded in stage2_track.py)
stage2_track:
  params:
    track_thresh: 0.15     # Confidence threshold for tracking (lowered for better recall)
    track_buffer: 30       # Buffer size for lost tracks (frames)
    match_thresh: 0.8      # IOU threshold for matching
    min_hits: 1            # Minimum consecutive detections to create track (1=immediate)
  
  input:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
  
  output:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz

# ============================================================================
# Stage 3a: Tracklet Analysis
# ============================================================================
# Computes statistics once for reuse by Stage 3b
# Includes motion features: velocity, jitter (used by enhanced grouping)
stage3a_analyze:
  input:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz
  
  output:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  advanced:
    verbose: false  # Set true to see per-tracklet statistics

# ============================================================================
# Stage 3b: Canonical Grouping
# ============================================================================
# Enhanced grouping with 5 merge checks (3 existing + 2 motion-based)
# Loads pre-computed stats from Stage 3a (no recomputation)
stage3b_group:
  grouping:
    method: enhanced  # Enhanced heuristic with motion checks
    
    # Enhanced criteria (5 checks total)
    enhanced_criteria:
      # Existing checks (from old Stage 5)
      max_temporal_gap: 30              # Temporal proximity (frames)
      max_spatial_distance: 200         # Spatial proximity (pixels)
      area_ratio_range: [0.7, 1.3]      # Size consistency
      
      # NEW motion-based checks
      min_motion_alignment: 0.6         # Cosine similarity of velocity vectors (>0.6 = same direction)
      max_jitter_difference: 40.0       # Max difference in center_jitter (pixels)
  
  input:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  output:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
    grouping_log_file: ${outputs_dir}/${current_video}/grouping_log.json
  
  advanced:
    verbose: false  # Set true to see merge criteria and sample merges

# ============================================================================
# Stage 3c: Person Filtering & Crop Extraction
# ============================================================================
# Filters canonical persons to TOP 10 based on ranking scores.
# Applies late-appearance penalty to top 10 (may reduce to ~8).
# Extracts crops for each selected person.
#
# Input: canonical_persons.npz (40+ persons from Stage 3b)
# Output: canonical_persons_filtered.npz (8-10 persons), final_crops.pkl (8-10 persons with crops)
#
# Two-step filtering:
# 1. Select TOP 10 by ranking score (duration, coverage, center, smoothness)
# 2. Apply late-appearance penalty to top 10
#    - Persons starting after 50% of video get penalized
#    - Penalty threshold (default 0.7) removes heavily penalized persons
#    - Result: typically 8-10 persons remain
stage3c_filter:
  filtering:
    min_duration_frames: 150               # STEP 0: Minimum 5 seconds (150 frames at 30fps) - removes short persons early
    top_n: 10                              # STEP 1: Select top 10 persons from filtered candidates
    penalty_threshold: 0.75                # STEP 2: Remove if penalty < this (0.75 = allows max 25% penalty; catches very late persons)
    crops_per_person: 50                   # Extract 50 crops per person
    
    # Ranking weights (must sum to 1.0)
    weights:
      duration: 0.4                        # Longest presence
      coverage: 0.3                        # Best frame coverage (% of timespan actually detected)
      center: 0.2                          # Center alignment (main action usually center)
      smoothness: 0.1                      # Motion stability (smooth > jittery)
      max_appearance_ratio: 0.5            # LATE-APPEARANCE: Penalty starts after 50% of video
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    canonical_persons_filtered_file: ${outputs_dir}/${current_video}/canonical_persons_filtered.npz
    final_crops_file: ${outputs_dir}/${current_video}/final_crops.pkl
  
  advanced:
    verbose: false  # Set true to see top persons with scores and penalties

# ============================================================================
# Stage 3d: Visual Refinement (OSNet ReID-based Person Merging)
# ============================================================================
# Uses OSNet embeddings to identify and merge split detections of the same person.
# Input: canonical_persons_filtered.npz + final_crops.pkl from Stage 3c (8-10 persons)
# Output: Same filenames, potentially fewer persons (if merges found)
#
# Algorithm:
# - Extract OSNet features from crops (averaged per person)
# - Find non-overlapping temporal pairs
# - Compute cosine similarity for all pairs
# - Build connected components (Union-Find) for same-person chains
# - Merge crops and canonical person records
#
# Note: Output file names are IDENTICAL to Stage 3c.
# Stage 3d overwrites Stage 3c outputs with refined (potentially merged) data.
# If no merges found: outputs are same as 3c inputs
# If merges found: outputs are reduced (e.g., 8 → 6)
stage3d_refine:
  osnet:
    model_path: ${models_dir}/reid/osnet_x0_25_msmt17.onnx  # Recommended: x0_25 (better discrimination)
    device: cuda
    num_crops_per_person: 16               # Use top 16 crops for feature extraction
  
  merging:
    temporal_gap_max: 15                   # Max frames between persons (gap > this = separate persons)
    temporal_overlap_tolerance: 15         # Allow up to 15 frames of overlap (small overlaps OK)
    similarity_threshold: 0.60             # Cosine similarity threshold (0-1)
  
  input:
    final_crops_file: ${outputs_dir}/${current_video}/final_crops.pkl
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons_filtered.npz
  
  output:
    final_crops_merged_file: ${outputs_dir}/${current_video}/final_crops_3d.pkl  # Stage 3d outputs
    canonical_persons_merged_file: ${outputs_dir}/${current_video}/canonical_persons_3d.npz  # Stage 3d outputs
    merging_report_file: ${outputs_dir}/${current_video}/merging_report.json
  
  advanced:
    verbose: false  # Set true to see person chains and similarity scores

# ============================================================================
# Stage 4: Generate HTML Viewer
# ============================================================================
# Reads BOTH Stage 3c and 3d outputs for comparison debugging.
# Shows 2 rows: 3c outputs (10 persons) vs 3d outputs (7 persons) with merge info.
stage4_generate_html:
  # Input files - Stage 3c (before merge)
  final_crops_3c_file: ${outputs_dir}/${current_video}/final_crops.pkl
  canonical_persons_3c_file: ${outputs_dir}/${current_video}/canonical_persons_filtered.npz
  
  # Input files - Stage 3d (after merge)
  final_crops_3d_file: ${outputs_dir}/${current_video}/final_crops_3d.pkl
  canonical_persons_3d_file: ${outputs_dir}/${current_video}/canonical_persons_3d.npz
  merging_report_file: ${outputs_dir}/${current_video}/merging_report.json
  
  # Video file for frame info
  video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
  
  # Output directory
  output_dir: ${outputs_dir}/${current_video}/webp_viewer
  
  # WebP generation parameters
  resize_to: [256, 256]                   # Resize crops to this size (width, height)
  webp_duration_ms: 100                   # Frame duration in WebP (100ms = 10 FPS)
  
  # Logging
  log_file: ${outputs_dir}/${current_video}/stage4_generate_html.log
  
  output:
    webp_dir: ${outputs_dir}/${current_video}/webp_viewer
    html_file: ${outputs_dir}/${current_video}/webp_viewer/person_selection.html
