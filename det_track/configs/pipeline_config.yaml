# Detection & Tracking Pipeline Configuration
# Multi-stage pipeline for person detection, tracking, and identity resolution

# ============================================================================
# Global Path Variables (Single Source of Truth)
# ============================================================================
global:
  # Repository root - change this for different environments
  repo_root: /content/unifiedposepipeline  # Colab
  # repo_root: D:/trials/unifiedpipeline/newrepo  # Windows (uncomment for local)
  
  # Derived paths (use ${variable} syntax)
  models_dir: ${repo_root}/models
  demo_data_dir: ${repo_root}/demo_data
  outputs_dir: ${demo_data_dir}/outputs
  
  # Video input settings - change video_file to process different videos
  video_dir: ${demo_data_dir}/videos/
  video_file: kohli_nets.mp4
  
  # Logging and output control
  verbose: false                             # Set to true for detailed debug output across all stages

# ============================================================================
# Pipeline Stage Control - SINGLE SOURCE OF TRUTH
# ============================================================================
# NOTE: These settings ALONE control which stages execute.
# Use ONLY the settings below to control pipeline execution.
# ============================================================================
pipeline:
  # Stage execution control - Simplified 5-stage pipeline (Phase 3)
  # 0. Video Normalization
  # 1. Detection
  # 2. Tracking
  # 3. Analysis & Ranking (3a → 3b → 3c)
  # 4. HTML Generation (on-demand extraction + WebP + viewer)
  stages:
    stage0: true                           # Stage 0: Video normalization & validation
    stage1: true                           # Stage 1: YOLO detection
    stage2: true                           # Stage 2: ByteTrack tracking
    stage3a: true                          # Stage 3a: Tracklet analysis
    stage3b: true                          # Stage 3b: Canonical grouping
    stage3c: true                          # Stage 3c: Person ranking
    stage4: true                           # Stage 4: Generate HTML viewer (on-demand extraction)
  
  # Global settings inherited by all stages (can be overridden per-stage if needed)
  advanced:
    verbose: false                         # Debug output for all stages (set true to see detailed logs)


# ============================================================================
# Stage 0: Video Normalization & Validation
# ============================================================================
# This is the FIRST stage - runs BEFORE YOLO detection
# Ensures all videos are in a consistent, optimal format
stage0_normalize:
  enabled: true  # Set false to skip normalization (use original video)
  
  # Video validation limits
  limits:
    max_duration_seconds: 120        # Max video length (2 minutes)
    max_filesize_mb: 200             # Max file size (200 MB)
    max_resolution: [1920, 1080]     # Max width and height
  
  # Normalization settings
  normalization:
    target_fps: 25                   # Constant FPS (matches current videos)
    force_constant_fps: true         # Convert VFR to CFR
  
  # Encoding settings (canonical format)
  encoding:
    codec: libx264                   # H.264 codec
    preset: veryfast                 # Encoding speed (ultrafast/veryfast/fast/medium)
    profile: main                    # H.264 profile (baseline/main/high)
    pix_fmt: yuv420p                 # Pixel format (universal compatibility)
    keyframe_interval: 30            # GOP size (keyframe every 30 frames = 1.2s at 25fps)
  
  # Input/Output
  input:
    video_file: ${video_dir}${video_file}
  
  output:
    canonical_video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    timing_file: ${outputs_dir}/${current_video}/stage0_timing.json
    symlink_if_canonical: true       # If already canonical, symlink instead of re-encoding

# ============================================================================
# Stage 1: YOLO Detection
# ============================================================================
stage1_detect:
  detector:
    model_path: ${models_dir}/yolo/yolov8s.pt  # Use .engine for TensorRT (requires tensorrt package)
    confidence: 0.3
    device: cuda
    detect_only_humans: true
  
  detection_limit:
    method: hybrid  # Options: top_n, confidence, hybrid
    max_count: 15   # Maximum detections per frame
    min_confidence: 0.3  # Minimum confidence threshold
  
  input:
    # Use canonical video from Stage 0 (falls back to original if Stage 0 disabled)
    video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    max_frames: 0  # 0 = all frames
  
  output:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
    # crops_cache_file REMOVED in Phase 3 (on-demand extraction replaces this)

# ============================================================================
# Stage 2: ByteTrack Offline Tracking
# ============================================================================
# NOTE: Only ByteTrack is currently supported (hardcoded in stage2_track.py)
stage2_track:
  params:
    track_thresh: 0.15     # Confidence threshold for tracking (lowered for better recall)
    track_buffer: 30       # Buffer size for lost tracks (frames)
    match_thresh: 0.8      # IOU threshold for matching
    min_hits: 1            # Minimum consecutive detections to create track (1=immediate)
  
  input:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
  
  output:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz

# ============================================================================
# Stage 3a: Tracklet Analysis
# ============================================================================
# Computes statistics once for reuse by Stage 3b
# Includes motion features: velocity, jitter (used by enhanced grouping)
stage3a_analyze:
  input:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz
  
  output:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  advanced:
    verbose: false  # Set true to see per-tracklet statistics

# ============================================================================
# Stage 3b: Canonical Grouping
# ============================================================================
# Enhanced grouping with 5 merge checks (3 existing + 2 motion-based)
# Loads pre-computed stats from Stage 3a (no recomputation)
stage3b_group:
  grouping:
    method: enhanced  # Enhanced heuristic with motion checks
    
    # Enhanced criteria (5 checks total)
    enhanced_criteria:
      # Existing checks (from old Stage 5)
      max_temporal_gap: 30              # Temporal proximity (frames)
      max_spatial_distance: 200         # Spatial proximity (pixels)
      area_ratio_range: [0.7, 1.3]      # Size consistency
      
      # NEW motion-based checks
      min_motion_alignment: 0.6         # Cosine similarity of velocity vectors (>0.6 = same direction)
      max_jitter_difference: 40.0       # Max difference in center_jitter (pixels)
  
  input:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  output:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
    grouping_log_file: ${outputs_dir}/${current_video}/grouping_log.json
  
  advanced:
    verbose: false  # Set true to see merge criteria and sample merges

# ============================================================================
# Stage 3c: Person Ranking
# ============================================================================
# Ranks canonical persons and selects primary person for pose estimation
stage3c_rank:
  ranking:
    method: auto  # Options: auto (manual selection not yet implemented)
    
    # Auto-ranking weights
    weights:
      duration: 0.4       # Longest presence in video
      coverage: 0.3       # Percentage of frames covered
      center: 0.2         # Proximity to frame center
      smoothness: 0.1     # Motion stability (less jitter)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    primary_person_file: ${outputs_dir}/${current_video}/primary_person.npz
    ranking_report_file: ${outputs_dir}/${current_video}/ranking_report.json
  
  advanced:
    verbose: false  # Set true to see top 5 persons with scores

# ============================================================================
# Stage 4: Generate HTML Viewer (Phase 3) + OSNet Clustering (Phase 4)
# ============================================================================
# On-demand crop extraction from video - no intermediate storage
# NEW: Integrated OSNet clustering for ReID-based duplicate detection
# Replaces old approach that used crops_by_person.pkl (~812 MB)
# 
# Key improvements:
# - 808 MB storage savings (no crops_by_person.pkl)
# - Faster execution (~6s vs ~11s old approach)
# - Better quality control (early appearance filter)
# - Simpler pipeline (no Stage 4/4b needed)
# 
# NEW Phase 4:
# - OSNet embeddings extracted for similarity matrix
# - Identifies duplicate canonical persons (same physical person, different time)
# - HTML enhanced with similarity heatmap and recommendations
stage4_generate_html:
  # Input files
  video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
  canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  # Output directory
  output_dir: ${outputs_dir}/${current_video}/webp_viewer
  
  # Extraction parameters
  crops_per_person: 50                    # Number of crops to extract per person
  top_n_persons: 10                       # Consider top N persons by frame count
  max_first_appearance_ratio: 0.5         # Exclude persons appearing after 50% of video
  
  # WebP generation parameters
  resize_to: [256, 256]                   # Resize crops to this size (width, height)
  webp_duration_ms: 100                   # Frame duration in WebP (100ms = 10 FPS)
  
  # OSNet Clustering Parameters (NEW Phase 4)
  clustering:
    enabled: true                         # Set false to disable clustering
    osnet_model: ${models_dir}/osnet/osnet_x0_25_msmt17.pth
    device: cuda                          # cuda or cpu
    num_best_crops: 8                     # Select 8 best crops per person for embedding
    similarity_threshold: 0.70            # Highlight pairs above this similarity (0-1)
  
  # Logging
  log_file: ${outputs_dir}/${current_video}/stage4_generate_html.log
  
  # Output paths
  output:
    webp_dir: ${outputs_dir}/${current_video}/webp_viewer
    html_file: ${outputs_dir}/${current_video}/webp_viewer/person_selection.html
    similarity_matrix_json: ${outputs_dir}/${current_video}/similarity_matrix.json
    similarity_matrix_npy: ${outputs_dir}/${current_video}/similarity_matrix.npy
    embeddings_json: ${outputs_dir}/${current_video}/embeddings.json
    embeddings_npy: ${outputs_dir}/${current_video}/embeddings.npy
