# Detection & Tracking Pipeline Configuration
# Multi-stage pipeline for person detection, tracking, and identity resolution

# ============================================================================
# Global Path Variables (Single Source of Truth)
# ============================================================================
global:
  # Repository root - change this for different environments
  repo_root: /content/unifiedposepipeline  # Colab
  # repo_root: D:/trials/unifiedpipeline/newrepo  # Windows (uncomment for local)
  
  # Derived paths (use ${variable} syntax)
  models_dir: ${repo_root}/models
  demo_data_dir: ${repo_root}/demo_data
  outputs_dir: ${demo_data_dir}/outputs
  
  # Video input settings - change video_file to process different videos
  video_dir: ${demo_data_dir}/videos/
  video_file: kohli_nets.mp4
  
  # Logging and output control
  verbose: false                             # Set to true for detailed debug output across all stages

# ============================================================================
# Pipeline Stage Control - SINGLE SOURCE OF TRUTH
# ============================================================================
# NOTE: These settings ALONE control which stages execute.
# Individual stage configs (e.g., stage6_create_output_video: enabled:) are
# IGNORED by run_pipeline.py and should NOT be used to enable/disable stages.
# Use ONLY the settings below to control pipeline execution.
# ============================================================================
pipeline:
  # Stage execution control - SIMPLE NUMERIC IDs 0-11 - the ONLY place to enable/disable stages
  # NOTE: Stage 0 runs FIRST - validates and normalizes video before YOLO
  # NOTE: Order matters! Stage 11 MUST run before Stage 10 (generate GIFs before embedding in HTML)
  # NEW: Stages 3a, 3b, 3c replace old stages 3, 5, 7 with optimized analysis chain
  stages:
    stage0: true                           # Stage 0: Video normalization & validation (RUNS FIRST)
    stage1: true                           # Stage 1: YOLO detection
    stage2: true                           # Stage 2: ByteTrack offline tracking
    stage3: false                          # Stage 3: OLD - Tracklet statistics (REPLACED by 3a)
    stage3a: true                          # Stage 3a: NEW - Tracklet analysis (computes stats once)
    stage3b: true                          # Stage 3b: NEW - Enhanced grouping (5 checks, loads 3a stats)
    stage3c: true                          # Stage 3c: NEW - Person ranking (clean separation)
    stage4: true                           # Stage 4: Load crops cache (lightweight)
    stage4b: true                          # Stage 4b: NEW - Reorganize crops by person (efficient lookup)
    stage5: false                          # Stage 5: OLD - Canonical grouping (REPLACED by 3b)
    stage6: false                          # Stage 6: DISABLED - In-memory optimization (skip HDF5 write)
    stage7: false                          # Stage 7: OLD - Rank persons (REPLACED by 3c)
    stage8: false                          # Stage 8: Optional: Show grouping tables (debug)
    stage9: false                          # Stage 9: DISABLED - Video output saves 22% time (optional)
    stage10: false                         # Stage 10: OLD - Complex WebP generation (REPLACED by 10b)
    stage10b: true                         # Stage 10b: NEW - Simplified WebP generation (uses 4b output)
    stage11: true                          # Stage 11: Generate animated WebPs for top 10 persons (BEFORE stage10!)
  
  # Global settings inherited by all stages (can be overridden per-stage if needed)
  advanced:
    verbose: false                         # Debug output for all stages (set true to see detailed logs)


# ============================================================================
# Stage 0: Video Normalization & Validation
# ============================================================================
# This is the FIRST stage - runs BEFORE YOLO detection
# Ensures all videos are in a consistent, optimal format
stage0_normalize:
  enabled: true  # Set false to skip normalization (use original video)
  
  # Video validation limits
  limits:
    max_duration_seconds: 120        # Max video length (2 minutes)
    max_filesize_mb: 200             # Max file size (200 MB)
    max_resolution: [1920, 1080]     # Max width and height
  
  # Normalization settings
  normalization:
    target_fps: 25                   # Constant FPS (matches current videos)
    force_constant_fps: true         # Convert VFR to CFR
  
  # Encoding settings (canonical format)
  encoding:
    codec: libx264                   # H.264 codec
    preset: veryfast                 # Encoding speed (ultrafast/veryfast/fast/medium)
    profile: main                    # H.264 profile (baseline/main/high)
    pix_fmt: yuv420p                 # Pixel format (universal compatibility)
    keyframe_interval: 30            # GOP size (keyframe every 30 frames = 1.2s at 25fps)
  
  # Input/Output
  input:
    video_file: ${video_dir}${video_file}
  
  output:
    canonical_video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    timing_file: ${outputs_dir}/${current_video}/stage0_timing.json
    symlink_if_canonical: true       # If already canonical, symlink instead of re-encoding

# ============================================================================
# Stage 1: YOLO Detection
# ============================================================================
stage1:
  detector:
    model_path: ${models_dir}/yolo/yolov8s.pt  # Use .engine for TensorRT (requires tensorrt package)
    confidence: 0.3
    device: cuda
    detect_only_humans: true
  
  detection_limit:
    method: hybrid  # Options: top_n, confidence, hybrid
    max_count: 15   # Maximum detections per frame
    min_confidence: 0.3  # Minimum confidence threshold
  
  input:
    # Use canonical video from Stage 0 (falls back to original if Stage 0 disabled)
    video_file: ${outputs_dir}/${current_video}/canonical_video.mp4
    max_frames: 0  # 0 = all frames
  
  output:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
    crops_cache_file: ${outputs_dir}/${current_video}/crops_cache.pkl

# ============================================================================
# Stage 2: ByteTrack Offline Tracking
# ============================================================================
# NOTE: Only ByteTrack is currently supported (hardcoded in stage2_track.py)
stage2:
  params:
    track_thresh: 0.15     # Confidence threshold for tracking (lowered for better recall)
    track_buffer: 30       # Buffer size for lost tracks (frames)
    match_thresh: 0.8      # IOU threshold for matching
    min_hits: 1            # Minimum consecutive detections to create track (1=immediate)
  
  input:
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
  
  output:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz

# ============================================================================
# Stage 3: Tracklet Analysis (OLD - LEGACY)
# ============================================================================
# NOTE: Stage 3 is REPLACED by Stage 3a in the new pipeline
# Keep this config for backward compatibility and rollback
stage3:
  analysis:
    compute_statistics: true  # Temporal, spatial, motion stats
    identify_candidates: true # Find suspicious tracklet pairs for ReID
  
  candidate_criteria:
    max_temporal_gap: 50       # Frames between tracklet end/start
    max_spatial_distance: 300  # Pixels between last/first bbox centers
    area_ratio_range: [0.6, 1.4]  # Min/max area ratio for size consistency
  
  input:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz
  
  output:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
    candidates_file: ${outputs_dir}/${current_video}/reid_candidates.json

# ============================================================================
# Stage 3a: Tracklet Analysis (NEW - Reorganized Pipeline)
# ============================================================================
# Computes statistics once for reuse by Stage 3b
# Includes motion features: velocity, jitter (used by enhanced grouping)
stage3a:
  input:
    tracklets_file: ${outputs_dir}/${current_video}/tracklets_raw.npz
  
  output:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  advanced:
    verbose: false  # Set true to see per-tracklet statistics

# ============================================================================
# Stage 3b: Enhanced Canonical Grouping (NEW - Reorganized Pipeline)
# ============================================================================
# Enhanced grouping with 5 merge checks (3 existing + 2 NEW motion-based)
# Loads pre-computed stats from Stage 3a (NO recomputation!)
stage3b:
  grouping:
    method: enhanced  # Enhanced heuristic with motion checks
    
    # Enhanced criteria (5 checks total)
    enhanced_criteria:
      # Existing checks (from old Stage 5)
      max_temporal_gap: 30              # Temporal proximity (frames)
      max_spatial_distance: 200         # Spatial proximity (pixels)
      area_ratio_range: [0.7, 1.3]      # Size consistency
      
      # NEW motion-based checks
      min_motion_alignment: 0.6         # Cosine similarity of velocity vectors (>0.6 = same direction)
      max_jitter_difference: 40.0       # Max difference in center_jitter (pixels)
  
  input:
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  output:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
    grouping_log_file: ${outputs_dir}/${current_video}/grouping_log.json
  
  advanced:
    verbose: false  # Set true to see merge criteria and sample merges

# ============================================================================
# Stage 3c: Person Ranking (NEW - Reorganized Pipeline)
# ============================================================================
# Ranks canonical persons and selects primary person for pose estimation
stage3c:
  ranking:
    method: auto  # Options: auto (manual selection not yet implemented)
    
    # Auto-ranking weights
    weights:
      duration: 0.4       # Longest presence in video
      coverage: 0.3       # Percentage of frames covered
      center: 0.2         # Proximity to frame center
      smoothness: 0.1     # Motion stability (less jitter)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    primary_person_file: ${outputs_dir}/${current_video}/primary_person.npz
    ranking_report_file: ${outputs_dir}/${current_video}/ranking_report.json
  
  advanced:
    verbose: false  # Set true to see top 5 persons with scores

# ============================================================================
# Stage 4: Load Crops Cache (Lightweight - No ReID Recovery)
# ============================================================================
# NOTE: This is the lightweight version that loads pre-cached crops from Stage 1
# It does NOT perform ReID-based tracklet recovery
# For full ReID-based recovery, use the old stage4a_reid_recovery_onnx.py
stage4:
  input:
    crops_cache_file: ${outputs_dir}/${current_video}/crops_cache.pkl
  
  output:
    # No outputs - crops cache is loaded and passed to Stage 7

# ============================================================================
# Stage 4b: Reorganize Crops by Person (NEW - EFFICIENT LOOKUP)
# ============================================================================
# Pre-organizes crops by person_id for efficient downstream access
# Eliminates complex index conversion logic in visualization stages
# Format: pickle (default - faster) or hdf5 (for >5GB datasets)
stage4b:
  input:
    crops_cache_file: ${outputs_dir}/${current_video}/crops_cache.pkl
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    format: pickle                                  # pickle (faster) or hdf5 (large datasets)
    crops_by_person_file: ${outputs_dir}/${current_video}/crops_by_person.pkl
    compression_level: 4                            # Only for HDF5 (ignored for pickle)

# ============================================================================
# Stage 5: Canonical Person Grouping (OLD - LEGACY)
# ============================================================================
# NOTE: Stage 5 is REPLACED by Stage 3b in the new pipeline
# Keep this config for backward compatibility and rollback
stage5:
  grouping:
    method: heuristic  # Options: heuristic, clustering, hybrid
    
    # Heuristic rules
    heuristic_criteria:
      max_temporal_gap: 30
      max_spatial_distance: 200
      area_ratio_range: [0.7, 1.3]
  
  input:
    # Auto-resolved based on stage4a.enabled
    recovered_tracklets_file: ${outputs_dir}/${current_video}/tracklets_recovered.npz
    tracklets_raw_file: ${outputs_dir}/${current_video}/tracklets_raw.npz
    tracklet_stats_file: ${outputs_dir}/${current_video}/tracklet_stats.npz
  
  output:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
    grouping_log_file: ${outputs_dir}/${current_video}/grouping_log.json

# ============================================================================
# Stage 6: Enrich Crops with Person IDs and Metadata (HDF5)
# ============================================================================
# Creates crops_enriched.h5 with fast indexed lookups
# Input: canonical_persons.npz + crops_cache.pkl
# Output: crops_enriched.h5 with structure: person_ID -> frame_ID -> {image_bgr, bbox, width, height}
stage6:
  
  enrichment:
    format: hdf5              # Output format (HDF5 for fast indexed lookups)
    color_space: bgr          # Store BGR (native OpenCV format)
    compression: gzip         # HDF5 compression level (gzip)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
    crops_cache_file: ${outputs_dir}/${current_video}/crops_cache.pkl
    detections_file: ${outputs_dir}/${current_video}/detections_raw.npz
  
  output:
    crops_enriched_file: ${outputs_dir}/${current_video}/crops_enriched.h5

# ============================================================================
# Stage 7: Rank Persons by Appearance Time (OLD - LEGACY)
# ============================================================================
# NOTE: Stage 7 is REPLACED by Stage 3c in the new pipeline
# Keep this config for backward compatibility and rollback
stage7:
  ranking:
    method: auto  # Options: auto (manual selection not yet implemented)
    
    # Auto-ranking weights
    weights:
      duration: 0.4       # Longest presence in video
      coverage: 0.3       # Percentage of frames covered
      center: 0.2         # Proximity to frame center
      smoothness: 0.1     # Motion stability (less jitter)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    primary_person_file: ${outputs_dir}/${current_video}/primary_person.npz
    ranking_report_file: ${outputs_dir}/${current_video}/ranking_report.json

# ============================================================================
# Stage 8: Visualize Grouping (Debug Only)
# ============================================================================
# To disable this stage, set pipeline.stages.stage8 to false
stage8:
  
  visualization:
    min_duration_seconds: 5    # Minimum appearance time to be included
    max_persons_shown: 10      # Show top 10 persons (or all if <10 qualify)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    video_file: ${outputs_dir}/${current_video}/top_persons_visualization.mp4

# ============================================================================
# Stage 9: Output Video Visualization
# ============================================================================
# To disable this stage, set pipeline.stages.stage9 to false
stage9:
  
  visualization:
    min_duration_seconds: 5    # Minimum appearance time to be included
    max_persons_shown: 10      # Show top 10 persons (or all if <10 qualify)
  
  input:
    canonical_persons_file: ${outputs_dir}/${current_video}/canonical_persons.npz
  
  output:
    video_file: ${outputs_dir}/${current_video}/top_persons_visualization.mp4

# ============================================================================
# Stage 10: Generate Person Animated WebPs (OLD - LEGACY)
# ============================================================================
# NOTE: Stage 10 is REPLACED by Stage 10b in the new pipeline
# Keep this config for backward compatibility and rollback
# OLD VERSION: 50+ lines of complex index conversion logic
stage10:
  
  video_generation:
    format: webp             # Output format (webp for auto-animating in HTML, smaller than GIF)
    fps: 10                  # Frames per second (10 fps = smooth playback at ~6 seconds)
    max_frames: 60           # Maximum frames per WebP (60 frames @ 10fps = 6 seconds)
    max_persons: 10          # Generate WebPs for top 10 persons
    frame_width: 128         # Reduced width for horizontal tape (3.5x smaller area)
    frame_height: 192        # Reduced height for horizontal tape
    quality: 80              # WebP quality (0-100, 80 is good balance)
  
  output:
    webp_dir: ${outputs_dir}/${current_video}/webp

# ============================================================================
# Stage 10b: Generate WebP Animations (NEW - SIMPLIFIED)
# ============================================================================
# Pure visualization - NO association logic!
# Uses pre-organized crops_by_person.pkl from Stage 4b
# DRAMATICALLY SIMPLIFIED: 20 lines vs 50+ lines in old Stage 10
stage10b:
  video_generation:
    fps: 10                  # Frames per second (10 fps = smooth playback at ~6 seconds)
    max_frames: 60           # Maximum frames per WebP (60 frames @ 10fps = 6 seconds)
    max_persons: 10          # Generate WebPs for top 10 persons
    frame_width: 128         # Reduced width for horizontal tape (3.5x smaller area)
    frame_height: 192        # Reduced height for horizontal tape
    quality: 80              # WebP quality (0-100, 80 is good balance)
  
  input:
    crops_by_person_file: ${outputs_dir}/${current_video}/crops_by_person.pkl
  
  output:
    webp_dir: ${outputs_dir}/${current_video}/webp

# ============================================================================
# Stage 11: HTML Selection Report
# ============================================================================
# To disable this stage, set pipeline.stages.stage11 to false
stage11:
  
  filters:
    min_duration_seconds: 5    # Minimum appearance time to be included
    max_persons_shown: 10      # Show top 10 persons (or all if <10 qualify)
  
  output:
    fullframe_grid: ${outputs_dir}/${current_video}/top10_persons_fullframe_grid.png
    cropped_grid: ${outputs_dir}/${current_video}/top10_persons_cropped_grid.png
    selection_info: ${outputs_dir}/${current_video}/selection_grid_info.json
