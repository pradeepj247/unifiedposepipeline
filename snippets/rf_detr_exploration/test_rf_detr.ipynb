{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5fc8df",
   "metadata": {},
   "source": [
    "# RF-DETR vs YOLO v8s - Head-to-Head Comparison\n",
    "\n",
    "Testing Roboflow's RF-DETR against YOLO v8s on **kohli_nets.mp4** to determine if RF-DETR should replace YOLO in our unified pose estimation pipeline.\n",
    "\n",
    "**Test Video**: kohli_nets.mp4\n",
    "- Already extensively tested with YOLO v8s (39.8 FPS baseline)\n",
    "- 1920x1080 @ 25 fps, 2027 frames, 81.08s duration\n",
    "- Cricket player detection scenario\n",
    "\n",
    "**Goals:**\n",
    "1. ‚ö° **Speed**: Does RF-DETR run faster than YOLO v8s?\n",
    "2. üéØ **Accuracy**: Does RF-DETR detect persons as well as YOLO?\n",
    "3. üîß **Integration**: Should we integrate RF-DETR into our pipeline?\n",
    "\n",
    "**Decision Criteria:**\n",
    "- ‚úÖ Integrate if: Faster AND comparable accuracy\n",
    "- ü§î Investigate if: Faster BUT different accuracy\n",
    "- ‚ùå Skip if: Slower or no clear advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone RF-DETR repository\n",
    "!git clone https://github.com/roboflow/rf-detr.git\n",
    "print(\"‚úÖ RF-DETR repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# RF-DETR models are typically available via transformers (HuggingFace)\n",
    "!pip install -q transformers torch torchvision opencv-python-headless pillow matplotlib tqdm ultralytics\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "print(\"   - transformers (for RT-DETR)\")\n",
    "print(\"   - ultralytics (for YOLO comparison)\")\n",
    "print(\"   - torch, opencv, matplotlib, tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19db368",
   "metadata": {},
   "source": [
    "## 2. Prepare Test Data - kohli_nets.mp4\n",
    "\n",
    "We'll use **kohli_nets.mp4** from Google Drive for head-to-head comparison:\n",
    "- Already tested extensively with YOLO v8s in our pipeline\n",
    "- Known baseline performance metrics\n",
    "- Good test case for person detection (cricket player)\n",
    "- Perfect for speed and accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access kohli_nets.mp4\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy kohli_nets.mp4 from Google Drive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create test data directories\n",
    "test_data_dir = Path('/content/test_data')\n",
    "test_data_dir.mkdir(exist_ok=True)\n",
    "(test_data_dir / 'videos').mkdir(exist_ok=True)\n",
    "(test_data_dir / 'outputs').mkdir(exist_ok=True)\n",
    "\n",
    "# Source video in Google Drive\n",
    "drive_video_path = Path('/content/drive/MyDrive/demo_data/videos/kohli_nets.mp4')\n",
    "local_video_path = test_data_dir / 'videos' / 'kohli_nets.mp4'\n",
    "\n",
    "# Copy video\n",
    "if drive_video_path.exists():\n",
    "    print(f\"üì• Copying kohli_nets.mp4 from Google Drive...\")\n",
    "    shutil.copy2(drive_video_path, local_video_path)\n",
    "    print(f\"‚úÖ Video copied to: {local_video_path}\")\n",
    "    \n",
    "    # Show video info\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(str(local_video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frames / fps\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"\\nüìπ Video Info:\")\n",
    "    print(f\"   Resolution: {width}x{height}\")\n",
    "    print(f\"   FPS: {fps:.2f}\")\n",
    "    print(f\"   Frames: {frames}\")\n",
    "    print(f\"   Duration: {duration:.2f}s\")\n",
    "else:\n",
    "    print(f\"‚ùå Video not found at: {drive_video_path}\")\n",
    "    print(f\"   Please ensure the video is in your Google Drive at: /MyDrive/samplevideos/kohli_nets.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b6cdb",
   "metadata": {},
   "source": [
    "## 3. Load RF-DETR Model\n",
    "\n",
    "**Note**: RF-DETR repository structure varies. The cell below explores the repo to find the correct import method. \n",
    "\n",
    "You may need to adjust the loading code based on the actual API (check README output below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76913a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore RF-DETR package to find correct API\n",
    "print(\"üì¶ Checking what's available in rfdetr package:\")\n",
    "\n",
    "import rfdetr\n",
    "print(f\"\\n‚úÖ Successfully imported rfdetr\")\n",
    "print(f\"   Package location: {rfdetr.__file__}\")\n",
    "\n",
    "print(\"\\nüìã Available attributes/functions in rfdetr:\")\n",
    "available = [name for name in dir(rfdetr) if not name.startswith('_')]\n",
    "for name in available:\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(\"\\nüìÑ Checking __init__.py contents:\")\n",
    "import inspect\n",
    "try:\n",
    "    source = inspect.getsource(rfdetr)\n",
    "    print(source[:1000])  # First 1000 chars\n",
    "except:\n",
    "    print(\"   (Could not get source)\")\n",
    "\n",
    "print(\"\\nüîç Checking for common model functions:\")\n",
    "for func_name in ['DETR', 'RFDETR', 'create_model', 'load_model', 'from_pretrained']:\n",
    "    if hasattr(rfdetr, func_name):\n",
    "        obj = getattr(rfdetr, func_name)\n",
    "        print(f\"   ‚úÖ Found: rfdetr.{func_name} -> {type(obj)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Not found: rfdetr.{func_name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Exploration complete! Check output above to see the correct API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead367eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RF-DETR model (CORRECT WAY - Native PyTorch with GPU)\n",
    "try:\n",
    "    import torch\n",
    "    from rfdetr import RFDETRSmall  # Using Small variant (3.52ms latency, ~284 FPS)\n",
    "    \n",
    "    print(\"üì• Installing rfdetr package...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rfdetr\"], check=True)\n",
    "    \n",
    "    print(\"‚úÖ rfdetr installed\")\n",
    "    print(\"\\nüì• Loading RF-DETR-Small model...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = RFDETRSmall()\n",
    "    \n",
    "    # Skip optimize_for_inference() - it has torch.jit.trace issues\n",
    "    # We'll still get GPU acceleration without it\n",
    "    \n",
    "    # Check GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n‚úÖ RF-DETR-Small model loaded successfully\")\n",
    "    print(f\"   Model: RF-DETR-Small (Native PyTorch)\")\n",
    "    print(f\"   Device: {device} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n",
    "    print(f\"   Expected latency: 3.52 ms (~284 FPS) with optimization\")\n",
    "    print(f\"   Current: No JIT optimization (may be slower, but still GPU-accelerated)\")\n",
    "    print(f\"   Resolution: 512x512\")\n",
    "    print(f\"   COCO AP50:95: 53.0\")\n",
    "    print(f\"\\nüí° API: model.predict(image, threshold=0.5)\")\n",
    "    print(f\"   Returns: supervision Detections object\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading RF-DETR model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba6a48",
   "metadata": {},
   "source": [
    "## 4. Test on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a855842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image_rfdetr(model, image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run RF-DETR detection on a single image using Roboflow inference API\n",
    "    \n",
    "    Args:\n",
    "        model: RF-DETR model from get_model()\n",
    "        image_path: Path to image\n",
    "        conf_threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        detections: List of detection dictionaries\n",
    "        inference_time: Inference time in seconds\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import time\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(str(image_path))\n",
    "    \n",
    "    # Run inference using Roboflow's API\n",
    "    start_time = time.time()\n",
    "    predictions = model.infer(image, confidence=conf_threshold)[0]\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Parse results from Roboflow inference format\n",
    "    detections = []\n",
    "    for pred in predictions.predictions:\n",
    "        detections.append({\n",
    "            'bbox': [pred.x - pred.width/2, pred.y - pred.height/2, \n",
    "                    pred.x + pred.width/2, pred.y + pred.height/2],  # Convert center to corners\n",
    "            'confidence': pred.confidence,\n",
    "            'class_id': pred.class_id,\n",
    "            'class': pred.class_name\n",
    "        })\n",
    "    \n",
    "    return detections, inference_time\n",
    "\n",
    "\n",
    "def visualize_detections(image_path, detections, person_only=True):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on image\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    count = 0\n",
    "    for det in detections:\n",
    "        # Filter for person class if needed (class_id=1 in RF-DETR)\n",
    "        if person_only and det['class_id'] != 1:\n",
    "            continue\n",
    "        \n",
    "        x1, y1, x2, y2 = map(int, det['bbox'])\n",
    "        conf = det['confidence']\n",
    "        cls = det['class']\n",
    "        \n",
    "        # Draw box\n",
    "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(img_rgb, f\"{cls}: {conf:.2f}\", (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        count += 1\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{'Person ' if person_only else ''}Detections: {count}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RF-DETR on a single frame from kohli_nets.mp4\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Define paths\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "test_frame_path = Path('/content/test_data/test_frame.jpg')\n",
    "\n",
    "print(f\"üìÅ Checking paths...\")\n",
    "print(f\"   Video exists: {video_path.exists()}\")\n",
    "print(f\"   Video path: {video_path}\")\n",
    "print(f\"   Model loaded: {'model' in locals()}\")\n",
    "\n",
    "if video_path.exists() and 'model' in locals():\n",
    "    # Extract frame 500 for testing\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 500)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        # Save frame\n",
    "        cv2.imwrite(str(test_frame_path), frame)\n",
    "        print(f\"‚úÖ Extracted frame 500, saved to: {test_frame_path}\")\n",
    "        \n",
    "        # Run RF-DETR detection\n",
    "        print(f\"\\nüñºÔ∏è  Testing RF-DETR on frame 500 from kohli_nets.mp4...\")\n",
    "        detections, inf_time = detect_image_rfdetr(model, test_frame_path, conf_threshold=0.5)\n",
    "        \n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"   Inference time: {inf_time*1000:.2f} ms\")\n",
    "        print(f\"   FPS: {1/inf_time:.1f}\")\n",
    "        print(f\"   Total detections: {len(detections)}\")\n",
    "        print(f\"   Person detections: {len([d for d in detections if d['class_id'] == 1])}\")\n",
    "        \n",
    "        # Visualize person detections only\n",
    "        print(f\"\\nüì∏ Visualizing detections...\")\n",
    "        vis_img = visualize_detections(test_frame_path, detections, person_only=True)\n",
    "    else:\n",
    "        print(\"‚ùå Failed to extract frame from video\")\n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found at: {video_path}\")\n",
    "        print(f\"   Run Cell 8 to copy the video from Google Drive\")\n",
    "    if 'model' not in locals():\n",
    "        print(f\"‚ùå RF-DETR model not loaded\")\n",
    "        print(f\"   Run Cell 11 to load the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check what class IDs RF-DETR is returning\n",
    "print(\"üîç Debugging detection class IDs:\")\n",
    "print(f\"   Total detections: {len(detections)}\")\n",
    "print(f\"\\nüìã Detection details:\")\n",
    "for i, det in enumerate(detections):\n",
    "    print(f\"   Detection {i+1}:\")\n",
    "    print(f\"      Class ID: {det['class_id']}\")\n",
    "    print(f\"      Class name: {det['class']}\")\n",
    "    print(f\"      Confidence: {det['confidence']:.3f}\")\n",
    "    print(f\"      BBox: [{det['bbox'][0]:.1f}, {det['bbox'][1]:.1f}, {det['bbox'][2]:.1f}, {det['bbox'][3]:.1f}]\")\n",
    "    print()\n",
    "\n",
    "# Count by class\n",
    "from collections import Counter\n",
    "class_counts = Counter([det['class'] for det in detections])\n",
    "print(f\"üìä Detection counts by class:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"   {class_name}: {count}\")\n",
    "\n",
    "# Visualize ALL detections (not just persons)\n",
    "print(f\"\\nüì∏ Visualizing ALL detections (person_only=False)...\")\n",
    "vis_img = visualize_detections(test_frame_path, detections, person_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61828a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed test - RF-DETR on GPU (CORRECT API)\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'model' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50  # Test on 50 frames\n",
    "    \n",
    "    inference_times = []\n",
    "    person_counts = []\n",
    "    \n",
    "    print(f\"üöÄ RF-DETR Speed Test (GPU - Native PyTorch)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Video: {video_path.name}\")\n",
    "    print(f\"Frames to process: {total_frames}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Time inference using .predict() method (GPU-accelerated)\n",
    "        start = time.time()\n",
    "        detections = model.predict(pil_image, threshold=0.5)\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        # Count persons (class_id in detections)\n",
    "        if hasattr(detections, 'class_id'):\n",
    "            # RF-DETR uses class_id=1 for person (not standard COCO 0)\n",
    "            person_count = sum(1 for cid in detections.class_id if cid == 1)\n",
    "        else:\n",
    "            person_count = len(detections)\n",
    "        \n",
    "        person_counts.append(person_count)\n",
    "        \n",
    "        # Debug first frame\n",
    "        if frame_idx == 0:\n",
    "            print(f\"\\nüîç First frame debug:\")\n",
    "            print(f\"   Detection type: {type(detections)}\")\n",
    "            print(f\"   Has class_id: {hasattr(detections, 'class_id')}\")\n",
    "            if hasattr(detections, 'class_id'):\n",
    "                print(f\"   Total detections: {len(detections.class_id)}\")\n",
    "                print(f\"   Class IDs: {detections.class_id}\")\n",
    "                print(f\"   Persons (class=1): {person_count}\")\n",
    "            print()\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Progress every 10 frames\n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    avg_persons = np.mean(person_counts)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä RF-DETR GPU Performance Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total frames processed: {frame_idx}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Std dev: {std_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    print(f\"Avg persons detected: {avg_persons:.2f}\")\n",
    "    print(f\"\\nüéØ Target (from README): 3.52 ms (~284 FPS)\")\n",
    "    print(f\"   Actual: {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'model' not in locals():\n",
    "        print(f\"‚ùå Model not loaded. Run Cell 11 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576629e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with pre-resized 512x512 images (RF-DETR's native resolution)\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'model' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50\n",
    "    \n",
    "    inference_times = []\n",
    "    person_counts = []\n",
    "    \n",
    "    print(f\"üöÄ RF-DETR Speed Test with 512x512 Resize\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Testing if image size is the bottleneck...\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize to 512x512 BEFORE inference (RF-DETR's native size)\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.time()\n",
    "        detections = model.predict(pil_image, threshold=0.5)\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        # Count persons\n",
    "        if hasattr(detections, 'class_id'):\n",
    "            person_count = sum(1 for cid in detections.class_id if cid == 1)\n",
    "        else:\n",
    "            person_count = len(detections)\n",
    "        \n",
    "        person_counts.append(person_count)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    avg_persons = np.mean(person_counts)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä RF-DETR with 512x512 Resize Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    print(f\"Avg persons detected: {avg_persons:.2f}\")\n",
    "    print(f\"\\nüéØ Comparison:\")\n",
    "    print(f\"   Target (README): 3.52 ms (~284 FPS)\")\n",
    "    print(f\"   1920x1080 input: 89.47 ms (11.18 FPS)\")\n",
    "    print(f\"   512x512 input:   {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    \n",
    "    if avg_fps > 11.18:\n",
    "        speedup = ((avg_fps - 11.18) / 11.18) * 100\n",
    "        print(f\"\\n‚úÖ Resize helped: {speedup:+.1f}% faster!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Resize didn't help - bottleneck is elsewhere\")\n",
    "    \n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'model' not in locals():\n",
    "        print(f\"‚ùå Model not loaded. Run Cell 11 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c590eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare detection quality: 1920x1080 vs 512x512\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'model' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Test on frame 500 (same frame we tested earlier)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 500)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        print(f\"üîç Detection Quality Comparison - Frame 500\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Original resolution (1920x1080)\n",
    "        frame_rgb_full = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_full = Image.fromarray(frame_rgb_full)\n",
    "        detections_full = model.predict(pil_full, threshold=0.5)\n",
    "        \n",
    "        # Resized (512x512)\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb_512 = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        pil_512 = Image.fromarray(frame_rgb_512)\n",
    "        detections_512 = model.predict(pil_512, threshold=0.5)\n",
    "        \n",
    "        # Compare detections\n",
    "        if hasattr(detections_full, 'class_id') and hasattr(detections_512, 'class_id'):\n",
    "            persons_full = sum(1 for cid in detections_full.class_id if cid == 1)\n",
    "            persons_512 = sum(1 for cid in detections_512.class_id if cid == 1)\n",
    "            \n",
    "            print(f\"üìä Detection Counts:\")\n",
    "            print(f\"   1920x1080: {persons_full} persons, {len(detections_full.class_id)} total\")\n",
    "            print(f\"   512x512:   {persons_512} persons, {len(detections_512.class_id)} total\")\n",
    "            \n",
    "            # Get confidences for persons\n",
    "            conf_full = [detections_full.confidence[i] for i, cid in enumerate(detections_full.class_id) if cid == 1]\n",
    "            conf_512 = [detections_512.confidence[i] for i, cid in enumerate(detections_512.class_id) if cid == 1]\n",
    "            \n",
    "            print(f\"\\nüìà Confidence Stats (persons only):\")\n",
    "            print(f\"   1920x1080: avg={np.mean(conf_full):.3f}, min={np.min(conf_full):.3f}, max={np.max(conf_full):.3f}\")\n",
    "            print(f\"   512x512:   avg={np.mean(conf_512):.3f}, min={np.min(conf_512):.3f}, max={np.max(conf_512):.3f}\")\n",
    "            \n",
    "            # Visual comparison\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "            \n",
    "            # 1920x1080 detections\n",
    "            img_full_vis = frame_rgb_full.copy()\n",
    "            for i, cid in enumerate(detections_full.class_id):\n",
    "                if cid == 1:  # person\n",
    "                    bbox = detections_full.xyxy[i]\n",
    "                    x1, y1, x2, y2 = map(int, bbox)\n",
    "                    conf = detections_full.confidence[i]\n",
    "                    cv2.rectangle(img_full_vis, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "                    cv2.putText(img_full_vis, f\"{conf:.2f}\", (x1, y1-10),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # 512x512 detections (scale bboxes back to 1920x1080 for comparison)\n",
    "            img_512_vis = frame_rgb_full.copy()\n",
    "            scale_x = frame.shape[1] / 512\n",
    "            scale_y = frame.shape[0] / 512\n",
    "            for i, cid in enumerate(detections_512.class_id):\n",
    "                if cid == 1:  # person\n",
    "                    bbox = detections_512.xyxy[i]\n",
    "                    # Scale bbox back to original resolution\n",
    "                    x1 = int(bbox[0] * scale_x)\n",
    "                    y1 = int(bbox[1] * scale_y)\n",
    "                    x2 = int(bbox[2] * scale_x)\n",
    "                    y2 = int(bbox[3] * scale_y)\n",
    "                    conf = detections_512.confidence[i]\n",
    "                    cv2.rectangle(img_512_vis, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "                    cv2.putText(img_512_vis, f\"{conf:.2f}\", (x1, y1-10),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "            \n",
    "            ax1.imshow(img_full_vis)\n",
    "            ax1.set_title(f'1920x1080 Input\\n{persons_full} persons | 11.18 FPS', fontsize=14)\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            ax2.imshow(img_512_vis)\n",
    "            ax2.set_title(f'512x512 Input (bboxes scaled back)\\n{persons_512} persons | 19.22 FPS', fontsize=14)\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Verdict\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"üéØ Quality Assessment:\")\n",
    "            if persons_full == persons_512:\n",
    "                print(f\"   ‚úÖ Detection count SAME ({persons_full} persons)\")\n",
    "            else:\n",
    "                diff = abs(persons_full - persons_512)\n",
    "                print(f\"   ‚ö†Ô∏è  Detection count differs by {diff} person(s)\")\n",
    "            \n",
    "            conf_diff = abs(np.mean(conf_full) - np.mean(conf_512))\n",
    "            if conf_diff < 0.05:\n",
    "                print(f\"   ‚úÖ Confidence scores similar (Œî={conf_diff:.3f})\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Confidence scores differ (Œî={conf_diff:.3f})\")\n",
    "            \n",
    "            print(f\"\\nüí° Visual inspection needed:\")\n",
    "            print(f\"   - Check if bboxes overlap well (green=1920x1080, blue=512x512)\")\n",
    "            print(f\"   - Verify no persons missed in 512x512 version\")\n",
    "            print(f\"   - Assess if bbox quality is acceptable for pose estimation\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to read frame 500\")\n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'model' not in locals():\n",
    "        print(f\"‚ùå Model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9ac2a",
   "metadata": {},
   "source": [
    "## üìä RF-DETR Investigation Summary\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "| Configuration | Avg FPS | Avg Inference Time | Persons Detected | Quality |\n",
    "|---------------|---------|-------------------|------------------|---------|\n",
    "| **1920x1080 input** | 11.18 | 89.47 ms | 3.72 | ‚úÖ Baseline |\n",
    "| **512x512 input** | 19.22 | 52.04 ms | 3.72 | ‚úÖ Same quality |\n",
    "| **Target (README)** | 284.00 | 3.52 ms | - | üéØ Advertised |\n",
    "| **YOLO v8s baseline** | 39.80 | 25.00 ms | - | üèÜ Current |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**‚úÖ Wins:**\n",
    "- 512x512 resize improves speed by **+71.9%** (11.18 ‚Üí 19.22 FPS)\n",
    "- Detection quality maintained (same count, Œî=0.003 confidence)\n",
    "- Bbox quality visually acceptable\n",
    "\n",
    "**‚ùå Problems:**\n",
    "- Still **51.7% SLOWER** than YOLO v8s (19.22 vs 39.8 FPS)\n",
    "- **14.8x slower** than advertised 284 FPS (missing JIT optimization)\n",
    "- Would **slow down** our pipeline vs current YOLO implementation\n",
    "\n",
    "### Integration Decision: ‚ùå **DO NOT INTEGRATE**\n",
    "\n",
    "**Reasons:**\n",
    "1. **Performance**: Even with 512x512 optimization, RF-DETR is 2x slower than YOLO\n",
    "2. **Reliability**: Cannot achieve advertised speeds (JIT optimization broken)\n",
    "3. **Risk**: No clear advantage to justify replacing proven YOLO pipeline\n",
    "4. **Class ID quirk**: Uses class_id=1 for person (non-standard)\n",
    "\n",
    "**Conclusion:**\n",
    "RF-DETR-Small on T4 GPU cannot match YOLO v8s performance. The advertised 284 FPS requires JIT optimization which fails with torch.jit.trace error. Even with optimized 512x512 input, RF-DETR achieves only 19.22 FPS compared to YOLO's 39.8 FPS baseline.\n",
    "\n",
    "**Recommendation:** **Stick with YOLO v8s** - proven, faster, and well-integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2638a",
   "metadata": {},
   "source": [
    "## üîç Deep Dive: GPU Utilization & Optimization Investigation\n",
    "\n",
    "Before giving up, let's verify:\n",
    "1. ‚úÖ Is the model actually running on GPU?\n",
    "2. ‚úÖ Are we using the correct inference API?\n",
    "3. ‚úÖ What optimizations are available?\n",
    "4. ‚úÖ Can we enable TensorRT/ONNX export?\n",
    "5. ‚úÖ Are we missing batch inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa27984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep GPU diagnostic - Check if RF-DETR is ACTUALLY using GPU\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "print(\"üîç GPU Utilization Diagnostic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check model device placement\n",
    "if 'model' in locals():\n",
    "    print(\"\\n1Ô∏è‚É£ Model Device Check:\")\n",
    "    print(f\"   Model type: {type(model)}\")\n",
    "    print(f\"   Model class: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Check if model has device attribute\n",
    "    if hasattr(model, 'model'):\n",
    "        print(f\"   Has nested .model: Yes\")\n",
    "        if hasattr(model.model, 'device'):\n",
    "            print(f\"   Nested model device: {model.model.device}\")\n",
    "        \n",
    "        # Check model parameters\n",
    "        if hasattr(model.model, 'parameters'):\n",
    "            params = list(model.model.parameters())\n",
    "            if params:\n",
    "                print(f\"   First parameter device: {params[0].device}\")\n",
    "                print(f\"   First parameter dtype: {params[0].dtype}\")\n",
    "    \n",
    "    # Try to get device info\n",
    "    print(\"\\n2Ô∏è‚É£ Checking model internals:\")\n",
    "    for attr in ['device', '_device', 'model_device']:\n",
    "        if hasattr(model, attr):\n",
    "            print(f\"   model.{attr}: {getattr(model, attr)}\")\n",
    "\n",
    "# 2. GPU Memory Usage Test\n",
    "print(\"\\n3Ô∏è‚É£ GPU Memory Test:\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    print(f\"   Initial GPU memory: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Load test image\n",
    "    video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "    if video_path.exists() and 'model' in locals():\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if ret:\n",
    "            # Run inference and monitor GPU memory\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            before_inference = torch.cuda.memory_allocated() / 1024**2\n",
    "            print(f\"   Before inference: {before_inference:.2f} MB\")\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.cuda.amp.autocast():  # Try mixed precision\n",
    "                detections = model.predict(pil_image, threshold=0.5)\n",
    "            \n",
    "            after_inference = torch.cuda.memory_allocated() / 1024**2\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            \n",
    "            print(f\"   After inference: {after_inference:.2f} MB\")\n",
    "            print(f\"   Peak GPU memory: {peak_memory:.2f} MB\")\n",
    "            print(f\"   Memory used: {peak_memory - initial_memory:.2f} MB\")\n",
    "            \n",
    "            if peak_memory - initial_memory < 10:\n",
    "                print(\"\\n   ‚ö†Ô∏è  WARNING: Very low GPU memory usage!\")\n",
    "                print(\"   This suggests the model might be running on CPU!\")\n",
    "            else:\n",
    "                print(\"\\n   ‚úÖ GPU memory usage detected - model is on GPU\")\n",
    "\n",
    "# 3. Check CUDA operations\n",
    "print(\"\\n4Ô∏è‚É£ CUDA Operations Check:\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   PyTorch compiled with CUDA: {torch.backends.cudnn.enabled}\")\n",
    "\n",
    "# 4. Check rfdetr package source\n",
    "print(\"\\n5Ô∏è‚É£ RF-DETR Package Info:\")\n",
    "import rfdetr\n",
    "print(f\"   Package location: {rfdetr.__file__}\")\n",
    "print(f\"   Available models: {[name for name in dir(rfdetr) if 'DETR' in name]}\")\n",
    "\n",
    "# Check if there's a .to() method\n",
    "if 'model' in locals():\n",
    "    print(f\"\\n6Ô∏è‚É£ Model Methods:\")\n",
    "    print(f\"   Has .to() method: {hasattr(model, 'to')}\")\n",
    "    print(f\"   Has .cuda() method: {hasattr(model, 'cuda')}\")\n",
    "    print(f\"   Has .eval() method: {hasattr(model, 'eval')}\")\n",
    "    print(f\"   Has .half() method: {hasattr(model, 'half')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to explicitly move model to GPU and optimize\n",
    "print(\"üöÄ Attempting GPU Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'model' in locals():\n",
    "    import torch\n",
    "    \n",
    "    # Option 1: Try .to() method\n",
    "    if hasattr(model, 'to'):\n",
    "        print(\"\\n1Ô∏è‚É£ Trying model.to('cuda')...\")\n",
    "        try:\n",
    "            model = model.to('cuda')\n",
    "            print(\"   ‚úÖ Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Option 2: Try nested model.model.to()\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'to'):\n",
    "        print(\"\\n2Ô∏è‚É£ Trying model.model.to('cuda')...\")\n",
    "        try:\n",
    "            model.model = model.model.to('cuda')\n",
    "            print(\"   ‚úÖ Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Option 3: Try .cuda() method\n",
    "    if hasattr(model, 'cuda'):\n",
    "        print(\"\\n3Ô∏è‚É£ Trying model.cuda()...\")\n",
    "        try:\n",
    "            model = model.cuda()\n",
    "            print(\"   ‚úÖ Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Option 4: Set eval mode for inference\n",
    "    if hasattr(model, 'eval'):\n",
    "        print(\"\\n4Ô∏è‚É£ Setting model.eval() mode...\")\n",
    "        try:\n",
    "            model.eval()\n",
    "            print(\"   ‚úÖ Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Option 5: Try half precision (FP16)\n",
    "    if hasattr(model, 'half'):\n",
    "        print(\"\\n5Ô∏è‚É£ Trying FP16 (half precision)...\")\n",
    "        try:\n",
    "            model = model.half()\n",
    "            print(\"   ‚úÖ Success! Model converted to FP16\")\n",
    "            print(\"   Expected speedup: 1.5-2x\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Option 6: Check for compilation options\n",
    "    print(\"\\n6Ô∏è‚É£ Checking torch.compile() availability...\")\n",
    "    if hasattr(torch, 'compile'):\n",
    "        print(\"   ‚úÖ torch.compile() available (PyTorch 2.0+)\")\n",
    "        print(\"   Attempting compilation...\")\n",
    "        try:\n",
    "            if hasattr(model, 'model'):\n",
    "                model.model = torch.compile(model.model, mode='max-autotune')\n",
    "                print(\"   ‚úÖ Model compiled with max-autotune!\")\n",
    "                print(\"   Expected speedup: 2-3x\")\n",
    "            else:\n",
    "                model = torch.compile(model, mode='max-autotune')\n",
    "                print(\"   ‚úÖ Model compiled with max-autotune!\")\n",
    "                print(\"   Expected speedup: 2-3x\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Compilation failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  torch.compile() not available (need PyTorch 2.0+)\")\n",
    "    \n",
    "    # Option 7: Check CUDNN settings\n",
    "    print(\"\\n7Ô∏è‚É£ CUDNN Optimization:\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    print(\"   ‚úÖ CUDNN benchmark enabled\")\n",
    "    print(\"   ‚úÖ CUDNN deterministic disabled (faster)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Now rerun the speed test to see if optimizations helped!\")\n",
    "print(\"   Run the 512x512 speed test cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed test with ALL optimizations enabled\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'model' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50\n",
    "    \n",
    "    inference_times = []\n",
    "    person_counts = []\n",
    "    \n",
    "    print(f\"üöÄ RF-DETR Speed Test - OPTIMIZED (512x512 + GPU + FP16 + Compile)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Optimizations applied:\")\n",
    "    print(f\"   ‚úÖ Input: 512x512 (native resolution)\")\n",
    "    print(f\"   ‚úÖ Device: GPU (explicit placement)\")\n",
    "    print(f\"   ‚úÖ Mode: eval() for inference\")\n",
    "    print(f\"   ‚úÖ Precision: FP16 (if supported)\")\n",
    "    print(f\"   ‚úÖ CUDNN: benchmark enabled\")\n",
    "    print(f\"   ‚úÖ Compile: torch.compile() (if available)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Warmup runs (important for GPU optimization)\n",
    "    print(\"üî• Warmup runs (3 frames)...\")\n",
    "    for _ in range(3):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_resized = cv2.resize(frame, (512, 512))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                _ = model.predict(pil_image, threshold=0.5)\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to start\n",
    "    print(\"‚úÖ Warmup complete\\n\")\n",
    "    \n",
    "    # Actual timing runs\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize to 512x512\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Time inference with torch.no_grad() for speed\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            detections = model.predict(pil_image, threshold=0.5)\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        # Count persons\n",
    "        if hasattr(detections, 'class_id'):\n",
    "            person_count = sum(1 for cid in detections.class_id if cid == 1)\n",
    "        else:\n",
    "            person_count = len(detections)\n",
    "        \n",
    "        person_counts.append(person_count)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    avg_persons = np.mean(person_counts)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä OPTIMIZED RF-DETR Performance Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    print(f\"Avg persons detected: {avg_persons:.2f}\")\n",
    "    print(f\"\\nüéØ Performance Comparison:\")\n",
    "    print(f\"   Target (README): 3.52 ms (~284 FPS)\")\n",
    "    print(f\"   Baseline (1920x1080): 89.47 ms (11.18 FPS)\")\n",
    "    print(f\"   512x512 unoptimized: 52.04 ms (19.22 FPS)\")\n",
    "    print(f\"   512x512 OPTIMIZED:   {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    print(f\"\\nüèÜ vs YOLO v8s:\")\n",
    "    print(f\"   YOLO: 25.00 ms (39.8 FPS)\")\n",
    "    print(f\"   RF-DETR: {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    \n",
    "    if avg_fps > 39.8:\n",
    "        speedup = ((avg_fps - 39.8) / 39.8) * 100\n",
    "        print(f\"   ‚úÖ RF-DETR is {speedup:+.1f}% FASTER than YOLO!\")\n",
    "    elif avg_fps > 35:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   üü° RF-DETR is {diff:.1f}% slower (close!)\")\n",
    "    else:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   ‚ùå RF-DETR is {diff:.1f}% slower than YOLO\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'model' not in locals():\n",
    "        print(f\"‚ùå Model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbf5e1",
   "metadata": {},
   "source": [
    "## üöÄ ONNX Model Testing - Maximum Speed\n",
    "\n",
    "ONNX Runtime typically achieves **2-5x faster** inference than native PyTorch!\n",
    "\n",
    "**Benefits:**\n",
    "- Optimized computational graph\n",
    "- GPU-accelerated with TensorRT execution provider\n",
    "- Lower memory footprint\n",
    "- Better batch processing\n",
    "\n",
    "Let's download and test the ONNX RF-DETR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ONNX Runtime with GPU support\n",
    "!pip install -q onnxruntime-gpu onnx\n",
    "\n",
    "print(\"‚úÖ ONNX Runtime GPU installed\")\n",
    "print(\"\\nüì¶ Checking available execution providers:\")\n",
    "\n",
    "import onnxruntime as ort\n",
    "print(f\"   Available providers: {ort.get_available_providers()}\")\n",
    "print(f\"\\nüí° Looking for: CUDAExecutionProvider or TensorrtExecutionProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RF-DETR ONNX model\n",
    "# Please provide the link URL in the next cell\n",
    "# For now, let's prepare the download structure\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('/content/models/rf_detr_onnx')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Models directory created: {models_dir}\")\n",
    "print(f\"\\nüí° Please provide the ONNX model link and we'll download it!\")\n",
    "print(f\"\\nExpected files:\")\n",
    "print(f\"   - rf-detr-small.onnx\")\n",
    "print(f\"   - rf-detr-medium.onnx\")\n",
    "print(f\"   - rf-detr-large.onnx\")\n",
    "print(f\"\\nüîó What's the link you found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe62365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ONNX model from provided URL\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(destination, 'wb') as f, tqdm(\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        desc=destination.name\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    return destination\n",
    "\n",
    "# Replace this URL with the one you found\n",
    "ONNX_MODEL_URL = \"PASTE_YOUR_LINK_HERE\"  # TODO: Update with actual URL\n",
    "\n",
    "models_dir = Path('/content/models/rf_detr_onnx')\n",
    "onnx_path = models_dir / 'rf-detr-small.onnx'\n",
    "\n",
    "print(f\"üì• Downloading RF-DETR ONNX model...\")\n",
    "print(f\"   URL: {ONNX_MODEL_URL}\")\n",
    "print(f\"   Destination: {onnx_path}\\n\")\n",
    "\n",
    "if ONNX_MODEL_URL != \"PASTE_YOUR_LINK_HERE\":\n",
    "    try:\n",
    "        download_file(ONNX_MODEL_URL, onnx_path)\n",
    "        print(f\"\\n‚úÖ Download complete!\")\n",
    "        print(f\"   File size: {onnx_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Download failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please update ONNX_MODEL_URL with the link you found\")\n",
    "    print(\"   Example: https://huggingface.co/.../rf-detr-small.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b458965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model with GPU acceleration\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path('/content/models/rf_detr_onnx/rf-detr-small.onnx')\n",
    "\n",
    "if onnx_path.exists():\n",
    "    print(f\"üì• Loading ONNX model: {onnx_path.name}\")\n",
    "    print(f\"   File size: {onnx_path.stat().st_size / 1024**2:.2f} MB\\n\")\n",
    "    \n",
    "    # Set up session options for maximum performance\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    sess_options.intra_op_num_threads = 4\n",
    "    \n",
    "    # Choose execution provider (GPU preferred)\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    \n",
    "    print(f\"üöÄ Creating ONNX Runtime session...\")\n",
    "    print(f\"   Providers: {providers}\")\n",
    "    \n",
    "    try:\n",
    "        onnx_session = ort.InferenceSession(\n",
    "            str(onnx_path),\n",
    "            sess_options=sess_options,\n",
    "            providers=providers\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ONNX session created successfully!\")\n",
    "        print(f\"   Active provider: {onnx_session.get_providers()[0]}\")\n",
    "        \n",
    "        # Get model info\n",
    "        print(f\"\\nüìã Model Input Info:\")\n",
    "        for inp in onnx_session.get_inputs():\n",
    "            print(f\"   Name: {inp.name}\")\n",
    "            print(f\"   Shape: {inp.shape}\")\n",
    "            print(f\"   Type: {inp.type}\")\n",
    "        \n",
    "        print(f\"\\nüìã Model Output Info:\")\n",
    "        for out in onnx_session.get_outputs():\n",
    "            print(f\"   Name: {out.name}\")\n",
    "            print(f\"   Shape: {out.shape}\")\n",
    "            print(f\"   Type: {out.type}\")\n",
    "        \n",
    "        print(f\"\\nüí° Model loaded and ready for inference!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load ONNX model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå ONNX model not found at: {onnx_path}\")\n",
    "    print(f\"   Please download it first using the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX Speed Test - 512x512 input\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'onnx_session' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50\n",
    "    \n",
    "    inference_times = []\n",
    "    \n",
    "    print(f\"üöÄ RF-DETR ONNX Speed Test (GPU)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"   Model: RF-DETR-Small (ONNX)\")\n",
    "    print(f\"   Provider: {onnx_session.get_providers()[0]}\")\n",
    "    print(f\"   Input size: 512x512\")\n",
    "    print(f\"   Frames: {total_frames}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Get input details\n",
    "    input_name = onnx_session.get_inputs()[0].name\n",
    "    input_shape = onnx_session.get_inputs()[0].shape\n",
    "    print(f\"Model expects input: {input_name} with shape {input_shape}\\n\")\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"üî• Warmup (5 frames)...\")\n",
    "    for _ in range(5):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Preprocess\n",
    "            frame_resized = cv2.resize(frame, (512, 512))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Normalize to [0, 1] and convert to CHW format\n",
    "            img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "            img_chw = np.transpose(img_normalized, (2, 0, 1))  # HWC -> CHW\n",
    "            img_batch = np.expand_dims(img_chw, axis=0)  # Add batch dimension\n",
    "            \n",
    "            # Run inference\n",
    "            _ = onnx_session.run(None, {input_name: img_batch})\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    print(\"‚úÖ Warmup complete\\n\")\n",
    "    \n",
    "    # Actual speed test\n",
    "    frame_idx = 0\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "        img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "        img_batch = np.expand_dims(img_chw, axis=0)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.time()\n",
    "        outputs = onnx_session.run(None, {input_name: img_batch})\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RF-DETR ONNX Performance Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Std dev: {std_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Comparison:\")\n",
    "    print(f\"   Target (README): 3.52 ms (~284 FPS)\")\n",
    "    print(f\"   PyTorch (1920x1080): 89.47 ms (11.18 FPS)\")\n",
    "    print(f\"   PyTorch (512x512): 52.04 ms (19.22 FPS)\")\n",
    "    print(f\"   ONNX (512x512):    {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    \n",
    "    print(f\"\\nüèÜ vs YOLO v8s Baseline:\")\n",
    "    print(f\"   YOLO v8s: 25.00 ms (39.8 FPS)\")\n",
    "    print(f\"   RF-DETR ONNX: {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    \n",
    "    if avg_fps > 39.8:\n",
    "        speedup = ((avg_fps - 39.8) / 39.8) * 100\n",
    "        print(f\"   üéâ RF-DETR is {speedup:+.1f}% FASTER! ‚úÖ\")\n",
    "    elif avg_fps > 35:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   üü° RF-DETR is {diff:.1f}% slower (competitive)\")\n",
    "    else:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   ‚ùå RF-DETR is {diff:.1f}% slower\")\n",
    "    \n",
    "    # Speedup vs PyTorch\n",
    "    pytorch_fps = 19.22\n",
    "    onnx_speedup = ((avg_fps - pytorch_fps) / pytorch_fps) * 100\n",
    "    print(f\"\\n‚ö° ONNX vs PyTorch speedup: {onnx_speedup:+.1f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'onnx_session' not in locals():\n",
    "        print(f\"‚ùå ONNX session not loaded\")\n",
    "        print(f\"   Load the ONNX model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57006771",
   "metadata": {},
   "source": [
    "## üîß ONNX Further Optimizations\n",
    "\n",
    "Current: **34.85 FPS** (12.4% slower than YOLO's 39.8 FPS)\n",
    "\n",
    "**Issues noticed:**\n",
    "- High std dev (14.20 ms) ‚Üí inconsistent performance\n",
    "- Still 8x slower than advertised (284 FPS)\n",
    "- Not using TensorRT execution provider\n",
    "\n",
    "**Optimizations to try:**\n",
    "1. ‚úÖ Enable TensorRT execution provider (2-3x faster)\n",
    "2. ‚úÖ Use batch inference instead of single frame\n",
    "3. ‚úÖ Pin memory and use CUDA streams\n",
    "4. ‚úÖ Reduce precision to FP16\n",
    "5. ‚úÖ Pre-allocate output buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload ONNX with TensorRT and maximum optimizations\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path('/content/models/rf_detr_onnx/rf-detr-small.onnx')\n",
    "\n",
    "if onnx_path.exists():\n",
    "    print(\"üöÄ Creating OPTIMIZED ONNX session with TensorRT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Advanced session options\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "    sess_options.intra_op_num_threads = 1  # GPU execution, don't need many threads\n",
    "    sess_options.inter_op_num_threads = 1\n",
    "    \n",
    "    # Enable memory optimizations\n",
    "    sess_options.enable_mem_pattern = True\n",
    "    sess_options.enable_cpu_mem_arena = True\n",
    "    \n",
    "    # Use CUDA with maximum optimizations (TensorRT not available in this environment)\n",
    "    providers = [\n",
    "        ('CUDAExecutionProvider', {\n",
    "            'device_id': 0,\n",
    "            'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "            'gpu_mem_limit': 4 * 1024 * 1024 * 1024,  # 4GB\n",
    "            'cudnn_conv_algo_search': 'EXHAUSTIVE',  # Find fastest convolution algorithm\n",
    "            'do_copy_in_default_stream': True,\n",
    "            'cudnn_conv_use_max_workspace': '1',\n",
    "            'cudnn_conv1d_pad_to_nc1d': '1',\n",
    "        }),\n",
    "        'CPUExecutionProvider'\n",
    "    ]\n",
    "    \n",
    "    print(\"Provider configuration:\")\n",
    "    for i, p in enumerate(providers, 1):\n",
    "        if isinstance(p, tuple):\n",
    "            print(f\"   {i}. {p[0]} (with optimizations)\")\n",
    "        else:\n",
    "            print(f\"   {i}. {p}\")\n",
    "    print(\"   ‚ö†Ô∏è  TensorRT not available in this Colab environment\")\n",
    "    print(\"   ‚úÖ Using CUDA with EXHAUSTIVE convolution search\\n\")\n",
    "    \n",
    "    try:\n",
    "        onnx_session_optimized = ort.InferenceSession(\n",
    "            str(onnx_path),\n",
    "            sess_options=sess_options,\n",
    "            providers=providers\n",
    "        )\n",
    "        \n",
    "        active_provider = onnx_session_optimized.get_providers()[0]\n",
    "        print(f\"‚úÖ Session created successfully!\")\n",
    "        print(f\"   Active provider: {active_provider}\")\n",
    "        \n",
    "        if 'CUDA' in active_provider:\n",
    "            print(f\"\\n   ‚úÖ CUDA enabled with EXHAUSTIVE convolution search\")\n",
    "            print(f\"   üîß CUDNN optimizations enabled\")\n",
    "            print(f\"   üì¶ GPU memory limit: 4GB\")\n",
    "            print(f\"   ‚ö° Expected: 2-3x faster than basic CUDA\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ö†Ô∏è  Running on CPU (much slower)\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create optimized session: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå ONNX model not found at: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED Speed Test with TensorRT/FP16\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'onnx_session_optimized' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50\n",
    "    \n",
    "    inference_times = []\n",
    "    \n",
    "    active_provider = onnx_session_optimized.get_providers()[0]\n",
    "    \n",
    "    print(f\"üöÄ RF-DETR ONNX OPTIMIZED Speed Test\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"   Model: RF-DETR-Small (ONNX)\")\n",
    "    print(f\"   Provider: {active_provider}\")\n",
    "    print(f\"   Precision: FP16\" if 'Tensorrt' in active_provider else \"   Precision: FP32\")\n",
    "    print(f\"   Input size: 512x512\")\n",
    "    print(f\"   Frames: {total_frames}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    input_name = onnx_session_optimized.get_inputs()[0].name\n",
    "    \n",
    "    # Extended warmup for TensorRT (it needs to build engine on first run)\n",
    "    warmup_runs = 10 if 'Tensorrt' in active_provider else 5\n",
    "    print(f\"üî• Warmup ({warmup_runs} frames)...\")\n",
    "    if 'Tensorrt' in active_provider:\n",
    "        print(\"   (TensorRT builds optimized engine on first run)\")\n",
    "    \n",
    "    for _ in range(warmup_runs):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_resized = cv2.resize(frame, (512, 512))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "            img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "            img_batch = np.expand_dims(img_chw, axis=0)\n",
    "            _ = onnx_session_optimized.run(None, {input_name: img_batch})\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    print(\"‚úÖ Warmup complete\\n\")\n",
    "    \n",
    "    # Actual speed test\n",
    "    frame_idx = 0\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "        img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "        img_batch = np.expand_dims(img_chw, axis=0)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.time()\n",
    "        outputs = onnx_session_optimized.run(None, {input_name: img_batch})\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    min_inf_time = np.min(inference_times)\n",
    "    max_inf_time = np.max(inference_times)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä OPTIMIZED ONNX Performance Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Min inference time: {min_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Max inference time: {max_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Std dev: {std_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Evolution:\")\n",
    "    print(f\"   PyTorch (1920x1080): 11.18 FPS\")\n",
    "    print(f\"   PyTorch (512x512):   19.22 FPS (+71.9%)\")\n",
    "    print(f\"   ONNX basic:          34.85 FPS (+81.3%)\")\n",
    "    print(f\"   ONNX optimized:      {avg_fps:.2f} FPS\", end=\"\")\n",
    "    \n",
    "    basic_fps = 34.85\n",
    "    if avg_fps > basic_fps:\n",
    "        improvement = ((avg_fps - basic_fps) / basic_fps) * 100\n",
    "        print(f\" (+{improvement:.1f}%)\")\n",
    "    else:\n",
    "        degradation = ((basic_fps - avg_fps) / basic_fps) * 100\n",
    "        print(f\" (-{degradation:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüèÜ vs YOLO v8s Baseline:\")\n",
    "    print(f\"   YOLO v8s:     25.00 ms (39.8 FPS)\")\n",
    "    print(f\"   RF-DETR ONNX: {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "    \n",
    "    if avg_fps > 39.8:\n",
    "        speedup = ((avg_fps - 39.8) / 39.8) * 100\n",
    "        print(f\"   üéâ RF-DETR is {speedup:+.1f}% FASTER! ‚úÖ‚úÖ‚úÖ\")\n",
    "        print(f\"\\n   ‚úÖ INTEGRATION RECOMMENDED!\")\n",
    "    elif avg_fps > 37:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   üü° RF-DETR is {diff:.1f}% slower (very close!)\")\n",
    "        print(f\"\\n   ü§î Consider integration if accuracy is better\")\n",
    "    else:\n",
    "        diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "        print(f\"   ‚ùå RF-DETR is {diff:.1f}% slower\")\n",
    "        print(f\"\\n   ‚ùå Stick with YOLO v8s\")\n",
    "    \n",
    "    # Check against target\n",
    "    target_fps = 284\n",
    "    gap = ((target_fps - avg_fps) / target_fps) * 100\n",
    "    print(f\"\\nüìâ Gap to advertised target: {gap:.1f}% slower than 284 FPS\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'onnx_session_optimized' not in locals():\n",
    "        print(f\"‚ùå Optimized ONNX session not loaded\")\n",
    "        print(f\"   Run the cell above to create optimized session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Try IO Binding for zero-copy GPU inference (fastest possible)\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Setting up IO Binding for maximum GPU performance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'onnx_session_optimized' in locals():\n",
    "    active_provider = onnx_session_optimized.get_providers()[0]\n",
    "    \n",
    "    if 'CUDA' in active_provider:\n",
    "        print(\"‚úÖ CUDA provider detected - IO Binding available!\")\n",
    "        print(\"\\nüí° IO Binding benefits:\")\n",
    "        print(\"   - Zero-copy data transfer to GPU\")\n",
    "        print(\"   - Pre-allocated GPU buffers\")\n",
    "        print(\"   - Eliminates CPU‚ÜîGPU transfer overhead\")\n",
    "        print(\"   - Expected: 20-40% faster than regular inference\\n\")\n",
    "        \n",
    "        # Get input/output details\n",
    "        input_name = onnx_session_optimized.get_inputs()[0].name\n",
    "        output_names = [o.name for o in onnx_session_optimized.get_outputs()]\n",
    "        \n",
    "        print(f\"Model interface:\")\n",
    "        print(f\"   Input: {input_name}\")\n",
    "        print(f\"   Outputs: {output_names}\\n\")\n",
    "        \n",
    "        print(\"‚úÖ IO Binding ready to use!\")\n",
    "        print(\"   Run the next cell to test with IO Binding\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  IO Binding requires CUDA provider\")\n",
    "        print(f\"   Current provider: {active_provider}\")\n",
    "        print(\"   IO Binding will not be used\")\n",
    "else:\n",
    "    print(\"‚ùå Optimized ONNX session not found\")\n",
    "    print(\"   Run the previous cell first\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAXIMUM SPEED Test with IO Binding (zero-copy GPU)\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'onnx_session_optimized' in locals():\n",
    "    active_provider = onnx_session_optimized.get_providers()[0]\n",
    "    \n",
    "    if 'CUDA' not in active_provider:\n",
    "        print(\"‚ö†Ô∏è  IO Binding requires CUDA provider\")\n",
    "        print(f\"   Current provider: {active_provider}\")\n",
    "        print(\"   Skipping IO Binding test\")\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = 50\n",
    "        \n",
    "        inference_times = []\n",
    "        \n",
    "        print(f\"üöÄ RF-DETR ONNX with IO Binding (MAXIMUM SPEED)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Model: RF-DETR-Small (ONNX)\")\n",
    "        print(f\"   Provider: {active_provider}\")\n",
    "        print(f\"   Optimization: IO Binding (zero-copy)\")\n",
    "        print(f\"   Input size: 512x512\")\n",
    "        print(f\"   Frames: {total_frames}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        input_name = onnx_session_optimized.get_inputs()[0].name\n",
    "        output_names = [o.name for o in onnx_session_optimized.get_outputs()]\n",
    "        \n",
    "        print(f\"üìã Model outputs: {output_names}\\n\")\n",
    "        \n",
    "        io_binding = onnx_session_optimized.io_binding()\n",
    "        \n",
    "        # Warmup\n",
    "        print(f\"üî• Warmup (5 frames with IO Binding)...\")\n",
    "        for _ in range(5):\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_resized = cv2.resize(frame, (512, 512))\n",
    "                frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "                img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "                img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "                img_batch = np.expand_dims(img_chw, axis=0).astype(np.float32)\n",
    "                \n",
    "                # Use IO Binding - bind all outputs\n",
    "                io_binding.bind_cpu_input(input_name, img_batch)\n",
    "                for output_name in output_names:\n",
    "                    io_binding.bind_output(output_name)\n",
    "                onnx_session_optimized.run_with_iobinding(io_binding)\n",
    "                io_binding.clear_binding_outputs()\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        print(\"‚úÖ Warmup complete\\n\")\n",
    "        \n",
    "        # Actual speed test\n",
    "        frame_idx = 0\n",
    "        start_total = time.time()\n",
    "        \n",
    "        while frame_idx < total_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Preprocess\n",
    "            frame_resized = cv2.resize(frame, (512, 512))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "            img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "            img_batch = np.expand_dims(img_chw, axis=0).astype(np.float32)\n",
    "            \n",
    "            # Time IO Binding inference\n",
    "            start = time.time()\n",
    "            io_binding.bind_cpu_input(input_name, img_batch)\n",
    "            for output_name in output_names:\n",
    "                io_binding.bind_output(output_name)\n",
    "            onnx_session_optimized.run_with_iobinding(io_binding)\n",
    "            outputs = io_binding.copy_outputs_to_cpu()\n",
    "            io_binding.clear_binding_outputs()\n",
    "            inf_time = time.time() - start\n",
    "            \n",
    "            inference_times.append(inf_time)\n",
    "            \n",
    "            frame_idx += 1\n",
    "            \n",
    "            if frame_idx % 10 == 0:\n",
    "                print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        total_time = time.time() - start_total\n",
    "        \n",
    "        # Calculate stats\n",
    "        avg_inf_time = np.mean(inference_times)\n",
    "        avg_fps = 1 / avg_inf_time\n",
    "        std_inf_time = np.std(inference_times)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìä IO Binding Performance Results\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "        print(f\"Std dev: {std_inf_time*1000:.2f} ms\")\n",
    "        print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Performance Evolution:\")\n",
    "        print(f\"   PyTorch (1920x1080):  11.18 FPS\")\n",
    "        print(f\"   PyTorch (512x512):    19.22 FPS\")\n",
    "        print(f\"   ONNX basic:           34.85 FPS\")\n",
    "        print(f\"   ONNX IO Binding:      {avg_fps:.2f} FPS\", end=\"\")\n",
    "        \n",
    "        basic_fps = 34.85\n",
    "        if avg_fps > basic_fps:\n",
    "            improvement = ((avg_fps - basic_fps) / basic_fps) * 100\n",
    "            print(f\" (+{improvement:.1f}%)\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        print(f\"\\nüèÜ vs YOLO v8s Baseline:\")\n",
    "        print(f\"   YOLO v8s:        25.00 ms (39.8 FPS)\")\n",
    "        print(f\"   RF-DETR ONNX+IO: {avg_inf_time*1000:.2f} ms ({avg_fps:.2f} FPS)\")\n",
    "        \n",
    "        if avg_fps > 39.8:\n",
    "            speedup = ((avg_fps - 39.8) / 39.8) * 100\n",
    "            print(f\"   üéâ RF-DETR is {speedup:+.1f}% FASTER! ‚úÖ‚úÖ‚úÖ\")\n",
    "            print(f\"\\n   ‚úÖ INTEGRATION RECOMMENDED!\")\n",
    "        elif avg_fps > 37:\n",
    "            diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "            print(f\"   üü° RF-DETR is {diff:.1f}% slower (close)\")\n",
    "        else:\n",
    "            diff = ((39.8 - avg_fps) / 39.8) * 100\n",
    "            print(f\"   ‚ùå RF-DETR is {diff:.1f}% slower\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'onnx_session_optimized' not in locals():\n",
    "        print(f\"‚ùå Optimized ONNX session not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a21d3",
   "metadata": {},
   "source": [
    "## üéØ Final RF-DETR Investigation Summary\n",
    "\n",
    "### Performance Achievement: **38.24 FPS** (3.9% from YOLO parity!)\n",
    "\n",
    "| Method | Avg FPS | Inference Time | vs YOLO | Speedup from PyTorch |\n",
    "|--------|---------|---------------|---------|---------------------|\n",
    "| PyTorch (1920x1080) | 11.18 | 89.47 ms | -71.9% | Baseline |\n",
    "| PyTorch (512x512) | 19.22 | 52.04 ms | -51.7% | +71.9% |\n",
    "| ONNX Basic | 34.85 | 28.69 ms | -12.4% | +211.7% |\n",
    "| **ONNX + IO Binding** | **38.24** | **26.15 ms** | **-3.9%** | **+242.0%** |\n",
    "| **YOLO v8s Target** | **39.8** | **25.00 ms** | **0%** | - |\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "‚úÖ **Wins:**\n",
    "- 3.4x faster than initial PyTorch (11.18 ‚Üí 38.24 FPS)\n",
    "- Only 1.56 FPS slower than YOLO (almost competitive!)\n",
    "- ONNX + IO Binding achieved 242% speedup\n",
    "- Detection quality maintained throughout optimizations\n",
    "\n",
    "‚ö†Ô∏è **Close but not quite:**\n",
    "- 3.9% slower than YOLO (1.15 ms difference)\n",
    "- Still 7.4x slower than advertised 284 FPS (JIT/TensorRT unavailable)\n",
    "- Lower std dev (6.62 ms) shows good consistency\n",
    "\n",
    "### Integration Decision:\n",
    "\n",
    "**üü° MARGINAL CALL - Consider These Factors:**\n",
    "\n",
    "**Arguments FOR integration:**\n",
    "- Only 3.9% slower (38.24 vs 39.8 FPS)\n",
    "- May have better accuracy/features than YOLO\n",
    "- ONNX model is production-ready and optimized\n",
    "- Consistent performance (low std dev)\n",
    "\n",
    "**Arguments AGAINST integration:**\n",
    "- Still technically slower than proven YOLO baseline\n",
    "- Would slow down pipeline slightly (3.9%)\n",
    "- YOLO is battle-tested and well-integrated\n",
    "- RF-DETR requires 512x512 resize (quality concern for pose)\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "**Option 1 (Conservative):** ‚ùå **Stick with YOLO v8s**\n",
    "- Reason: YOLO is faster and proven\n",
    "- 3.9% may compound with tracking/pose stages\n",
    "- Not worth the integration risk for marginal difference\n",
    "\n",
    "**Option 2 (Progressive):** üü° **Run accuracy comparison first**\n",
    "- Test RF-DETR vs YOLO detection quality on kohli_nets.mp4\n",
    "- If RF-DETR has significantly better accuracy ‚Üí integrate\n",
    "- If similar accuracy ‚Üí stick with YOLO\n",
    "\n",
    "**Option 3 (Aggressive):** ‚úÖ **Integrate with feature flag**\n",
    "- Add RF-DETR as optional backend\n",
    "- Let users choose: YOLO (faster) vs RF-DETR (newer)\n",
    "- Document: \"RF-DETR: 38 FPS, YOLO: 40 FPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec8ece",
   "metadata": {},
   "source": [
    "## üöÄ Batch Inference - Final Optimization\n",
    "\n",
    "Current: **38.24 FPS** (single frame inference)\n",
    "\n",
    "**Batch inference benefits:**\n",
    "- Process multiple frames simultaneously\n",
    "- Better GPU utilization (parallel processing)\n",
    "- Amortize overhead across multiple frames\n",
    "- Expected: 20-40% throughput increase\n",
    "\n",
    "**Trade-offs:**\n",
    "- Increased latency per frame (batch must complete)\n",
    "- Higher memory usage\n",
    "- Not suitable for real-time streaming\n",
    "- Good for video processing pipelines\n",
    "\n",
    "Let's test with batch sizes: 2, 4, 8 to find optimal throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de47430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ONNX model input shape for dynamic batching support\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if 'onnx_session_optimized' in locals():\n",
    "    print(\"üîç Checking ONNX model for dynamic batch support\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    input_shape = onnx_session_optimized.get_inputs()[0].shape\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    if input_shape[0] == 1:\n",
    "        print(\"\\n‚ö†Ô∏è  Model has FIXED batch size = 1\")\n",
    "        print(\"   Cannot use native batching\")\n",
    "        print(\"\\nüí° Alternative: Pipeline parallelism\")\n",
    "        print(\"   - Pre-load frames while GPU processes current frame\")\n",
    "        print(\"   - Use async inference if available\")\n",
    "        print(\"   - Process multiple videos in parallel\")\n",
    "    elif isinstance(input_shape[0], str) or input_shape[0] is None or input_shape[0] == -1:\n",
    "        print(f\"\\n‚úÖ Model supports DYNAMIC batching\")\n",
    "        print(f\"   Batch dimension: {input_shape[0]}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Model has fixed batch size = {input_shape[0]}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "if video_path.exists() and 'onnx_session_optimized' in locals():\n",
    "    # Since batch size is fixed at 1, test with optimized single-frame pipeline\n",
    "    print(\"\\nüöÄ Optimized Single-Frame Pipeline Test\")\n",
    "    print(\"   (Pre-loading next frame while GPU processes current)\")\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing optimized pipeline with CPU-GPU overlap\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 100  # More frames to better measure throughput\n",
    "    \n",
    "    inference_times = []\n",
    "    preprocess_times = []\n",
    "    frames_processed = 0\n",
    "    \n",
    "    input_name = onnx_session_optimized.get_inputs()[0].name\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"\\nüî• Warmup (10 frames)...\")\n",
    "    for _ in range(10):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_resized = cv2.resize(frame, (512, 512))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "            img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "            img_batch = np.expand_dims(img_chw, axis=0).astype(np.float32)\n",
    "            _ = onnx_session_optimized.run(None, {input_name: img_batch})\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    print(\"‚úÖ Warmup complete\\n\")\n",
    "    \n",
    "    # Optimized pipeline: preprocess next frame while GPU runs\n",
    "    start_total = time.time()\n",
    "    \n",
    "    # Pre-load first frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        prep_start = time.time()\n",
    "        frame_resized = cv2.resize(frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "        img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "        current_batch = np.expand_dims(img_chw, axis=0).astype(np.float32)\n",
    "        prep_time = time.time() - prep_start\n",
    "        preprocess_times.append(prep_time)\n",
    "        frames_processed += 1\n",
    "    \n",
    "    while frames_processed < total_frames:\n",
    "        # Start loading next frame (CPU work while GPU is busy)\n",
    "        ret, next_frame = cap.read()\n",
    "        \n",
    "        # Run inference on current frame (GPU work)\n",
    "        inf_start = time.time()\n",
    "        outputs = onnx_session_optimized.run(None, {input_name: current_batch})\n",
    "        inf_time = time.time() - inf_start\n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        if not ret or frames_processed >= total_frames:\n",
    "            break\n",
    "        \n",
    "        # Preprocess next frame (CPU work, potentially overlapped)\n",
    "        prep_start = time.time()\n",
    "        frame_resized = cv2.resize(next_frame, (512, 512))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "        img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "        current_batch = np.expand_dims(img_chw, axis=0).astype(np.float32)\n",
    "        prep_time = time.time() - prep_start\n",
    "        preprocess_times.append(prep_time)\n",
    "        \n",
    "        frames_processed += 1\n",
    "        \n",
    "        if frames_processed % 25 == 0:\n",
    "            print(f\"Processed {frames_processed}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_prep_time = np.mean(preprocess_times)\n",
    "    throughput_fps = frames_processed / total_time\n",
    "    theoretical_max_fps = 1 / avg_inf_time\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä Optimized Pipeline Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Frames processed: {frames_processed}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg preprocess time: {avg_prep_time*1000:.2f} ms\")\n",
    "    print(f\"Total per frame: {(avg_inf_time + avg_prep_time)*1000:.2f} ms\")\n",
    "    print(f\"Actual throughput: {throughput_fps:.2f} FPS\")\n",
    "    print(f\"Theoretical max (inf only): {theoretical_max_fps:.2f} FPS\")\n",
    "    \n",
    "    print(f\"\\nüéØ vs Previous Best (IO Binding):\")\n",
    "    print(f\"   IO Binding:           38.24 FPS\")\n",
    "    print(f\"   Optimized Pipeline:   {throughput_fps:.2f} FPS\")\n",
    "    \n",
    "    improvement = ((throughput_fps - 38.24) / 38.24) * 100\n",
    "    if throughput_fps > 38.24:\n",
    "        print(f\"   ‚úÖ Improvement: {improvement:+.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Degradation: {improvement:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüèÜ vs YOLO v8s Baseline:\")\n",
    "    print(f\"   YOLO v8s:             39.8 FPS\")\n",
    "    print(f\"   RF-DETR Optimized:    {throughput_fps:.2f} FPS\")\n",
    "    \n",
    "    if throughput_fps > 39.8:\n",
    "        speedup = ((throughput_fps - 39.8) / 39.8) * 100\n",
    "        print(f\"   üéâ RF-DETR is {speedup:+.1f}% FASTER! ‚úÖ‚úÖ‚úÖ\")\n",
    "        print(f\"\\n   ‚úÖ INTEGRATION RECOMMENDED!\")\n",
    "    elif throughput_fps > 38:\n",
    "        diff = ((39.8 - throughput_fps) / 39.8) * 100\n",
    "        print(f\"   üü° RF-DETR is {diff:.1f}% slower (marginal)\")\n",
    "    else:\n",
    "        diff = ((39.8 - throughput_fps) / 39.8) * 100\n",
    "        print(f\"   ‚ùå RF-DETR is {diff:.1f}% slower\")\n",
    "    \n",
    "    print(f\"\\nüí° Bottleneck Analysis:\")\n",
    "    if avg_prep_time > avg_inf_time * 0.5:\n",
    "        print(f\"   ‚ö†Ô∏è  Preprocessing takes {(avg_prep_time/avg_inf_time)*100:.1f}% of inference time\")\n",
    "        print(f\"   Consider GPU preprocessing or multi-threading\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Preprocessing is fast ({(avg_prep_time/avg_inf_time)*100:.1f}% of inference)\")\n",
    "        print(f\"   GPU inference is the bottleneck\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'onnx_session_optimized' not in locals():\n",
    "        print(f\"‚ùå Optimized ONNX session not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0cb23",
   "metadata": {},
   "source": [
    "## üéØ **FINAL VERDICT: RF-DETR vs YOLO v8s**\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Configuration | FPS | Inference Time | vs YOLO | Notes |\n",
    "|---------------|-----|----------------|---------|-------|\n",
    "| **YOLO v8s** | **39.8** | **25.0 ms** | **0%** | ‚úÖ Current baseline |\n",
    "| RF-DETR ONNX + IO Binding | **38.24** | **26.15 ms** | **-3.9%** | üü° Very close! |\n",
    "\n",
    "### Achievement: **96.1% of YOLO performance!**\n",
    "\n",
    "We went from:\n",
    "- ‚ùå Initial PyTorch: 11.18 FPS (28% of YOLO)\n",
    "- ‚úÖ **Optimized ONNX: 38.24 FPS (96% of YOLO)**\n",
    "- üöÄ **3.4x speedup through optimization!**\n",
    "\n",
    "### Why We Can't Beat YOLO (for now):\n",
    "\n",
    "1. **Model architecture** - RF-DETR is fundamentally a transformer-based detector (more accurate but heavier than YOLO's CNN)\n",
    "2. **Fixed batch size** - ONNX model locked to batch=1, can't leverage batch parallelism\n",
    "3. **No TensorRT** - T4 GPU doesn't have TensorRT support in this Colab, would give 2-3x more speed\n",
    "4. **Advertised 284 FPS** - Requires specific hardware (A100?) + TensorRT + JIT optimizations we can't access\n",
    "\n",
    "### üéØ Final Recommendation:\n",
    "\n",
    "**Option 1: ‚ùå Stick with YOLO (RECOMMENDED)**\n",
    "- **Reason**: YOLO is 3.9% faster and proven\n",
    "- **Risk**: RF-DETR might slow down overall pipeline\n",
    "- **Safe choice**: Don't fix what isn't broken\n",
    "\n",
    "**Option 2: üü° Add RF-DETR as Optional Backend**\n",
    "- **Reason**: 3.9% difference is marginal\n",
    "- **Use case**: Let users test if RF-DETR has better accuracy for their videos\n",
    "- **Implementation**: Feature flag in config\n",
    "- **Documentation**: \"RF-DETR: 38 FPS (newer, may be more accurate), YOLO: 40 FPS (faster, proven)\"\n",
    "\n",
    "**Option 3: ‚úÖ Test Accuracy First**\n",
    "- **Action**: Run detection quality comparison on kohli_nets.mp4\n",
    "- **Decision**: If RF-DETR significantly more accurate ‚Üí integrate\n",
    "- **If similar**: Stick with YOLO\n",
    "\n",
    "### My Recommendation: **Option 3**\n",
    "\n",
    "Since we're only 3.9% slower, **accuracy should decide**. If RF-DETR detects persons better (fewer misses, better bboxes), the slight speed trade-off is worth it for better pose estimation downstream.\n",
    "\n",
    "**Next step**: Run visual detection quality comparison?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c775a7",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Export Custom ONNX with Dynamic Batching\n",
    "\n",
    "**Why custom export?**\n",
    "- ‚úÖ Enable **dynamic batch size** (current ONNX is fixed batch=1)\n",
    "- ‚úÖ Apply **graph optimizations** during export\n",
    "- ‚úÖ Enable **FP16 precision** for 2x speed\n",
    "- ‚úÖ Optimize **for specific GPU** (T4)\n",
    "- ‚úÖ Remove unnecessary operations\n",
    "\n",
    "**Expected benefits:**\n",
    "- Dynamic batching: +30-50% throughput\n",
    "- FP16: +50-100% speed (if supported)\n",
    "- Graph optimization: +10-20% speed\n",
    "- **Combined**: Could reach 50-70 FPS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PyTorch RF-DETR to ONNX with dynamic batching\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîß Exporting RF-DETR PyTorch model to ONNX with optimizations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'model' in locals():\n",
    "    # Create export directory\n",
    "    export_dir = Path('/content/models/rf_detr_custom_onnx')\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_path = export_dir / 'rf-detr-small-dynamic.onnx'\n",
    "    \n",
    "    print(f\"Export path: {onnx_path}\\n\")\n",
    "    \n",
    "    # Prepare model for export\n",
    "    print(\"üì¶ Preparing model for export...\")\n",
    "    \n",
    "    # The rfdetr package wraps the model, need to access internal torch model\n",
    "    print(f\"   Model type: {type(model)}\")\n",
    "    print(f\"   Model attributes: {[a for a in dir(model) if not a.startswith('_')][:10]}\")\n",
    "    \n",
    "    # Try different ways to access the underlying PyTorch model\n",
    "    torch_model = None\n",
    "    \n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'eval'):\n",
    "        torch_model = model.model\n",
    "        print(f\"   Found via model.model\")\n",
    "    elif hasattr(model, 'detector') and hasattr(model.detector, 'eval'):\n",
    "        torch_model = model.detector\n",
    "        print(f\"   Found via model.detector\")\n",
    "    elif hasattr(model, 'net') and hasattr(model.net, 'eval'):\n",
    "        torch_model = model.net\n",
    "        print(f\"   Found via model.net\")\n",
    "    else:\n",
    "        # Try to reload model directly from rfdetr\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Cannot access PyTorch model from rfdetr wrapper\")\n",
    "        print(f\"   üí° Trying to load PyTorch model directly...\")\n",
    "        \n",
    "        try:\n",
    "            from rfdetr.models import RFDETRSmall as RFDETRSmallModel\n",
    "            torch_model = RFDETRSmallModel()\n",
    "            print(f\"   ‚úÖ Loaded PyTorch model directly\")\n",
    "        except:\n",
    "            print(f\"   ‚ùå Cannot load PyTorch model\")\n",
    "            print(f\"   The rfdetr package may not expose the underlying model\")\n",
    "            raise AttributeError(\"Cannot access PyTorch model for ONNX export\")\n",
    "    \n",
    "    if torch_model is None:\n",
    "        raise AttributeError(\"Cannot find PyTorch model to export\")\n",
    "    \n",
    "    # Set to eval mode\n",
    "    torch_model.eval()\n",
    "    \n",
    "    # Move to GPU for export\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch_model = torch_model.to(device)\n",
    "    \n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Torch model type: {type(torch_model)}\")\n",
    "    \n",
    "    # Create dummy input with dynamic batch dimension\n",
    "    dummy_input = torch.randn(1, 3, 512, 512, device=device)\n",
    "    \n",
    "    print(f\"   Dummy input shape: {dummy_input.shape}\")\n",
    "    print(f\"\\n‚öôÔ∏è  Exporting with optimizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Export with dynamic axes for batching\n",
    "        torch.onnx.export(\n",
    "            torch_model,\n",
    "            dummy_input,\n",
    "            str(onnx_path),\n",
    "            export_params=True,\n",
    "            opset_version=17,  # Latest stable opset\n",
    "            do_constant_folding=True,  # Optimize constant operations\n",
    "            input_names=['input'],\n",
    "            output_names=['pred_boxes', 'pred_logits'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},  # Dynamic batch dimension\n",
    "                'pred_boxes': {0: 'batch_size'},\n",
    "                'pred_logits': {0: 'batch_size'}\n",
    "            },\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Export successful!\")\n",
    "        print(f\"   File: {onnx_path}\")\n",
    "        print(f\"   Size: {onnx_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dynamic batching enabled:\")\n",
    "        print(f\"   Batch dimension: 0 (variable)\")\n",
    "        print(f\"   Input shape: [batch_size, 3, 512, 512]\")\n",
    "        print(f\"   Can now test batch sizes: 1, 2, 4, 8, 16!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nüí° Trying alternative export method...\")\n",
    "        try:\n",
    "            # Try with torch.jit.trace first\n",
    "            traced_model = torch.jit.trace(torch_model, dummy_input)\n",
    "            \n",
    "            torch.onnx.export(\n",
    "                traced_model,\n",
    "                dummy_input,\n",
    "                str(onnx_path),\n",
    "                export_params=True,\n",
    "                opset_version=17,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=['output'],\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch_size'},\n",
    "                    'output': {0: 'batch_size'}\n",
    "                }\n",
    "            )\n",
    "            print(f\"‚úÖ Alternative export successful!\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Alternative export also failed: {e2}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå PyTorch model not loaded\")\n",
    "    print(\"   Load the model first (Cell 11)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4517c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom ONNX model with dynamic batching\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path('/content/models/rf_detr_custom_onnx/rf-detr-small-dynamic.onnx')\n",
    "\n",
    "if onnx_path.exists():\n",
    "    print(f\"üì• Loading custom ONNX model with dynamic batching\")\n",
    "    print(f\"=\"*70)\n",
    "    \n",
    "    # Session options for maximum performance\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess_options.inter_op_num_threads = 1\n",
    "    sess_options.enable_mem_pattern = True\n",
    "    sess_options.enable_cpu_mem_arena = True\n",
    "    \n",
    "    # CUDA provider with optimizations\n",
    "    providers = [\n",
    "        ('CUDAExecutionProvider', {\n",
    "            'device_id': 0,\n",
    "            'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "            'gpu_mem_limit': 4 * 1024 * 1024 * 1024,\n",
    "            'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "            'do_copy_in_default_stream': True,\n",
    "        }),\n",
    "        'CPUExecutionProvider'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        custom_onnx_session = ort.InferenceSession(\n",
    "            str(onnx_path),\n",
    "            sess_options=sess_options,\n",
    "            providers=providers\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Custom ONNX session created!\")\n",
    "        print(f\"   Provider: {custom_onnx_session.get_providers()[0]}\")\n",
    "        \n",
    "        # Check input/output details\n",
    "        print(f\"\\nüìã Model Interface:\")\n",
    "        for inp in custom_onnx_session.get_inputs():\n",
    "            print(f\"   Input: {inp.name}, Shape: {inp.shape}, Type: {inp.type}\")\n",
    "        \n",
    "        for out in custom_onnx_session.get_outputs():\n",
    "            print(f\"   Output: {out.name}, Shape: {out.shape}, Type: {out.type}\")\n",
    "        \n",
    "        # Check if batch dimension is dynamic\n",
    "        input_shape = custom_onnx_session.get_inputs()[0].shape\n",
    "        if isinstance(input_shape[0], str) or input_shape[0] is None:\n",
    "            print(f\"\\n‚úÖ Dynamic batching confirmed!\")\n",
    "            print(f\"   Batch dimension: '{input_shape[0]}' (variable)\")\n",
    "            print(f\"   Ready to test batch sizes: 1, 2, 4, 8!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Batch dimension: {input_shape[0]} (fixed)\")\n",
    "        \n",
    "        print(f\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load custom ONNX: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Custom ONNX not found: {onnx_path}\")\n",
    "    print(f\"   Export the model first using the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom ONNX with TRUE batch inference\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'custom_onnx_session' in locals():\n",
    "    batch_sizes = [1, 2, 4, 8]\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üöÄ Custom ONNX Batch Inference Test (TRUE BATCHING)\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"Testing batch sizes: {batch_sizes}\")\n",
    "    print(f\"=\"*70)\n",
    "    \n",
    "    input_name = custom_onnx_session.get_inputs()[0].name\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing Batch Size: {batch_size}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = 100\n",
    "        \n",
    "        inference_times = []\n",
    "        frames_processed = 0\n",
    "        \n",
    "        # Warmup\n",
    "        print(f\"üî• Warmup (5 batches)...\")\n",
    "        for _ in range(5):\n",
    "            batch_frames = []\n",
    "            for _ in range(batch_size):\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_resized = cv2.resize(frame, (512, 512))\n",
    "                    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "                    img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "                    img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "                    batch_frames.append(img_chw)\n",
    "            \n",
    "            if len(batch_frames) == batch_size:\n",
    "                batch_input = np.stack(batch_frames, axis=0).astype(np.float32)\n",
    "                _ = custom_onnx_session.run(None, {input_name: batch_input})\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        print(\"‚úÖ Warmup complete\")\n",
    "        \n",
    "        # Actual test\n",
    "        start_total = time.time()\n",
    "        \n",
    "        while frames_processed < total_frames:\n",
    "            batch_frames = []\n",
    "            \n",
    "            # Collect batch\n",
    "            for _ in range(batch_size):\n",
    "                ret, frame = cap.read()\n",
    "                if ret and frames_processed < total_frames:\n",
    "                    frame_resized = cv2.resize(frame, (512, 512))\n",
    "                    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "                    img_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "                    img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "                    batch_frames.append(img_chw)\n",
    "                    frames_processed += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not batch_frames:\n",
    "                break\n",
    "            \n",
    "            # Run inference on actual batch\n",
    "            batch_input = np.stack(batch_frames, axis=0).astype(np.float32)\n",
    "            \n",
    "            start = time.time()\n",
    "            outputs = custom_onnx_session.run(None, {input_name: batch_input})\n",
    "            inf_time = time.time() - start\n",
    "            \n",
    "            inference_times.append(inf_time)\n",
    "            \n",
    "            if frames_processed % 25 == 0:\n",
    "                print(f\"   Processed {frames_processed}/{total_frames} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        total_time = time.time() - start_total\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_batch_time = np.mean(inference_times)\n",
    "        avg_frame_time = avg_batch_time / batch_size\n",
    "        throughput_fps = frames_processed / total_time\n",
    "        std_batch_time = np.std(inference_times)\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'avg_batch_time': avg_batch_time,\n",
    "            'avg_frame_time': avg_frame_time,\n",
    "            'throughput_fps': throughput_fps,\n",
    "            'std_batch_time': std_batch_time,\n",
    "            'total_time': total_time,\n",
    "            'frames': frames_processed\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   Results:\")\n",
    "        print(f\"      Frames: {frames_processed}\")\n",
    "        print(f\"      Total time: {total_time:.2f}s\")\n",
    "        print(f\"      Avg batch time: {avg_batch_time*1000:.2f} ms (¬±{std_batch_time*1000:.2f})\")\n",
    "        print(f\"      Avg per frame: {avg_frame_time*1000:.2f} ms\")\n",
    "        print(f\"      Throughput: {throughput_fps:.2f} FPS\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä Batch Inference Results\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    print(f\"{'Batch':<8} {'Batch Time':<12} {'Per Frame':<12} {'Throughput':<12} {'vs BS=1':<12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    single_fps = results[1]['throughput_fps']\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        r = results[bs]\n",
    "        improvement = ((r['throughput_fps'] - single_fps) / single_fps) * 100\n",
    "        \n",
    "        print(f\"{bs:<8} {r['avg_batch_time']*1000:<12.2f} {r['avg_frame_time']*1000:<12.2f} \"\n",
    "              f\"{r['throughput_fps']:<12.2f} {improvement:>+7.1f}%\")\n",
    "    \n",
    "    # Find best\n",
    "    best_bs = max(results.keys(), key=lambda k: results[k]['throughput_fps'])\n",
    "    best_fps = results[best_bs]['throughput_fps']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèÜ BEST CONFIGURATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Batch size: {best_bs}\")\n",
    "    print(f\"Throughput: {best_fps:.2f} FPS\")\n",
    "    print(f\"Improvement: {((best_fps - single_fps) / single_fps) * 100:+.1f}% vs single-frame\")\n",
    "    \n",
    "    print(f\"\\nüéØ vs YOLO v8s Baseline (39.8 FPS):\")\n",
    "    print(f\"   Custom ONNX (bs=1):   {single_fps:.2f} FPS\")\n",
    "    print(f\"   Custom ONNX (bs={best_bs}):   {best_fps:.2f} FPS\")\n",
    "    \n",
    "    if best_fps > 39.8:\n",
    "        speedup = ((best_fps - 39.8) / 39.8) * 100\n",
    "        print(f\"\\n   üéâüéâüéâ RF-DETR is {speedup:+.1f}% FASTER than YOLO! üéâüéâüéâ\")\n",
    "        print(f\"   ‚úÖ‚úÖ‚úÖ INTEGRATION STRONGLY RECOMMENDED! ‚úÖ‚úÖ‚úÖ\")\n",
    "    elif best_fps > 38:\n",
    "        diff = ((39.8 - best_fps) / 39.8) * 100\n",
    "        print(f\"\\n   üü° RF-DETR is {diff:.1f}% slower (very competitive)\")\n",
    "        print(f\"   üí° Consider integration based on accuracy\")\n",
    "    else:\n",
    "        diff = ((39.8 - best_fps) / 39.8) * 100\n",
    "        print(f\"\\n   ‚ùå RF-DETR is {diff:.1f}% slower\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'custom_onnx_session' not in locals():\n",
    "        print(f\"‚ùå Custom ONNX session not loaded\")\n",
    "        print(f\"   Export and load the custom model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd30013",
   "metadata": {},
   "source": [
    "## üî• Problem: Roboflow API is CPU-only (0.92 FPS)\n",
    "\n",
    "The Roboflow inference API runs on their servers (CPU), not your local GPU. This is why we're getting 0.92 FPS instead of the expected ~200+ FPS.\n",
    "\n",
    "**Solution Options:**\n",
    "1. ‚úÖ **Use native PyTorch model with GPU** (best option)\n",
    "2. ‚úÖ **Export to ONNX and run with TensorRT** (fastest, but more setup)\n",
    "3. ‚ùå **Roboflow API** (current - too slow, CPU-only)\n",
    "\n",
    "Let's try loading RF-DETR natively with PyTorch on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ae512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading RF-DETR natively with PyTorch (GPU-accelerated)\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Checking RF-DETR repo structure for native PyTorch model...\")\n",
    "\n",
    "rf_detr_path = Path('/content/rf-detr')\n",
    "if rf_detr_path.exists():\n",
    "    # Add to path\n",
    "    sys.path.insert(0, str(rf_detr_path))\n",
    "    \n",
    "    # Check what's available\n",
    "    print(f\"‚úÖ RF-DETR repo found at: {rf_detr_path}\")\n",
    "    print(f\"\\nüìÅ Directory structure:\")\n",
    "    for item in rf_detr_path.iterdir():\n",
    "        print(f\"   {item.name}\")\n",
    "    \n",
    "    # Try importing\n",
    "    try:\n",
    "        # Common patterns in DETR repos\n",
    "        import_attempts = [\n",
    "            \"from models import build_model\",\n",
    "            \"from rfdetr.models import RFDETR\",\n",
    "            \"from rfdetr import RFDETR\",\n",
    "            \"import rfdetr\",\n",
    "        ]\n",
    "        \n",
    "        for attempt in import_attempts:\n",
    "            try:\n",
    "                exec(attempt)\n",
    "                print(f\"\\n‚úÖ Success: {attempt}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {attempt} - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not import RF-DETR: {e}\")\n",
    "    \n",
    "    # Check for model files\n",
    "    print(f\"\\nüîç Looking for model definition files:\")\n",
    "    model_files = list(rf_detr_path.rglob(\"*model*.py\"))\n",
    "    for f in model_files[:10]:  # First 10\n",
    "        print(f\"   {f.relative_to(rf_detr_path)}\")\n",
    "    \n",
    "    # Check for weights\n",
    "    print(f\"\\nüîç Looking for weight files:\")\n",
    "    weight_files = list(rf_detr_path.rglob(\"*.pth\")) + list(rf_detr_path.rglob(\"*.pt\"))\n",
    "    if weight_files:\n",
    "        for f in weight_files:\n",
    "            print(f\"   {f.relative_to(rf_detr_path)}\")\n",
    "    else:\n",
    "        print(f\"   No .pth/.pt files found\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå RF-DETR repo not found at {rf_detr_path}\")\n",
    "    print(f\"   The repo was cloned in Cell 3 - check if it succeeded\")\n",
    "\n",
    "print(f\"\\nüí° If native PyTorch loading fails, we have two options:\")\n",
    "print(f\"   1. Use ultralytics RT-DETR instead (different model, but GPU-accelerated)\")\n",
    "print(f\"   2. Skip RF-DETR comparison, stick with YOLO (0.92 FPS is unusable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Roboflow inference model can use GPU\n",
    "import torch\n",
    "\n",
    "print(\"üîç Checking Roboflow inference GPU configuration:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "print(f\"\\nüîç Checking model object:\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model attributes: {[attr for attr in dir(model) if not attr.startswith('_')][:20]}\")\n",
    "\n",
    "# Try to find if there's a device attribute or method\n",
    "if hasattr(model, 'device'):\n",
    "    print(f\"\\n‚úÖ Model has device attribute: {model.device}\")\n",
    "elif hasattr(model, 'to'):\n",
    "    print(f\"\\n‚úÖ Model has .to() method - trying to move to GPU...\")\n",
    "    try:\n",
    "        model.to('cuda')\n",
    "        print(f\"   Successfully moved to CUDA\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "elif hasattr(model, 'model'):\n",
    "    print(f\"\\nüîç Model has .model attribute, checking nested model:\")\n",
    "    print(f\"   Type: {type(model.model)}\")\n",
    "    if hasattr(model.model, 'device'):\n",
    "        print(f\"   Device: {model.model.device}\")\n",
    "    if hasattr(model.model, 'to'):\n",
    "        print(f\"   Has .to() method - trying to move to GPU...\")\n",
    "        try:\n",
    "            model.model.to('cuda')\n",
    "            print(f\"   ‚úÖ Successfully moved to CUDA\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "# Check the inference method signature\n",
    "if hasattr(model, 'infer'):\n",
    "    import inspect\n",
    "    sig = inspect.signature(model.infer)\n",
    "    print(f\"\\nüìã model.infer() signature:\")\n",
    "    print(f\"   {sig}\")\n",
    "    print(f\"\\nüí° Check if there's a 'device' parameter we can pass\")\n",
    "\n",
    "print(f\"\\nüî¨ Alternative: Try using ultralytics RTDETR (different model but GPU-optimized)\")\n",
    "print(f\"   from ultralytics import RTDETR\")\n",
    "print(f\"   model = RTDETR('rtdetr-l.pt')\")\n",
    "print(f\"   This would give us ~100+ FPS on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8308f1",
   "metadata": {},
   "source": [
    "## üöÄ Solution: Use Ultralytics RT-DETR (GPU-Optimized)\n",
    "\n",
    "**Problem confirmed**: Roboflow inference API has no `device` parameter and runs on CPU (0.92 FPS).\n",
    "\n",
    "**Solution**: Use ultralytics RT-DETR instead:\n",
    "- ‚úÖ Native GPU support\n",
    "- ‚úÖ Same YOLO-like API\n",
    "- ‚úÖ Expected ~100-200 FPS on T4 GPU\n",
    "- ‚ö†Ô∏è Different model than RF-DETR (but still DETR-based, similar architecture)\n",
    "\n",
    "Let's test ultralytics RT-DETR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01063ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ultralytics RT-DETR (GPU-optimized alternative)\n",
    "from ultralytics import RTDETR\n",
    "import torch\n",
    "\n",
    "print(\"üì• Loading ultralytics RT-DETR...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Available models: rtdetr-l (large), rtdetr-x (xlarge)\n",
    "# rtdetr-l is comparable to RF-DETR-small in size/accuracy\n",
    "rtdetr_model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "# Move to GPU\n",
    "if torch.cuda.is_available():\n",
    "    rtdetr_model.to('cuda')\n",
    "    print(f\"‚úÖ RT-DETR model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  CUDA not available, using CPU\")\n",
    "\n",
    "print(f\"\\nüìä Model info:\")\n",
    "print(f\"   Model: RT-DETR-L (Ultralytics)\")\n",
    "print(f\"   Expected FPS on T4: ~100-200 FPS\")\n",
    "print(f\"   COCO mAP: ~53.0 (similar to RF-DETR-small)\")\n",
    "print(f\"\\nüí° RT-DETR uses standard COCO classes (person = class_id 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick GPU speed test - RT-DETR on 50 frames\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "if video_path.exists() and 'rtdetr_model' in locals():\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = 50\n",
    "    \n",
    "    inference_times = []\n",
    "    person_counts = []\n",
    "    \n",
    "    print(f\"üöÄ RT-DETR GPU Speed Test (50 frames)\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    start_total = time.time()\n",
    "    \n",
    "    while frame_idx < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # RT-DETR inference (GPU)\n",
    "        start = time.time()\n",
    "        results = rtdetr_model(frame_rgb, verbose=False)[0]\n",
    "        inf_time = time.time() - start\n",
    "        \n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        # Count persons (class_id=0 in COCO)\n",
    "        person_count = sum(1 for box in results.boxes if box.cls == 0 and box.conf > 0.5)\n",
    "        person_counts.append(person_count)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    std_inf_time = np.std(inference_times)\n",
    "    avg_persons = np.mean(person_counts)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä RT-DETR GPU Performance\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Avg inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Std dev: {std_inf_time*1000:.2f} ms\")\n",
    "    print(f\"Avg FPS: {avg_fps:.2f}\")\n",
    "    print(f\"Avg persons detected: {avg_persons:.2f}\")\n",
    "    print(f\"\\nüéØ Comparison:\")\n",
    "    print(f\"   RF-DETR (CPU): 0.92 FPS\")\n",
    "    print(f\"   RT-DETR (GPU): {avg_fps:.2f} FPS\")\n",
    "    print(f\"   Speedup: {avg_fps/0.92:.1f}x faster!\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "else:\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ùå Video not found: {video_path}\")\n",
    "    if 'rtdetr_model' not in locals():\n",
    "        print(f\"‚ùå RT-DETR model not loaded. Run the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34434ff",
   "metadata": {},
   "source": [
    "## 5. Test on Video (Person Detection Focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_video(model, video_path, output_path, conf_threshold=0.5, person_only=True):\n",
    "    \"\"\"\n",
    "    Run detection on video and save results\n",
    "    \n",
    "    Args:\n",
    "        person_only: If True, filter to only show person detections (class_id=0 in COCO)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    inference_times = []\n",
    "    detection_counts = []\n",
    "    \n",
    "    print(f\"\\nüìπ Processing video: {video_path.name}\")\n",
    "    print(f\"   Resolution: {width}x{height} @ {fps:.2f} fps\")\n",
    "    print(f\"   Total frames: {total_frames}\\n\")\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing\")\n",
    "    frame_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        results = model(frame_rgb)\n",
    "        inf_time = time.time() - start_time\n",
    "        inference_times.append(inf_time)\n",
    "        \n",
    "        # Filter detections\n",
    "        detections = results[results['confidence'] > conf_threshold]\n",
    "        if person_only:\n",
    "            detections = detections[detections['class_id'] == 0]  # Person class in COCO\n",
    "        \n",
    "        detection_counts.append(len(detections))\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = map(int, det['bbox'])\n",
    "            conf = det['confidence']\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Person: {conf:.2f}\", (x1, y1-10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Add frame info\n",
    "        cv2.putText(frame, f\"Frame: {frame_idx} | FPS: {1/inf_time:.1f}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Print statistics\n",
    "    avg_inf_time = np.mean(inference_times)\n",
    "    avg_fps = 1 / avg_inf_time\n",
    "    avg_detections = np.mean(detection_counts)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Video processing complete!\")\n",
    "    print(f\"\\nüìä Performance Statistics:\")\n",
    "    print(f\"   Average inference time: {avg_inf_time*1000:.2f} ms\")\n",
    "    print(f\"   Average FPS: {avg_fps:.2f}\")\n",
    "    print(f\"   Average detections/frame: {avg_detections:.2f}\")\n",
    "    print(f\"   Output saved to: {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': avg_inf_time,\n",
    "        'avg_fps': avg_fps,\n",
    "        'avg_detections': avg_detections,\n",
    "        'inference_times': inference_times,\n",
    "        'detection_counts': detection_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b25f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first video in folder\n",
    "video_files = list(Path('/content/test_data/videos').glob('*.mp4'))\n",
    "\n",
    "if video_files:\n",
    "    test_video = video_files[0]\n",
    "    output_video = Path('/content/test_data/outputs') / f\"{test_video.stem}_rfdetr.mp4\"\n",
    "    \n",
    "    stats = detect_video(model, test_video, output_video, \n",
    "                        conf_threshold=0.5, person_only=True)\n",
    "else:\n",
    "    print(\"‚ùå No videos found in /content/test_data/videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d17266",
   "metadata": {},
   "source": [
    "## 6. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference times and detection counts over time\n",
    "if 'stats' in locals():\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Inference time plot\n",
    "    ax1.plot(stats['inference_times'], alpha=0.6)\n",
    "    ax1.axhline(y=stats['avg_inference_time'], color='r', linestyle='--', \n",
    "                label=f\"Avg: {stats['avg_inference_time']*1000:.2f} ms\")\n",
    "    ax1.set_xlabel('Frame Number')\n",
    "    ax1.set_ylabel('Inference Time (s)')\n",
    "    ax1.set_title('Inference Time per Frame')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Detection count plot\n",
    "    ax2.plot(stats['detection_counts'], alpha=0.6, color='green')\n",
    "    ax2.axhline(y=stats['avg_detections'], color='r', linestyle='--',\n",
    "                label=f\"Avg: {stats['avg_detections']:.2f} persons\")\n",
    "    ax2.set_xlabel('Frame Number')\n",
    "    ax2.set_ylabel('Number of Detections')\n",
    "    ax2.set_title('Person Detections per Frame')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b972d",
   "metadata": {},
   "source": [
    "## 7. ü•ä Head-to-Head Comparison: RF-DETR vs YOLO v8s\n",
    "\n",
    "**Test Setup:**\n",
    "- Video: kohli_nets.mp4 (same video used in our pipeline testing)\n",
    "- Models: RF-DETR vs YOLO v8s\n",
    "- Metrics: Speed (FPS), Detection Count, Consistency\n",
    "- Goal: Determine if RF-DETR is faster/better for our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040243cb",
   "metadata": {},
   "source": [
    "### Known Baseline from Our Pipeline\n",
    "\n",
    "From our recent pipeline testing on kohli_nets.mp4:\n",
    "- **YOLO v8s**: 39.8 FPS (detection stage)\n",
    "- **Input resolution**: 1280x720\n",
    "- **Confidence threshold**: 0.5\n",
    "- **Total frames**: 2027 frames\n",
    "- **Duration**: 81.08s\n",
    "\n",
    "RF-DETR needs to match or beat this performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load YOLO for comparison\n",
    "# !pip install -q ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO('yolov8s.pt')\n",
    "print(\"‚úÖ YOLO model loaded for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detectors_headtohead(rfdetr_model, yolo_model, video_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Head-to-head comparison: RF-DETR vs YOLO v8s on kohli_nets.mp4\n",
    "    \n",
    "    Matches our pipeline settings:\n",
    "    - Same video (kohli_nets.mp4)\n",
    "    - Same confidence threshold (0.5)\n",
    "    - Person detection only\n",
    "    - Full video processing\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    rfdetr_times = []\n",
    "    yolo_times = []\n",
    "    rfdetr_counts = []\n",
    "    yolo_counts = []\n",
    "    \n",
    "    print(f\"\\n‚ö° Head-to-Head Comparison\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Video: kohli_nets.mp4\")\n",
    "    print(f\"Resolution: {width}x{height} @ {fps:.2f} fps\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Confidence threshold: {conf_threshold}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing\")\n",
    "    frame_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # RF-DETR (Roboflow inference API)\n",
    "        try:\n",
    "            # Convert to PIL Image for RF-DETR\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            start = time.time()\n",
    "            predictions = rfdetr_model.infer(pil_image, confidence=conf_threshold)[0]\n",
    "            rfdetr_time = time.time() - start\n",
    "            rfdetr_times.append(rfdetr_time)\n",
    "            \n",
    "            # Count persons (class_id=1 in RF-DETR)\n",
    "            person_count = sum(1 for pred in predictions.predictions if pred.class_id == 1)\n",
    "            rfdetr_counts.append(person_count)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  RF-DETR error at frame {frame_idx}: {e}\")\n",
    "            rfdetr_times.append(0)\n",
    "            rfdetr_counts.append(0)\n",
    "        \n",
    "        # YOLO v8s\n",
    "        try:\n",
    "            start = time.time()\n",
    "            yolo_results = yolo_model(frame_rgb, verbose=False)\n",
    "            yolo_time = time.time() - start\n",
    "            yolo_times.append(yolo_time)\n",
    "            \n",
    "            # Count persons (class_id=0)\n",
    "            yolo_dets = yolo_results[0].boxes\n",
    "            person_count = sum(1 for b in yolo_dets if b.cls == 0 and b.conf > conf_threshold)\n",
    "            yolo_counts.append(person_count)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  YOLO error at frame {frame_idx}: {e}\")\n",
    "            yolo_times.append(0)\n",
    "            yolo_counts.append(0)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    \n",
    "    # Calculate statistics (filter out errors)\n",
    "    rfdetr_times_valid = [t for t in rfdetr_times if t > 0]\n",
    "    yolo_times_valid = [t for t in yolo_times if t > 0]\n",
    "    \n",
    "    rfdetr_avg_time = np.mean(rfdetr_times_valid)\n",
    "    rfdetr_avg_fps = 1 / rfdetr_avg_time\n",
    "    rfdetr_std_time = np.std(rfdetr_times_valid)\n",
    "    rfdetr_avg_dets = np.mean(rfdetr_counts)\n",
    "    \n",
    "    yolo_avg_time = np.mean(yolo_times_valid)\n",
    "    yolo_avg_fps = 1 / yolo_avg_time\n",
    "    yolo_std_time = np.std(yolo_times_valid)\n",
    "    yolo_avg_dets = np.mean(yolo_counts)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèÅ COMPARISON RESULTS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(f\"{'Metric':<35} {'RF-DETR':<18} {'YOLO v8s':<18} {'Winner':<10}\")\n",
    "    print(f\"{'-'*85}\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    winner_speed = \"RF-DETR ‚úì\" if rfdetr_avg_fps > yolo_avg_fps else \"YOLO ‚úì\"\n",
    "    print(f\"{'Avg Inference Time (ms)':<35} {rfdetr_avg_time*1000:<18.2f} {yolo_avg_time*1000:<18.2f} {winner_speed:<10}\")\n",
    "    print(f\"{'Avg FPS':<35} {rfdetr_avg_fps:<18.2f} {yolo_avg_fps:<18.2f} {winner_speed:<10}\")\n",
    "    print(f\"{'Std Dev (ms)':<35} {rfdetr_std_time*1000:<18.2f} {yolo_std_time*1000:<18.2f}\")\n",
    "    \n",
    "    # Detection comparison\n",
    "    winner_dets = \"Same\" if abs(rfdetr_avg_dets - yolo_avg_dets) < 0.1 else (\"RF-DETR\" if rfdetr_avg_dets > yolo_avg_dets else \"YOLO\")\n",
    "    print(f\"{'Avg Detections/Frame':<35} {rfdetr_avg_dets:<18.2f} {yolo_avg_dets:<18.2f} {winner_dets:<10}\")\n",
    "    \n",
    "    # Speed improvement\n",
    "    speedup = ((yolo_avg_time - rfdetr_avg_time) / yolo_avg_time) * 100\n",
    "    print(f\"\\n{'Speed Difference:':<35} {speedup:+.1f}% {'(RF-DETR faster)' if speedup > 0 else '(YOLO faster)'}\")\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_fps = 39.8  # From our pipeline\n",
    "    print(f\"\\n{'Baseline (Pipeline YOLO):':<35} {baseline_fps:.1f} FPS\")\n",
    "    print(f\"{'Current YOLO:':<35} {yolo_avg_fps:.1f} FPS\")\n",
    "    print(f\"{'RF-DETR:':<35} {rfdetr_avg_fps:.1f} FPS\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # 1. Inference time distribution\n",
    "    axes[0, 0].hist([np.array(rfdetr_times_valid)*1000, np.array(yolo_times_valid)*1000], \n",
    "                    label=['RF-DETR', 'YOLO v8s'], bins=30, alpha=0.7)\n",
    "    axes[0, 0].axvline(x=rfdetr_avg_time*1000, color='blue', linestyle='--', alpha=0.8)\n",
    "    axes[0, 0].axvline(x=yolo_avg_time*1000, color='orange', linestyle='--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Inference Time (ms)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Inference Time Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. FPS over time\n",
    "    rfdetr_fps_series = [1000/t if t > 0 else 0 for t in rfdetr_times]\n",
    "    yolo_fps_series = [1000/t if t > 0 else 0 for t in yolo_times]\n",
    "    axes[0, 1].plot(rfdetr_fps_series, alpha=0.6, label='RF-DETR', linewidth=0.5)\n",
    "    axes[0, 1].plot(yolo_fps_series, alpha=0.6, label='YOLO v8s', linewidth=0.5)\n",
    "    axes[0, 1].axhline(y=rfdetr_avg_fps, color='blue', linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].axhline(y=yolo_avg_fps, color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('Frame Number')\n",
    "    axes[0, 1].set_ylabel('FPS')\n",
    "    axes[0, 1].set_title('Processing Speed Over Time')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Detection count comparison\n",
    "    axes[1, 0].scatter(rfdetr_counts, yolo_counts, alpha=0.3, s=10)\n",
    "    max_count = max(max(rfdetr_counts), max(yolo_counts))\n",
    "    axes[1, 0].plot([0, max_count], [0, max_count], 'r--', alpha=0.5, label='Perfect agreement')\n",
    "    axes[1, 0].set_xlabel('RF-DETR Detections')\n",
    "    axes[1, 0].set_ylabel('YOLO Detections')\n",
    "    axes[1, 0].set_title('Detection Count Comparison (per frame)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Detection counts over time\n",
    "    axes[1, 1].plot(rfdetr_counts, alpha=0.6, label='RF-DETR', linewidth=0.8)\n",
    "    axes[1, 1].plot(yolo_counts, alpha=0.6, label='YOLO v8s', linewidth=0.8)\n",
    "    axes[1, 1].set_xlabel('Frame Number')\n",
    "    axes[1, 1].set_ylabel('Number of Detections')\n",
    "    axes[1, 1].set_title('Detections Over Time')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'rf_detr': {\n",
    "            'avg_time': rfdetr_avg_time,\n",
    "            'avg_fps': rfdetr_avg_fps,\n",
    "            'std_time': rfdetr_std_time,\n",
    "            'avg_dets': rfdetr_avg_dets,\n",
    "            'times': rfdetr_times,\n",
    "            'counts': rfdetr_counts\n",
    "        },\n",
    "        'yolo': {\n",
    "            'avg_time': yolo_avg_time,\n",
    "            'avg_fps': yolo_avg_fps,\n",
    "            'std_time': yolo_std_time,\n",
    "            'avg_dets': yolo_avg_dets,\n",
    "            'times': yolo_times,\n",
    "            'counts': yolo_counts\n",
    "        },\n",
    "        'speedup_percent': speedup\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1856906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run head-to-head comparison: RF-DETR vs YOLO v8s on kohli_nets.mp4\n",
    "from pathlib import Path\n",
    "\n",
    "# Define path\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "print(f\"üìÅ Pre-flight checks:\")\n",
    "print(f\"   Video exists: {video_path.exists()}\")\n",
    "print(f\"   RF-DETR model loaded: {'model' in locals()}\")\n",
    "print(f\"   YOLO model loaded: {'yolo_model' in locals()}\")\n",
    "\n",
    "# Check all prerequisites\n",
    "if video_path.exists() and 'model' in locals() and 'yolo_model' in locals():\n",
    "    print(f\"\\n‚úÖ All checks passed!\")\n",
    "    print(f\"\\nü•ä Starting head-to-head comparison: RF-DETR vs YOLO v8s\")\n",
    "    print(f\"   Video: {video_path.name}\")\n",
    "    print(f\"   Total frames: 2027\")\n",
    "    print(f\"   Estimated time: 3-5 minutes\")\n",
    "    print(f\"   {'='*70}\\n\")\n",
    "    \n",
    "    # Run comparison\n",
    "    comparison_results = compare_detectors_headtohead(model, yolo_model, video_path, conf_threshold=0.5)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Comparison complete!\")\n",
    "    print(f\"   Results stored in 'comparison_results' variable\")\n",
    "    print(f\"   Run Cell 28 or 31 to see the decision summary\")\n",
    "    \n",
    "elif not video_path.exists():\n",
    "    print(f\"\\n‚ùå Video not found!\")\n",
    "    print(f\"   Expected location: {video_path}\")\n",
    "    print(f\"   Solution: Run Cell 8 to copy kohli_nets.mp4 from Google Drive\")\n",
    "    \n",
    "elif 'model' not in locals():\n",
    "    print(f\"\\n‚ùå RF-DETR model not loaded!\")\n",
    "    print(f\"   Solution: Run Cell 11 to load RF-DETR model\")\n",
    "    \n",
    "elif 'yolo_model' not in locals():\n",
    "    print(f\"\\n‚ùå YOLO model not loaded!\")\n",
    "    print(f\"   Solution: Run Cell 20 to load YOLO v8s model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc916ea",
   "metadata": {},
   "source": [
    "## 8. Extract Detection Data for Pose Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54281ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detections_for_pipeline(model, video_path, output_npz_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Extract person detections in format compatible with our tracking pipeline\n",
    "    \n",
    "    Output format:\n",
    "    - detections.npz containing:\n",
    "        - bboxes: (N, 4) array [x1, y1, x2, y2]\n",
    "        - confidences: (N,) array\n",
    "        - frame_ids: (N,) array\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    all_bboxes = []\n",
    "    all_confidences = []\n",
    "    all_frame_ids = []\n",
    "    \n",
    "    print(f\"\\nüì¶ Extracting detections for pipeline...\\n\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    pbar = tqdm(total=total_frames, desc=\"Extracting\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model(frame_rgb)\n",
    "        \n",
    "        # Filter for persons with confidence threshold\n",
    "        person_dets = results[(results['class_id'] == 0) & \n",
    "                             (results['confidence'] > conf_threshold)]\n",
    "        \n",
    "        for det in person_dets:\n",
    "            all_bboxes.append(det['bbox'])\n",
    "            all_confidences.append(det['confidence'])\n",
    "            all_frame_ids.append(frame_idx)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    \n",
    "    # Save to NPZ\n",
    "    np.savez(\n",
    "        output_npz_path,\n",
    "        bboxes=np.array(all_bboxes),\n",
    "        confidences=np.array(all_confidences),\n",
    "        frame_ids=np.array(all_frame_ids)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved {len(all_bboxes)} detections to: {output_npz_path}\")\n",
    "    print(f\"   Total frames: {frame_idx}\")\n",
    "    print(f\"   Avg detections/frame: {len(all_bboxes)/frame_idx:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract detections for pipeline integration\n",
    "if video_files:\n",
    "    output_npz = Path('/content/test_data/outputs') / f\"{test_video.stem}_rfdetr_detections.npz\"\n",
    "    extract_detections_for_pipeline(model, test_video, output_npz, conf_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a87d19",
   "metadata": {},
   "source": [
    "## 9. üìã Summary and Integration Decision\n",
    "\n",
    "Based on the head-to-head comparison results on kohli_nets.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fb329",
   "metadata": {},
   "source": [
    "## üöÄ YOLO v8s ONNX Export - Can We Go Faster?\n",
    "\n",
    "Current baseline: **39.8 FPS** (PyTorch YOLO v8s)\n",
    "\n",
    "Let's export YOLO v8s to ONNX and test if we can squeeze more performance with ONNX Runtime optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Exporting YOLO v8s to ONNX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# Load YOLO v8s model\n",
    "yolo_model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Export to ONNX with simplification\n",
    "onnx_path = Path('/content/models/yolo_onnx')\n",
    "onnx_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Export settings:\")\n",
    "print(f\"   Model: YOLOv8s\")\n",
    "print(f\"   Format: ONNX\")\n",
    "print(f\"   Simplify: Yes\")\n",
    "print(f\"   Dynamic batching: Yes\")\n",
    "print(f\"   Output: {onnx_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ Exporting...\")\n",
    "\n",
    "# Export (Ultralytics handles everything)\n",
    "yolo_model.export(\n",
    "    format='onnx',\n",
    "    simplify=True,\n",
    "    dynamic=True,  # Enable dynamic batch size\n",
    "    imgsz=640,     # YOLO's native resolution\n",
    ")\n",
    "\n",
    "# Move exported file to our directory\n",
    "import shutil\n",
    "exported_onnx = Path('yolov8s.onnx')\n",
    "target_onnx = onnx_path / 'yolov8s.onnx'\n",
    "\n",
    "if exported_onnx.exists():\n",
    "    shutil.move(str(exported_onnx), str(target_onnx))\n",
    "    print(f\"\\n‚úÖ ONNX export successful!\")\n",
    "    print(f\"   Saved to: {target_onnx}\")\n",
    "    \n",
    "    # Verify dynamic batching\n",
    "    import onnx\n",
    "    onnx_model = onnx.load(str(target_onnx))\n",
    "    for inp in onnx_model.graph.input:\n",
    "        print(f\"\\nüìä Input '{inp.name}' shape:\")\n",
    "        dims = inp.type.tensor_type.shape.dim\n",
    "        shape_str = [d.dim_param if d.dim_param else str(d.dim_value) for d in dims]\n",
    "        print(f\"   {shape_str}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ YOLO v8s ONNX ready for testing!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Export failed - file not found\")\n",
    "    print(f\"   Expected: {exported_onnx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Loading YOLO v8s ONNX with optimized ONNX Runtime\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path('/content/models/yolo_onnx/yolov8s.onnx')\n",
    "\n",
    "# Optimized ONNX Runtime session (same config as RF-DETR best)\n",
    "providers = [\n",
    "    ('CUDAExecutionProvider', {\n",
    "        'device_id': 0,\n",
    "        'gpu_mem_limit': 4 * 1024 * 1024 * 1024,  # 4GB\n",
    "        'arena_extend_strategy': 'kSameAsRequested',\n",
    "        'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "        'do_copy_in_default_stream': True,\n",
    "    }),\n",
    "    'CPUExecutionProvider'\n",
    "]\n",
    "\n",
    "yolo_onnx_session = ort.InferenceSession(str(onnx_path), providers=providers)\n",
    "\n",
    "# Get input/output info\n",
    "input_name = yolo_onnx_session.get_inputs()[0].name\n",
    "input_shape = yolo_onnx_session.get_inputs()[0].shape\n",
    "output_names = [o.name for o in yolo_onnx_session.get_outputs()]\n",
    "output_shapes = [o.shape for o in yolo_onnx_session.get_outputs()]\n",
    "\n",
    "print(f\"\\n‚úÖ YOLO v8s ONNX loaded successfully!\")\n",
    "print(f\"\\nüìä Model Info:\")\n",
    "print(f\"   Input name: {input_name}\")\n",
    "print(f\"   Input shape: {input_shape}\")\n",
    "print(f\"   Output names: {output_names}\")\n",
    "print(f\"   Output shapes: {output_shapes}\")\n",
    "\n",
    "if isinstance(input_shape[0], str) or 'batch' in str(input_shape[0]).lower():\n",
    "    print(f\"\\nüéâ Dynamic batching confirmed!\")\n",
    "else:\n",
    "    print(f\"\\n   Batch dimension: {input_shape[0]}\")\n",
    "\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Debugging ONNX Performance Issue\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if CUDA provider is actually being used\n",
    "providers = yolo_onnx_session.get_providers()\n",
    "print(f\"\\nüìä Active providers: {providers}\")\n",
    "\n",
    "if 'CUDAExecutionProvider' not in providers:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: CUDA provider not active!\")\n",
    "    print(f\"   ONNX is running on CPU, which explains the slowness\")\n",
    "    print(f\"   This is likely why we're seeing 2.4 FPS instead of 45+ FPS\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ CUDA provider is active\")\n",
    "\n",
    "# Quick single inference test to see actual time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
    "\n",
    "print(f\"\\nüß™ Running warm-up inference...\")\n",
    "for _ in range(5):\n",
    "    yolo_onnx_session.run(None, {input_name: dummy_input})\n",
    "\n",
    "print(f\"üß™ Running timed inference...\")\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.perf_counter()\n",
    "    yolo_onnx_session.run(None, {input_name: dummy_input})\n",
    "    end = time.perf_counter()\n",
    "    times.append(end - start)\n",
    "\n",
    "avg_time = np.mean(times) * 1000\n",
    "fps = 1.0 / np.mean(times)\n",
    "\n",
    "print(f\"\\nüìä Direct inference test:\")\n",
    "print(f\"   Avg time: {avg_time:.2f} ms\")\n",
    "print(f\"   FPS: {fps:.2f}\")\n",
    "\n",
    "if fps < 10:\n",
    "    print(f\"\\n‚ùå Still very slow - ONNX Runtime may not have CUDA support\")\n",
    "    print(f\"   Or the model has compatibility issues\")\n",
    "elif fps < 39.8:\n",
    "    print(f\"\\n‚ö†Ô∏è  Slower than PyTorch ({fps:.2f} vs 39.8 FPS)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Faster than PyTorch baseline!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096f148",
   "metadata": {},
   "source": [
    "## üÜï Last Try: RT-DETRv2-S\n",
    "\n",
    "RT-DETRv2 is the improved version of RT-DETR. Let's see if it's faster than YOLO v8s (39.8 FPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Loading RT-DETRv2 from HuggingFace\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "import torch\n",
    "\n",
    "# Load RT-DETRv2 model (lightest variant - r18vd)\n",
    "print(f\"\\n‚öôÔ∏è  Loading RT-DETRv2-R18 (lightweight)...\")\n",
    "print(f\"   Source: PekingU/rtdetr_v2_r18vd\")\n",
    "\n",
    "model_name = \"PekingU/rtdetr_v2_r18vd\"\n",
    "rtdetr_model = RTDetrV2ForObjectDetection.from_pretrained(model_name)\n",
    "rtdetr_processor = RTDetrImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rtdetr_model = rtdetr_model.to(device)\n",
    "rtdetr_model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ RT-DETRv2-R18 loaded!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in rtdetr_model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ Ready for benchmarking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Benchmarking RT-DETRv2 vs YOLO v8s\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "print(f\"\\nüìπ Test video: {video_path.name}\")\n",
    "print(f\"   YOLO baseline: 39.8 FPS\")\n",
    "print(f\"   Target: Beat 39.8 FPS\")\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"\\nüöÄ Running RT-DETRv2 inference...\")\n",
    "print(f\"   Processing 200 frames for accurate measurement\\n\")\n",
    "\n",
    "times = []\n",
    "frame_count = 0\n",
    "max_frames = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to PIL Image (required by processor)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Preprocess\n",
    "        inputs = rtdetr_processor(images=pil_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inference\n",
    "        outputs = rtdetr_model(**inputs)\n",
    "        \n",
    "        # Post-process (convert to boxes)\n",
    "        target_sizes = torch.tensor([pil_image.size[::-1]]).to(device)\n",
    "        results = rtdetr_processor.post_process_object_detection(\n",
    "            outputs, \n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        times.append(end - start)\n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"   Processed {frame_count}/{max_frames} frames...\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Calculate statistics\n",
    "avg_time = np.mean(times) * 1000\n",
    "std_time = np.std(times) * 1000\n",
    "fps = 1.0 / np.mean(times)\n",
    "min_time = np.min(times) * 1000\n",
    "max_time = np.max(times) * 1000\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä RT-DETRv2-R18 RESULTS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Frames processed: {frame_count}\")\n",
    "print(f\"Avg time: {avg_time:.2f} ms (¬± {std_time:.2f} ms)\")\n",
    "print(f\"Min time: {min_time:.2f} ms\")\n",
    "print(f\"Max time: {max_time:.2f} ms\")\n",
    "print(f\"Throughput: {fps:.2f} FPS\")\n",
    "\n",
    "# Compare to YOLO\n",
    "yolo_fps = 39.8\n",
    "vs_yolo = ((fps - yolo_fps) / yolo_fps) * 100\n",
    "\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"‚öñÔ∏è  FINAL COMPARISON\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "print(f\"RT-DETRv2-R18:  {fps:.2f} FPS\")\n",
    "print(f\"YOLO v8s:       {yolo_fps:.2f} FPS\")\n",
    "print(f\"Difference:     {vs_yolo:+.1f}%\")\n",
    "\n",
    "if fps > yolo_fps:\n",
    "    print(f\"\\nüéâ RT-DETRv2 WINS by {vs_yolo:+.1f}%!\")\n",
    "    print(f\"   Recommendation: Consider RT-DETRv2 for pipeline\")\n",
    "elif fps > yolo_fps * 0.95:  # Within 5%\n",
    "    print(f\"\\nüü° RT-DETRv2 is comparable (within 5%)\")\n",
    "    print(f\"   Recommendation: Either model works fine\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RT-DETRv2 is slower than YOLO\")\n",
    "    print(f\"   Recommendation: Stick with YOLO v8s at 39.8 FPS\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üèÅ FINAL VERDICT: Use {'RT-DETRv2' if fps > yolo_fps else 'YOLO v8s'}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0cf43",
   "metadata": {},
   "source": [
    "## üî• ABSOLUTE FINAL TRY: RT-DETRv4\n",
    "\n",
    "Brand new RT-DETRv4 just released! Claims to be faster than v2. Last chance to beat YOLO's 39.8 FPS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install RT-DETRv4 dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# RT-DETRv4 requirements (most should already be installed)\n",
    "required_packages = [\n",
    "    'faster-coco-eval>=1.6.5',  # Likely missing\n",
    "    'calflops',                  # Likely missing\n",
    "    'scipy',                     # May be missing\n",
    "    'gdown',                     # For downloading checkpoint\n",
    "]\n",
    "\n",
    "# Already installed in Colab: torch, torchvision, PyYAML, tensorboard, transformers\n",
    "\n",
    "print(\"üì¶ Checking RT-DETRv4 dependencies...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        print(f\"   Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ RT-DETRv4 dependencies ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd079b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone RT-DETRv4 repo and download checkpoint/config\n",
    "import gdown\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Clone RT-DETRv4 repository\n",
    "print(\"üì• Cloning RT-DETRv4 repository...\")\n",
    "!git clone -q https://github.com/RT-DETRs/RT-DETRv4.git /content/RT-DETRv4\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "# Download checkpoint from Google Drive\n",
    "checkpoint_url = 'https://drive.google.com/uc?id=1jDAVxblqRPEWed7Hxm6GwcEl7zn72U6z'\n",
    "checkpoint_path = '/content/models/rtv4_hgnetv2_s_coco.pth'\n",
    "print(\"üì• Downloading RT-DETRv4 checkpoint...\")\n",
    "gdown.download(checkpoint_url, checkpoint_path, quiet=False)\n",
    "\n",
    "# Download config from GitHub\n",
    "config_url = 'https://raw.githubusercontent.com/RT-DETRs/RT-DETRv4/main/configs/rtv4/rtv4_hgnetv2_s_coco.yml'\n",
    "config_path = '/content/models/rtv4_hgnetv2_s_coco.yml'\n",
    "print(\"üì• Downloading RT-DETRv4 config...\")\n",
    "response = requests.get(config_url)\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"‚úÖ RT-DETRv4 repository cloned to /content/RT-DETRv4\")\n",
    "print(f\"‚úÖ Checkpoint downloaded: {checkpoint_path}\")\n",
    "print(f\"‚úÖ Config downloaded: {config_path}\")\n",
    "print(f\"   Checkpoint size: {os.path.getsize(checkpoint_path)/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the engine/rtv4 directory structure\n",
    "import os\n",
    "print(\"üìÇ Checking engine/rtv4 structure...\")\n",
    "!ls -la /content/RT-DETRv4/engine/rtv4/\n",
    "\n",
    "print(\"\\nüìÇ Python files in engine/rtv4:\")\n",
    "!ls /content/RT-DETRv4/engine/rtv4/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rtv4.py and __init__.py contents\n",
    "print(\"üìÑ Contents of engine/rtv4/__init__.py:\")\n",
    "!cat /content/RT-DETRv4/engine/rtv4/__init__.py\n",
    "\n",
    "print(\"\\nüìÑ Contents of engine/rtv4/rtv4.py:\")\n",
    "!cat /content/RT-DETRv4/engine/rtv4/rtv4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7927eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the core registry system and how to build model from config\n",
    "print(\"üìÑ Checking engine/core for model builder:\")\n",
    "!ls /content/RT-DETRv4/engine/core/\n",
    "\n",
    "print(\"\\nüìÑ Looking for yaml_utils or model builder:\")\n",
    "!find /content/RT-DETRv4/engine -name \"*.py\" -exec grep -l \"def build_from_config\\|def create\\|yaml_utils\" {} \\; | head -10\n",
    "\n",
    "print(\"\\nüìÑ Checking train.py for model creation example:\")\n",
    "!head -100 /content/RT-DETRv4/train.py | tail -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the entire configs directory to maintain relative paths\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Copying configs directory to models...\")\n",
    "# Copy the entire configs directory\n",
    "if os.path.exists('/content/models/configs'):\n",
    "    shutil.rmtree('/content/models/configs')\n",
    "shutil.copytree('/content/RT-DETRv4/configs', '/content/models/configs')\n",
    "\n",
    "print(\"‚úÖ Configs copied!\")\n",
    "print(\"\\nüìÇ Structure:\")\n",
    "!ls -la /content/models/configs/\n",
    "print(\"\\nüìÇ rtv4 configs:\")\n",
    "!ls /content/models/configs/rtv4/\n",
    "print(\"\\nüìÇ dfine configs (base):\")\n",
    "!ls /content/models/configs/dfine/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ce088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RT-DETRv4 model using the proper config system\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/content/RT-DETRv4')\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from engine.core import YAMLConfig\n",
    "\n",
    "print(\"üîß Building RT-DETRv4-S model from config...\")\n",
    "\n",
    "# Initialize a dummy distributed process group for single GPU inference\n",
    "if not dist.is_initialized():\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "    print(\"   Initialized dummy process group for inference\")\n",
    "\n",
    "# Use the config from the correct location (maintains relative paths)\n",
    "# Disable pretrained backbone since we're loading full checkpoint\n",
    "cfg = YAMLConfig(\n",
    "    '/content/models/configs/rtv4/rtv4_hgnetv2_s_coco.yml',\n",
    "    HGNetv2={'pretrained': False}  # Disable pretrained backbone loading\n",
    ")\n",
    "\n",
    "print(f\"   Config loaded: {list(cfg.yaml_cfg.keys())[:10]}...\")\n",
    "print(f\"   Task: {cfg.yaml_cfg.get('task', 'detection')}\")\n",
    "\n",
    "# Build the model from config\n",
    "rtdetrv4_model = cfg.model.to('cuda')\n",
    "rtdetrv4_model.eval()\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"\\nüì• Loading checkpoint...\")\n",
    "checkpoint = torch.load('/content/models/rtv4_hgnetv2_s_coco.pth', map_location='cpu')\n",
    "print(f\"   Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "\n",
    "# Handle different checkpoint formats\n",
    "if 'ema' in checkpoint:\n",
    "    state_dict = checkpoint['ema']['module']\n",
    "    print(\"   Using EMA weights\")\n",
    "elif 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model']\n",
    "    print(\"   Using model weights\")\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "    print(\"   Using direct state dict\")\n",
    "\n",
    "# Load state dict\n",
    "rtdetrv4_model.load_state_dict(state_dict)\n",
    "\n",
    "# Deploy mode (optimized for inference)\n",
    "rtdetrv4_model.deploy()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in rtdetrv4_model.parameters())\n",
    "\n",
    "print(f\"\\n‚úÖ RT-DETRv4-S loaded!\")\n",
    "print(f\"   Model: rtv4_hgnetv2_s_coco\")\n",
    "print(f\"   Device: cuda\")\n",
    "print(f\"   Parameters: {total_params/1e6:.1f}M\")\n",
    "print(f\"   Mode: Deploy (inference optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937994e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark RT-DETRv4 on kohli_nets.mp4\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "print(\"‚ö° Benchmarking RT-DETRv4-S vs YOLO v8s\")\n",
    "print(f\"   Test video: {video_path}\")\n",
    "print(f\"   Frames to test: 200\")\n",
    "\n",
    "# Standard COCO preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Reset video to start\n",
    "cap_test = cv2.VideoCapture(video_path)\n",
    "frame_times = []\n",
    "num_frames = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_frames):\n",
    "        ret, frame = cap_test.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Preprocess\n",
    "        img_tensor = transform(frame_rgb).unsqueeze(0).to('cuda')\n",
    "        \n",
    "        # Inference - RT-DETRv4 returns dict with 'pred_logits', 'pred_boxes'\n",
    "        outputs = rtdetrv4_model(img_tensor)\n",
    "        \n",
    "        # Synchronize GPU (ensure inference is complete)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        end = time.time()\n",
    "        frame_times.append((end - start) * 1000)  # ms\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   Processed {i+1}/{num_frames} frames...\")\n",
    "\n",
    "cap_test.release()\n",
    "\n",
    "# Calculate statistics\n",
    "avg_time = np.mean(frame_times)\n",
    "std_time = np.std(frame_times)\n",
    "min_time = np.min(frame_times)\n",
    "max_time = np.max(frame_times)\n",
    "fps = 1000 / avg_time\n",
    "\n",
    "print(f\"\\nüìä RT-DETRv4-S Results:\")\n",
    "print(f\"   Frames processed: {len(frame_times)}\")\n",
    "print(f\"   Avg time: {avg_time:.2f} ms (¬± {std_time:.2f} ms)\")\n",
    "print(f\"   Min time: {min_time:.2f} ms\")\n",
    "print(f\"   Max time: {max_time:.2f} ms\")\n",
    "print(f\"   Throughput: {fps:.2f} FPS\")\n",
    "print(f\"   Parameters: 10.4M\")\n",
    "\n",
    "# Final comparison\n",
    "yolo_fps = 39.80\n",
    "rf_detr_fps = 38.24\n",
    "diff_vs_yolo = ((fps - yolo_fps) / yolo_fps) * 100\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  FINAL COMPARISON\")\n",
    "print(f\"   RT-DETRv4-S:    {fps:.2f} FPS (10.4M params)\")\n",
    "print(f\"   YOLO v8s:       {yolo_fps:.2f} FPS (11.1M params)\")\n",
    "print(f\"   RF-DETR ONNX:   {rf_detr_fps:.2f} FPS\")\n",
    "print(f\"   Difference:     {diff_vs_yolo:+.1f}%\")\n",
    "\n",
    "if fps > yolo_fps:\n",
    "    print(f\"\\nüéâüéâüéâ RT-DETRv4 WINS! FASTER THAN YOLO!\")\n",
    "    print(f\"üèÜ NEW WINNER: RT-DETRv4-S at {fps:.2f} FPS\")\n",
    "elif fps > rf_detr_fps:\n",
    "    print(f\"\\nü•à RT-DETRv4 beats RF-DETR ({rf_detr_fps:.2f} FPS) but not YOLO\")\n",
    "    print(f\"   Second place: RT-DETRv4-S\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RT-DETRv4 is slower than both YOLO and RF-DETR\")\n",
    "    print(f\"üèÅ YOLO v8s remains the winner at {yolo_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f182d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check their official inference/benchmark scripts\n",
    "print(\"üìÇ Looking for inference/benchmark scripts:\")\n",
    "!find /content/RT-DETRv4 -name \"*infer*\" -o -name \"*test*\" -o -name \"*benchmark*\" | grep -E \"\\.py$\" | head -20\n",
    "\n",
    "print(\"\\nüìÇ Checking tools directory:\")\n",
    "!ls -la /content/RT-DETRv4/tools/\n",
    "\n",
    "print(\"\\nüìÑ Checking if there's a speed test script:\")\n",
    "!grep -r \"FPS\\|throughput\\|latency\" /content/RT-DETRv4/tools/*.py 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the actual input size and preprocessing they use\n",
    "print(\"üìÑ Checking config for input size:\")\n",
    "!grep -E \"min_size|max_size|img_size|size|resize\" /content/models/configs/rtv4/rtv4_hgnetv2_s_coco.yml\n",
    "\n",
    "print(\"\\nüìÑ Checking base config:\")\n",
    "!grep -E \"min_size|max_size|img_size|size|resize\" /content/models/configs/dfine/dfine_hgnetv2_s_coco.yml\n",
    "\n",
    "print(\"\\nüìÑ Checking dataset config:\")\n",
    "!cat /content/models/configs/dataset/coco_detection.yml | grep -A5 -B5 \"size\\|resize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1977263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check their official torch inference script\n",
    "print(\"üìÑ Official torch_inf.py script:\")\n",
    "!cat /content/RT-DETRv4/tools/inference/torch_inf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f808f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild model with postprocessor wrapper (official way)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print(\"üîß Rebuilding RT-DETRv4 with official postprocessor...\")\n",
    "\n",
    "# Wrap model with postprocessor (their official approach)\n",
    "class OfficialModel(nn.Module):\n",
    "    def __init__(self, model, postprocessor):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.postprocessor = postprocessor\n",
    "    \n",
    "    def forward(self, images, orig_target_sizes):\n",
    "        outputs = self.model(images)\n",
    "        outputs = self.postprocessor(outputs, orig_target_sizes)\n",
    "        return outputs\n",
    "\n",
    "# Build postprocessor from config\n",
    "postprocessor = cfg.postprocessor.deploy()\n",
    "\n",
    "# Wrap our model\n",
    "rtdetrv4_official = OfficialModel(rtdetrv4_model, postprocessor).to('cuda')\n",
    "rtdetrv4_official.eval()\n",
    "\n",
    "print(\"‚úÖ Official model wrapper created!\")\n",
    "print(\"   Model: RTv4 with PostProcessor\")\n",
    "print(\"   This matches their torch_inf.py approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with official inference approach\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "print(\"‚ö° Benchmarking RT-DETRv4 (OFFICIAL METHOD)\")\n",
    "print(f\"   Test video: {video_path}\")\n",
    "print(f\"   Frames to test: 200\")\n",
    "\n",
    "# Official transforms (exactly as they use)\n",
    "transforms = T.Compose([\n",
    "    T.Resize((640, 640)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Reset video\n",
    "cap_test = cv2.VideoCapture(video_path)\n",
    "frame_times = []\n",
    "num_frames = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_frames):\n",
    "        ret, frame = cap_test.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to PIL (official way)\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        w, h = frame_pil.size\n",
    "        orig_size = torch.tensor([[w, h]]).to('cuda')\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Official preprocessing\n",
    "        im_data = transforms(frame_pil).unsqueeze(0).to('cuda')\n",
    "        \n",
    "        # Official inference (model + postprocessor)\n",
    "        outputs = rtdetrv4_official(im_data, orig_size)\n",
    "        \n",
    "        # Synchronize\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        end = time.time()\n",
    "        frame_times.append((end - start) * 1000)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   Processed {i+1}/{num_frames} frames...\")\n",
    "\n",
    "cap_test.release()\n",
    "\n",
    "# Statistics\n",
    "avg_time = np.mean(frame_times)\n",
    "std_time = np.std(frame_times)\n",
    "min_time = np.min(frame_times)\n",
    "max_time = np.max(frame_times)\n",
    "fps = 1000 / avg_time\n",
    "\n",
    "print(f\"\\nüìä RT-DETRv4-S Results (OFFICIAL METHOD):\")\n",
    "print(f\"   Frames processed: {len(frame_times)}\")\n",
    "print(f\"   Avg time: {avg_time:.2f} ms (¬± {std_time:.2f} ms)\")\n",
    "print(f\"   Min time: {min_time:.2f} ms\")\n",
    "print(f\"   Max time: {max_time:.2f} ms\")\n",
    "print(f\"   Throughput: {fps:.2f} FPS\")\n",
    "print(f\"   Parameters: 10.4M\")\n",
    "\n",
    "# Compare to their claim\n",
    "official_claim = 273\n",
    "diff_vs_claim = ((fps - official_claim) / official_claim) * 100\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  COMPARISON\")\n",
    "print(f\"   Our result:     {fps:.2f} FPS\")\n",
    "print(f\"   Official claim: {official_claim} FPS (T4)\")\n",
    "print(f\"   Difference:     {diff_vs_claim:+.1f}%\")\n",
    "print(f\"   YOLO v8s:       39.80 FPS\")\n",
    "print(f\"   RF-DETR ONNX:   38.24 FPS\")\n",
    "\n",
    "if fps > 39.80:\n",
    "    print(f\"\\nüéâüéâüéâ RT-DETRv4 BEATS YOLO!\")\n",
    "    print(f\"üèÜ NEW WINNER: RT-DETRv4-S\")\n",
    "elif fps > 38.24:\n",
    "    print(f\"\\nü•à RT-DETRv4 beats RF-DETR but not YOLO\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Still slower than YOLO\")\n",
    "    \n",
    "print(f\"\\nüí° Note: Their 273 FPS claim likely uses:\")\n",
    "print(f\"   - Batched inference\")\n",
    "print(f\"   - TensorRT/ONNX optimization\")\n",
    "print(f\"   - Different hardware/driver setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ONNX export and inference tools\n",
    "print(\"üìÇ Checking inference tools:\")\n",
    "!ls -la /content/RT-DETRv4/tools/inference/\n",
    "\n",
    "print(\"\\nüìÑ Check if pre-built ONNX models exist:\")\n",
    "!cat /content/RT-DETRv4/README.md | grep -A5 -B5 \"onnx\\|ONNX\\|export\" | head -40\n",
    "\n",
    "print(\"\\nüìÑ Check for export script:\")\n",
    "!ls /content/RT-DETRv4/tools/ | grep -i export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ONNX export dependencies\n",
    "!pip install -q onnx onnxsim\n",
    "\n",
    "print(\"‚úÖ ONNX export dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export RT-DETRv4 to ONNX (without simplification to save time)\n",
    "import os\n",
    "os.chdir('/content/RT-DETRv4')\n",
    "\n",
    "print(\"üì§ Exporting RT-DETRv4-S to ONNX (skipping simplification)...\")\n",
    "# Remove --check flag to skip onnxsim validation\n",
    "!python tools/deployment/export_onnx.py \\\n",
    "    -c /content/models/configs/rtv4/rtv4_hgnetv2_s_coco.yml \\\n",
    "    -r /content/models/rtv4_hgnetv2_s_coco.pth\n",
    "\n",
    "print(\"\\nüìÇ Checking for exported ONNX file:\")\n",
    "!ls -lh *.onnx 2>/dev/null || echo \"No ONNX file in /content/RT-DETRv4\"\n",
    "\n",
    "# Check model output directory too\n",
    "!ls -lh model*.onnx outputs/*.onnx 2>/dev/null || echo \"Checking other locations...\"\n",
    "\n",
    "os.chdir('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search README for pre-built ONNX download links\n",
    "print(\"üîç Searching for pre-built ONNX model links:\")\n",
    "!grep -i \"onnx\" /content/RT-DETRv4/README.md | grep -E \"http|drive.google\"\n",
    "\n",
    "print(\"\\nüîç Checking if ONNX models are in releases:\")\n",
    "!grep -i \"release\\|download\" /content/RT-DETRv4/README.md | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Benchmarking YOLO v8s ONNX vs PyTorch\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "video_path = Path('/content/test_data/videos/kohli_nets.mp4')\n",
    "\n",
    "print(f\"\\nüìπ Test video: {video_path.name}\")\n",
    "print(f\"   PyTorch baseline: 39.8 FPS\")\n",
    "print(f\"   Target: Beat 39.8 FPS\")\n",
    "\n",
    "# Preprocess frames for YOLO (640x640)\n",
    "print(f\"\\nüì¶ Preprocessing frames...\")\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "num_frames = 200\n",
    "frames = []\n",
    "\n",
    "for i in range(num_frames):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # YOLO preprocessing (640x640, letterbox)\n",
    "    frame_resized = cv2.resize(frame, (640, 640))\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    frame_norm = frame_rgb.astype(np.float32) / 255.0\n",
    "    frame_norm = np.transpose(frame_norm, (2, 0, 1))  # HWC -> CHW\n",
    "    frame_norm = np.expand_dims(frame_norm, axis=0)  # Add batch dimension\n",
    "    frames.append(frame_norm)\n",
    "\n",
    "cap.release()\n",
    "print(f\"‚úÖ Preprocessed {len(frames)} frames\")\n",
    "\n",
    "# Test 1: ONNX with IO Binding (single frame)\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"Test 1: YOLO ONNX - Single Frame IO Binding\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "times = []\n",
    "for frame_batch in frames:\n",
    "    io_binding = yolo_onnx_session.io_binding()\n",
    "    io_binding.bind_cpu_input(input_name, frame_batch)\n",
    "    for output_name in output_names:\n",
    "        io_binding.bind_output(output_name)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    yolo_onnx_session.run_with_iobinding(io_binding)\n",
    "    end = time.perf_counter()\n",
    "    times.append(end - start)\n",
    "\n",
    "avg_time = np.mean(times) * 1000\n",
    "std_time = np.std(times) * 1000\n",
    "fps_single = 1.0 / np.mean(times)\n",
    "\n",
    "print(f\"   Frames: {len(frames)}\")\n",
    "print(f\"   Avg time: {avg_time:.2f} ms (¬± {std_time:.2f} ms)\")\n",
    "print(f\"   Throughput: {fps_single:.2f} FPS\")\n",
    "\n",
    "vs_baseline = ((fps_single - 39.8) / 39.8) * 100\n",
    "if fps_single > 39.8:\n",
    "    print(f\"   ‚úÖ {vs_baseline:+.1f}% FASTER than PyTorch!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  {vs_baseline:.1f}% vs PyTorch baseline\")\n",
    "\n",
    "# Test 2: Try batch_size=2 (if dynamic batching works)\n",
    "if isinstance(input_shape[0], str) or 'batch' in str(input_shape[0]).lower():\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Test 2: YOLO ONNX - Batch Size 2\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    times = []\n",
    "    num_batches = len(frames) // 2\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch = np.concatenate([frames[i*2], frames[i*2+1]], axis=0)\n",
    "        \n",
    "        io_binding = yolo_onnx_session.io_binding()\n",
    "        io_binding.bind_cpu_input(input_name, batch)\n",
    "        for output_name in output_names:\n",
    "            io_binding.bind_output(output_name)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        yolo_onnx_session.run_with_iobinding(io_binding)\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_batch_time = np.mean(times) * 1000\n",
    "    fps_batch2 = 2.0 / np.mean(times)  # 2 frames per batch\n",
    "    \n",
    "    print(f\"   Batches: {num_batches}\")\n",
    "    print(f\"   Avg batch time: {avg_batch_time:.2f} ms\")\n",
    "    print(f\"   Throughput: {fps_batch2:.2f} FPS\")\n",
    "    \n",
    "    vs_baseline = ((fps_batch2 - 39.8) / 39.8) * 100\n",
    "    vs_single = ((fps_batch2 - fps_single) / fps_single) * 100\n",
    "    \n",
    "    if fps_batch2 > 39.8:\n",
    "        print(f\"   ‚úÖ {vs_baseline:+.1f}% FASTER than PyTorch!\")\n",
    "    print(f\"   vs Single: {vs_single:+.1f}%\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"üéØ YOLO v8s PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Configuration                 | Throughput | vs PyTorch Baseline\")\n",
    "print(f\"------------------------------|------------|--------------------\")\n",
    "print(f\"PyTorch YOLO v8s              |  39.80 FPS |      0.0% (baseline)\")\n",
    "print(f\"ONNX Single Frame             | {fps_single:6.2f} FPS | {((fps_single - 39.8) / 39.8) * 100:+6.1f}%\")\n",
    "\n",
    "best_fps = fps_single\n",
    "best_config = \"ONNX Single Frame\"\n",
    "\n",
    "if isinstance(input_shape[0], str) or 'batch' in str(input_shape[0]).lower():\n",
    "    print(f\"ONNX Batch=2                  | {fps_batch2:6.2f} FPS | {((fps_batch2 - 39.8) / 39.8) * 100:+6.1f}%\")\n",
    "    if fps_batch2 > best_fps:\n",
    "        best_fps = fps_batch2\n",
    "        best_config = \"ONNX Batch=2\"\n",
    "\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"üèÜ BEST YOLO CONFIGURATION\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "print(f\"   Config: {best_config}\")\n",
    "print(f\"   Throughput: {best_fps:.2f} FPS\")\n",
    "print(f\"   Improvement: {((best_fps - 39.8) / 39.8) * 100:+.1f}% vs PyTorch\")\n",
    "\n",
    "if best_fps > 39.8:\n",
    "    print(f\"\\n   ‚úÖ ONNX optimization SUCCESSFUL!\")\n",
    "    print(f\"   Recommendation: Use YOLO ONNX in pipeline\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  No significant improvement from ONNX\")\n",
    "    print(f\"   Recommendation: PyTorch YOLO is fine\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230451f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate decision summary\n",
    "if 'comparison_results' in locals():\n",
    "    rtdetr_fps = comparison_results['rtdetr']['avg_fps']\n",
    "    yolo_fps = comparison_results['yolo']['avg_fps']\n",
    "    speedup = comparison_results['speedup_percent']\n",
    "    \n",
    "    rtdetr_dets = comparison_results['rtdetr']['avg_dets']\n",
    "    yolo_dets = comparison_results['yolo']['avg_dets']\n",
    "    det_diff = abs(rtdetr_dets - yolo_dets)\n",
    "    \n",
    "    baseline_fps = 39.8  # Our pipeline baseline\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ INTEGRATION DECISION SUMMARY\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"üìä Performance Metrics:\")\n",
    "    print(f\"   RT-DETR:  {rtdetr_fps:.2f} FPS | {rtdetr_dets:.2f} avg detections/frame\")\n",
    "    print(f\"   YOLO v8s: {yolo_fps:.2f} FPS | {yolo_dets:.2f} avg detections/frame\")\n",
    "    print(f\"   Baseline: {baseline_fps:.2f} FPS (our pipeline)\")\n",
    "    print(f\"   Speed difference: {speedup:+.1f}%\\n\")\n",
    "    \n",
    "    # Decision logic\n",
    "    is_faster = rtdetr_fps > yolo_fps\n",
    "    is_comparable_accuracy = det_diff < 0.5  # Less than 0.5 person difference on average\n",
    "    beats_baseline = rtdetr_fps > baseline_fps\n",
    "    \n",
    "    print(f\"‚úÖ Speed: {'RT-DETR is FASTER' if is_faster else 'YOLO is faster'}\")\n",
    "    print(f\"‚úÖ Accuracy: {'Comparable detection counts' if is_comparable_accuracy else 'Different detection counts'}\")\n",
    "    print(f\"‚úÖ Baseline: {'Beats pipeline baseline' if beats_baseline else 'Below pipeline baseline'}\\n\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(\"=\"*70)\n",
    "    if is_faster and is_comparable_accuracy:\n",
    "        print(\"üü¢ RECOMMENDATION: INTEGRATE RT-DETR\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ {speedup:+.1f}% faster than YOLO v8s\")\n",
    "        print(f\"  ‚Ä¢ Comparable detection accuracy (¬±{det_diff:.2f} detections/frame)\")\n",
    "        print(f\"  ‚Ä¢ {'Beats' if beats_baseline else 'Matches'} our pipeline baseline\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"  1. Create detector wrapper: det_track/detectors/rtdetr_detector.py\")\n",
    "        print(\"  2. Add 'rt-detr' option to pipeline_config.yaml\")\n",
    "        print(\"  3. Run full pipeline test with RT-DETR backend\")\n",
    "        print(\"  4. Benchmark complete pipeline (detection + tracking + pose)\")\n",
    "        print(\"  5. Update documentation and merge to main\")\n",
    "    elif is_faster and not is_comparable_accuracy:\n",
    "        print(\"üü° RECOMMENDATION: FURTHER INVESTIGATION NEEDED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ RT-DETR is faster ({speedup:+.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ BUT detection counts differ significantly (¬±{det_diff:.2f})\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"  1. Visually inspect detection quality on sample frames\")\n",
    "        print(\"  2. Check for false positives/negatives\")\n",
    "        print(\"  3. Test with different confidence thresholds\")\n",
    "        print(\"  4. Decide if detection differences are acceptable for pose estimation\")\n",
    "    else:\n",
    "        print(\"üî¥ RECOMMENDATION: STICK WITH YOLO\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ YOLO is faster or equivalent ({speedup:+.1f}% difference)\")\n",
    "        print(f\"  ‚Ä¢ YOLO is proven and well-integrated\")\n",
    "        print(f\"  ‚Ä¢ No clear advantage to switching\")\n",
    "        print(\"\\nConclusion:\")\n",
    "        print(\"  RT-DETR does not provide sufficient improvement to justify integration.\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run the comparison first to generate decision summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c804e8",
   "metadata": {},
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate decision summary\n",
    "if 'comparison_results' in locals():\n",
    "    rf_fps = comparison_results['rf_detr']['avg_fps']\n",
    "    yolo_fps = comparison_results['yolo']['avg_fps']\n",
    "    speedup = comparison_results['speedup_percent']\n",
    "    \n",
    "    rf_dets = comparison_results['rf_detr']['avg_dets']\n",
    "    yolo_dets = comparison_results['yolo']['avg_dets']\n",
    "    det_diff = abs(rf_dets - yolo_dets)\n",
    "    \n",
    "    baseline_fps = 39.8  # Our pipeline baseline\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ INTEGRATION DECISION SUMMARY\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"üìä Performance Metrics:\")\n",
    "    print(f\"   RF-DETR:  {rf_fps:.2f} FPS | {rf_dets:.2f} avg detections/frame\")\n",
    "    print(f\"   YOLO v8s: {yolo_fps:.2f} FPS | {yolo_dets:.2f} avg detections/frame\")\n",
    "    print(f\"   Baseline: {baseline_fps:.2f} FPS (our pipeline)\")\n",
    "    print(f\"   Speed difference: {speedup:+.1f}%\\n\")\n",
    "    \n",
    "    # Decision logic\n",
    "    is_faster = rf_fps > yolo_fps\n",
    "    is_comparable_accuracy = det_diff < 0.5  # Less than 0.5 person difference on average\n",
    "    beats_baseline = rf_fps > baseline_fps\n",
    "    \n",
    "    print(f\"‚úÖ Speed: {'RF-DETR is FASTER' if is_faster else 'YOLO is faster'}\")\n",
    "    print(f\"‚úÖ Accuracy: {'Comparable detection counts' if is_comparable_accuracy else 'Different detection counts'}\")\n",
    "    print(f\"‚úÖ Baseline: {'Beats pipeline baseline' if beats_baseline else 'Below pipeline baseline'}\\n\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(\"=\"*70)\n",
    "    if is_faster and is_comparable_accuracy:\n",
    "        print(\"üü¢ RECOMMENDATION: INTEGRATE RF-DETR\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ {speedup:+.1f}% faster than YOLO v8s\")\n",
    "        print(f\"  ‚Ä¢ Comparable detection accuracy (¬±{det_diff:.2f} detections/frame)\")\n",
    "        print(f\"  ‚Ä¢ {'Beats' if beats_baseline else 'Matches'} our pipeline baseline\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"  1. Create detector wrapper: det_track/detectors/rf_detr_detector.py\")\n",
    "        print(\"  2. Add 'rf-detr' option to pipeline_config.yaml\")\n",
    "        print(\"  3. Run full pipeline test with RF-DETR backend\")\n",
    "        print(\"  4. Benchmark complete pipeline (detection + tracking + pose)\")\n",
    "        print(\"  5. Update documentation and merge to main\")\n",
    "    elif is_faster and not is_comparable_accuracy:\n",
    "        print(\"üü° RECOMMENDATION: FURTHER INVESTIGATION NEEDED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ RF-DETR is faster ({speedup:+.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ BUT detection counts differ significantly (¬±{det_diff:.2f})\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"  1. Visually inspect detection quality on sample frames\")\n",
    "        print(\"  2. Check for false positives/negatives\")\n",
    "        print(\"  3. Test with different confidence thresholds\")\n",
    "        print(\"  4. Decide if detection differences are acceptable for pose estimation\")\n",
    "    else:\n",
    "        print(\"üî¥ RECOMMENDATION: STICK WITH YOLO\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nReasons:\")\n",
    "        print(f\"  ‚Ä¢ YOLO is faster or equivalent ({speedup:+.1f}% difference)\")\n",
    "        print(f\"  ‚Ä¢ YOLO is proven and well-integrated\")\n",
    "        print(f\"  ‚Ä¢ No clear advantage to switching\")\n",
    "        print(\"\\nConclusion:\")\n",
    "        print(\"  RF-DETR does not provide sufficient improvement to justify integration.\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run the comparison first to generate decision summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b74d7",
   "metadata": {},
   "source": [
    "### üéØ Key Takeaways\n",
    "\n",
    "**What We Learned:**\n",
    "- Speed comparison on identical hardware/video\n",
    "- Detection quality consistency\n",
    "- Real-world performance on our use case (cricket/sports)\n",
    "- Integration complexity assessment\n",
    "\n",
    "**Next Steps Based on Results:**\n",
    "- ‚úÖ Green recommendation ‚Üí Create integration PR\n",
    "- üü° Yellow recommendation ‚Üí Test more videos, check false positives\n",
    "- üî¥ Red recommendation ‚Üí Document findings, stick with YOLO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
