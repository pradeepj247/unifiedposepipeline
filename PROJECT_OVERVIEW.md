# Unified Pose Pipeline - Project Overview & Cold Start Guide

**Last Updated**: January 17, 2026  
**Status**: Detection/Tracking Complete âœ… | Pose Pipeline Active ğŸ”„ | Ready for Cleanup & Refinement

---

## ğŸ¯ Project Goal

Build a **unified pose estimation pipeline** that processes sports videos to extract biomechanical analytics for coaching and performance analysis. The pipeline handles the complete workflow:

1. **Detect & Track**: Identify and track all persons in a video
2. **Select**: User selects the target athlete visually
3. **Extract**: Get 2D and 3D pose keypoints for the selected person
4. **Analyze**: Derive biomechanics (joint angles, COM, forces, etc.)

**Target Use Case**: Sports coaching, movement analysis, performance optimization

---

## ğŸ“Š Pipeline Stages Overview

### Stage Group 1: Detection, Tracking & Selection (COMPLETE âœ…)
Detailed documentation: [`det_track/newdocs/`](det_track/newdocs/)

```
Video Input
    â†“
[Stage 0] Video Normalization (H.264, GOP=30, â‰¤1080p)
    â†“
[Stage 1] YOLO Detection (YOLOv8s) + Eager Crop Extraction
    â†“
[Stage 2] ByteTrack Offline Tracking
    â†“
[Stage 3] Multi-Stage Analysis & Refinement
    â”œâ”€ Stage 3a: Tracklet Statistics
    â”œâ”€ Stage 3b: Canonical Grouping (67 tracklets â†’ 40+ persons)
    â”œâ”€ Stage 3c: Filter & Crop Selection (40+ â†’ 8-10 persons)
    â””â”€ Stage 3d: OSNet Visual ReID (8-10 â†’ 7-8 persons, optional)
    â†“
[Stage 4] HTML Viewer Generation (WebP animations)
    â†“
[Stage 5] Person Selection (User picks person_id)
    â†“
Output: selected_person.npz (bboxes for target athlete)
```

**Documentation**: See [`det_track/newdocs/README_MASTER.md`](det_track/newdocs/README_MASTER.md) for complete details.

**Performance**: ~60 seconds for 2025 frames (1920Ã—1080) on T4 GPU

### Stage Group 2: Pose Estimation & 3D Lifting (ACTIVE ğŸ”„)
Current implementation files (need cleanup):

- **`udp_video.py`**: Main unified detection pipeline (2D pose)
- **`run_posedet.py`**: Pose detection runner
- **`run_detector.py`**: Detector runner
- **`test_detector.py`**: Detector testing
- **`vis_wb3d.py`**: 3D wholebody visualization
- **`udp_3d_lifting.py`**: 3D lifting pipeline

**Models Configured** (see [`setup/models.yaml`](setup/models.yaml)):
- **2D Pose**: RTMPose (COCO/Halpe26), ViTPose-B, RTM Wholebody 3D
- **3D Lifting**: MotionAGFormer
- **Detection**: YOLOv8s (PyTorch + TensorRT)
- **ReID**: OSNet x0.25, OSNet x1.0 (PyTorch + ONNX)

### Stage Group 3: Biomechanics Analysis (PLANNED ğŸ”œ)
- Joint angle computation
- Center of mass tracking
- Ground reaction force estimation
- Movement quality metrics

---

## ğŸ—‚ï¸ Repository Structure

### Local Development (Windows)
```
D:\trials\unifiedpipeline\newrepo\              # Main repository root
â”œâ”€â”€ det_track/                                  # Detection & tracking pipeline (COMPLETE)
â”‚   â”œâ”€â”€ stage0_normalize_video.py              # Video normalization
â”‚   â”œâ”€â”€ stage1_detect.py                       # YOLO detection + crop extraction
â”‚   â”œâ”€â”€ stage2_track.py                        # ByteTrack tracking
â”‚   â”œâ”€â”€ stage3a_analyze_tracklets.py           # Tracklet statistics
â”‚   â”œâ”€â”€ stage3b_group_canonical.py             # Canonical grouping
â”‚   â”œâ”€â”€ stage3c_filter_persons.py              # Person filtering + crop selection
â”‚   â”œâ”€â”€ stage3d_refine_visual.py               # OSNet ReID merging
â”‚   â”œâ”€â”€ stage4_generate_html.py                # HTML viewer generation
â”‚   â”œâ”€â”€ stage5_extract_person.py               # Person bbox extraction (to implement)
â”‚   â”œâ”€â”€ run_pipeline.py                        # Orchestrator script
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ pipeline_config.yaml               # All stage configurations
â”‚   â”œâ”€â”€ newdocs/                               # ğŸ“š COMPREHENSIVE DOCUMENTATION
â”‚   â”‚   â”œâ”€â”€ README_MASTER.md                   # Start here!
â”‚   â”‚   â”œâ”€â”€ STAGE0_VIDEO_VALIDATION.md
â”‚   â”‚   â”œâ”€â”€ STAGE1_DETECTION.md
â”‚   â”‚   â”œâ”€â”€ STAGE2_TRACKING.md
â”‚   â”‚   â”œâ”€â”€ STAGE3A_ANALYSIS.md
â”‚   â”‚   â”œâ”€â”€ STAGE3B_GROUPING.md
â”‚   â”‚   â”œâ”€â”€ STAGE3C_FILTER_PERSONS.md
â”‚   â”‚   â”œâ”€â”€ STAGE3D_VISUAL_REFINEMENT.md
â”‚   â”‚   â”œâ”€â”€ STAGE4_HTML_GENERATION.md
â”‚   â”‚   â”œâ”€â”€ STAGE5_PERSON_SELECTION.md
â”‚   â”‚   â”œâ”€â”€ PIPELINE_CONFIG_REFERENCE.md
â”‚   â”‚   â””â”€â”€ RUN_PIPELINE_EXECUTION.md
â”‚   â””â”€â”€ outputs/                               # Pipeline outputs (gitignored)
â”‚
â”œâ”€â”€ setup/                                      # Environment setup scripts
â”‚   â”œâ”€â”€ step1_install_packages.py              # Install Python dependencies
â”‚   â”œâ”€â”€ step2_fetch_models.py                  # Download model weights
â”‚   â”œâ”€â”€ step3_fetch_demodata.py                # Download demo videos
â”‚   â”œâ”€â”€ step4_verify_envt.py                   # Verify installation
â”‚   â”œâ”€â”€ models.yaml                            # Model configuration (all sources)
â”‚   â””â”€â”€ setup_utils.py                         # Shared utilities
â”‚
â”œâ”€â”€ configs/                                    # Global configs (pose estimation)
â”‚   â”œâ”€â”€ udp_video.yaml
â”‚   â””â”€â”€ udp_image.yaml
â”‚
â”œâ”€â”€ lib/                                        # Core libraries
â”‚   â”œâ”€â”€ vitpose/                               # ViTPose implementation
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ utils/                                      # Shared utilities
â”‚   â”œâ”€â”€ logger.py                              # Pipeline logging
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ demo_data/                                  # Demo videos & images
â”‚   â”œâ”€â”€ videos/
â”‚   â”‚   â””â”€â”€ dance.mp4
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ sample.jpg
â”‚
â”œâ”€â”€ snippets/                                   # Temporary debug code (NOT committed)
â”‚
â”œâ”€â”€ udp_video.py                                # ğŸ”„ Main pose pipeline (needs cleanup)
â”œâ”€â”€ run_posedet.py                              # ğŸ”„ Pose detection runner
â”œâ”€â”€ run_detector.py                             # ğŸ”„ Detector runner
â”œâ”€â”€ test_detector.py                            # ğŸ”„ Testing script
â”œâ”€â”€ vis_wb3d.py                                 # ğŸ”„ 3D wholebody visualization
â”œâ”€â”€ udp_3d_lifting.py                           # ğŸ”„ 3D lifting pipeline
â”‚
â”œâ”€â”€ requirements.txt                            # Python dependencies
â”œâ”€â”€ README.md                                   # Main project README
â””â”€â”€ PROJECT_OVERVIEW.md                         # â† This file (cold start guide)
```

### Google Colab Deployment (Production)
```
/content/
â”œâ”€â”€ unifiedposepipeline/                        # Cloned repository
â”‚   â”œâ”€â”€ det_track/                             # Detection & tracking pipeline
â”‚   â”œâ”€â”€ setup/                                 # Setup scripts (run these first)
â”‚   â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ demo_data/
â”‚   â”œâ”€â”€ udp_video.py
â”‚   â”œâ”€â”€ run_posedet.py
â”‚   â”œâ”€â”€ vis_wb3d.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                                     # Downloaded model weights
â”‚   â”œâ”€â”€ yolo/
â”‚   â”‚   â”œâ”€â”€ yolov8s.pt                         # 22.6 MB
â”‚   â”‚   â””â”€â”€ yolov8s.engine                     # 23.0 MB (TensorRT)
â”‚   â”œâ”€â”€ vitpose/
â”‚   â”‚   â””â”€â”€ vitpose-b.pth                      # 360 MB
â”‚   â”œâ”€â”€ rtmlib/
â”‚   â”‚   â”œâ”€â”€ rtmpose-l-coco-384x288.onnx        # 111.1 MB
â”‚   â”‚   â””â”€â”€ rtmpose-l-halpe26-384x288.onnx     # 112.9 MB
â”‚   â”œâ”€â”€ motionagformer/
â”‚   â”‚   â””â”€â”€ motionagformer-base-h36m.pth.tr    # 141.9 MB
â”‚   â”œâ”€â”€ wb3d/
â”‚   â”‚   â””â”€â”€ rtmw3d-l.onnx                      # 230 MB
â”‚   â””â”€â”€ reid/
â”‚       â”œâ”€â”€ osnet_x0_25_msmt17.onnx            # 0.9 MB
â”‚       â””â”€â”€ osnet_x1_0_msmt17.pt               # 10.9 MB
â”‚
â””â”€â”€ drive/                                      # Google Drive mount (optional backup)
    â””â”€â”€ MyDrive/
        â””â”€â”€ pipelinemodels/                     # Fallback model location
```

---

## ğŸš€ Getting Started on Google Colab

### Step 1: Clone Repository
```python
# In Colab notebook
!git clone https://github.com/pradeepj247/unifiedposepipeline.git
%cd unifiedposepipeline
```

**GitHub Repository**: `https://github.com/pradeepj247/unifiedposepipeline`  
**Owner**: `pradeepj247`

### Step 2: Run Setup Scripts
```python
# Install Python packages
!python setup/step1_install_packages.py

# Download model weights (~1 GB total)
!python setup/step2_fetch_models.py

# Download demo data (optional)
!python setup/step3_fetch_demodata.py

# Verify environment
!python setup/step4_verify_envt.py
```

**Note**: Models are fetched from GitHub releases with automatic fallback to Google Drive if needed.

### Step 3: Run Detection & Tracking Pipeline
```python
# Process demo video (outputs HTML viewer for person selection)
%cd det_track
!python run_pipeline.py --config configs/pipeline_config.yaml

# Open HTML viewer to select person
from google.colab import files
files.download('outputs/dance/webp_viewer/person_selection.html')
```

### Step 4: Extract Selected Person
```python
# After visual selection, extract person bboxes
!python stage5_extract_person.py \
    --config configs/pipeline_config.yaml \
    --person_id 5 \
    --source 3c
```

### Step 5: Run Pose Estimation (Current Implementation)
```python
# Navigate back to root
%cd /content/unifiedposepipeline

# Run unified pose pipeline (using selected_person.npz)
!python udp_video.py --config configs/udp_video.yaml

# Or run pose detection separately
!python run_posedet.py --input outputs/dance/selected_person.npz

# Or visualize 3D wholebody
!python vis_wb3d.py --video demo_data/videos/dance.mp4
```

---

## ğŸ”„ Development Workflow

### Local â†’ GitHub â†’ Colab Cycle

1. **Local Development (Windows)**:
   ```powershell
   # Work on code locally at:
   D:\trials\unifiedpipeline\newrepo\
   
   # Make changes, test locally if possible
   python det_track/run_pipeline.py --config det_track/configs/pipeline_config.yaml
   ```

2. **Commit to GitHub**:
   ```bash
   # Stage changes
   git add det_track/stage2_track.py
   
   # Commit with descriptive message
   git commit -m "Optimize ByteTrack: reuse dummy frame, target 800+ FPS"
   
   # Push to remote
   git push origin main
   ```

3. **Pull on Colab**:
   ```python
   # In Colab notebook
   %cd /content/unifiedposepipeline
   !git pull origin main
   ```

4. **Test on Colab**:
   ```python
   # Run pipeline with GPU
   !python det_track/run_pipeline.py --config det_track/configs/pipeline_config.yaml
   
   # Check outputs
   !ls -lh det_track/outputs/dance/
   ```

5. **Share Results Back**:
   ```python
   # Download outputs for local analysis
   from google.colab import files
   files.download('det_track/outputs/dance/webp_viewer/person_selection.html')
   
   # Or zip and download entire outputs folder
   !zip -r outputs.zip det_track/outputs/
   files.download('outputs.zip')
   ```

6. **Debug Locally**:
   - Review downloaded outputs
   - Analyze logs, HTML viewers
   - Identify issues, fix code locally
   - Repeat cycle

**Why This Workflow?**
- **Local**: Fast iteration, full IDE support, debugging tools
- **Colab**: Free GPU (T4), reproducible environment, easy sharing
- **GitHub**: Version control, collaboration, backup

---

## ğŸ“ What Has Been Achieved

### âœ… Detection, Tracking & Selection Pipeline (COMPLETE)
**Location**: `det_track/` folder  
**Documentation**: [`det_track/newdocs/README_MASTER.md`](det_track/newdocs/README_MASTER.md)

**Highlights**:
- 8-stage pipeline (0 â†’ 5) with comprehensive documentation
- Performance: 60 seconds for 2025 frames on T4 GPU
- YOLOv8s detection with eager crop extraction (11Ã— faster Stage 3c)
- ByteTrack optimization (694 FPS, 27% improvement)
- 3-bin contiguous crop selection (temporal diversity)
- Optional OSNet ReID visual matching
- WebP-based HTML viewer for person selection
- Standardized NPZ output format for pose pipelines

**Key Design Decisions** (all documented):
- YOLOv8s over v8n (no speed penalty, better accuracy)
- Eager crop extraction (net +5-6s speedup)
- 3-bin contiguous selection (deterministic, smooth animations)
- ByteTrack dummy frame reuse (27% faster)
- detection_idx linkage (O(1) lookup throughout pipeline)

**Output**: `selected_person.npz` with all bboxes for target athlete

### âœ… Pose Estimation Pipeline (IMPLEMENTED, NEEDS CLEANUP)
**Current Files**:
- `udp_video.py`: Main unified detection pipeline
- `run_posedet.py`: Pose detection runner
- `run_detector.py`: Detector runner
- `test_detector.py`: Testing utilities
- `vis_wb3d.py`: 3D wholebody visualization
- `udp_3d_lifting.py`: 3D lifting pipeline

**Models Integrated**:
| Model | Type | Size | Purpose |
|-------|------|------|---------|
| YOLOv8s | Detection | 22.6 MB | Person detection |
| YOLOv8s TensorRT | Detection | 23.0 MB | Accelerated detection |
| ViTPose-B | 2D Pose | 360 MB | High-quality 2D keypoints (17 COCO) |
| RTMPose-L (COCO) | 2D Pose | 111.1 MB | Fast 2D keypoints (17 COCO) |
| RTMPose-L (Halpe26) | 2D Pose | 112.9 MB | Extended keypoints (26 Halpe) |
| RTM Wholebody 3D | 3D Pose | 230 MB | Wholebody 3D (133 keypoints) |
| MotionAGFormer | 3D Lifting | 141.9 MB | Temporal 3D pose refinement |
| OSNet x0.25 | ReID | 0.9 MB | Visual person matching (ONNX) |
| OSNet x1.0 | ReID | 10.9 MB | Visual person matching (PyTorch) |

**Capabilities**:
- 2D pose estimation (17-133 keypoints)
- 3D pose lifting with temporal smoothing
- Multi-backend support (RTMPose ONNX, ViTPose PyTorch)
- Wholebody estimation (body + hands + face)

**Current Status**: Working implementation, but spread across multiple files. **Needs consolidation and cleanup**.

---

## ğŸ¯ Next Steps & Cleanup Plan

### Immediate Tasks
1. **Implement Stage 5**: Create `det_track/stage5_extract_person.py` (spec ready in docs)
2. **Consolidate Pose Pipeline**: Merge `udp_video.py`, `run_posedet.py` logic into unified stage
3. **Create Stage 6**: 2D pose estimation using `selected_person.npz` as input
4. **Create Stage 7**: 3D lifting using Stage 6 output
5. **Clean Up**: Remove redundant files (`test_detector.py`, etc.)

### Documentation Needed
- Stage 6 documentation (2D pose estimation)
- Stage 7 documentation (3D lifting)
- Stage 8+ documentation (biomechanics)
- Integration guide (det_track â†’ pose â†’ analytics)

### Testing Strategy
- Test full pipeline end-to-end on Colab
- Validate NPZ format compatibility
- Benchmark performance (2D/3D stages)
- Document model switching (RTMPose â†” ViTPose)

---

## ğŸ“š Key Documentation Files

### Must-Read for Cold Start
1. **This file** (`PROJECT_OVERVIEW.md`): Overall project context
2. [`det_track/newdocs/README_MASTER.md`](det_track/newdocs/README_MASTER.md): Detection/tracking pipeline master doc
3. [`det_track/newdocs/QUICK_REFERENCE.md`](det_track/newdocs/QUICK_REFERENCE.md): Command cheat sheet
4. [`setup/models.yaml`](setup/models.yaml): All model sources and configurations

### Deep Dives (When Needed)
- [`det_track/newdocs/STAGE*.md`](det_track/newdocs/): Individual stage documentation (0-5)
- [`det_track/newdocs/PIPELINE_CONFIG_REFERENCE.md`](det_track/newdocs/PIPELINE_CONFIG_REFERENCE.md): Config file explained
- [`det_track/newdocs/RUN_PIPELINE_EXECUTION.md`](det_track/newdocs/RUN_PIPELINE_EXECUTION.md): Orchestrator logic
- [`README.md`](README.md): Main project README

---

## ğŸ› ï¸ Troubleshooting

### Common Issues on Colab

**Issue**: Models not downloading  
**Solution**: Mount Google Drive and use fallback location:
```python
from google.colab import drive
drive.mount('/content/drive')
!python setup/step2_fetch_models.py  # Will auto-fallback to Drive
```

**Issue**: Out of GPU memory  
**Solution**: Reduce batch size or use smaller models:
```yaml
# In configs/udp_video.yaml
pose_estimation:
  method: rtmpose  # Use RTMPose instead of ViTPose (smaller)
```

**Issue**: Path resolution errors (`${repo_root}` in logs)  
**Solution**: Check YAML syntax in config files, ensure multi-pass resolution works

**Issue**: HTML viewer not rendering in Colab  
**Solution**: Download HTML file and open locally:
```python
from google.colab import files
files.download('det_track/outputs/dance/webp_viewer/person_selection.html')
```

### Getting Help

1. **Check documentation**: `det_track/newdocs/` has comprehensive answers
2. **Review logs**: Pipeline outputs detailed logs with timing and errors
3. **Inspect outputs**: HTML viewers, NPZ files show intermediate results
4. **Test incrementally**: Run individual stages to isolate issues

---

## ğŸ“Š Performance Benchmarks

### Detection & Tracking (Stages 0-4)
- **Hardware**: Google Colab T4 GPU
- **Video**: 2025 frames, 1920Ã—1080, 30 FPS
- **Total Time**: 60.24 seconds (33.6 FPS overall)
- **Breakdown**: See [`det_track/newdocs/README_MASTER.md`](det_track/newdocs/README_MASTER.md) for detailed table

### Pose Estimation (Estimated)
- **RTMPose**: ~50 FPS (ONNX optimized)
- **ViTPose**: ~40 FPS (PyTorch)
- **MotionAGFormer**: ~30 FPS (temporal refinement)
- **Wholebody 3D**: ~25 FPS (133 keypoints)

---

## ğŸ”— Quick Links

- **GitHub Repository**: https://github.com/pradeepj247/unifiedposepipeline
- **Main README**: [README.md](README.md)
- **Detection/Tracking Docs**: [det_track/newdocs/README_MASTER.md](det_track/newdocs/README_MASTER.md)
- **Model Configuration**: [setup/models.yaml](setup/models.yaml)
- **Setup Scripts**: [setup/](setup/)

---

**Document Version**: 1.0  
**Last Updated**: January 17, 2026  
**For Questions**: Review comprehensive documentation in `det_track/newdocs/` first

